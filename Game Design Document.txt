 
 
 
 
 
 
 
 
 
 
   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–„â–ˆ  â–€â–ˆâ–ˆâ–ˆâ–ˆ    â–â–ˆâ–ˆâ–ˆâ–ˆâ–€      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„     â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„     â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 
  â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–Œ   â–ˆâ–ˆâ–ˆâ–ˆâ–€       â–ˆâ–ˆâ–ˆ   â–€â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ 
  â–ˆâ–ˆâ–ˆ    â–ˆâ–€  â–ˆâ–ˆâ–ˆâ–Œ    â–ˆâ–ˆâ–ˆ  â–â–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–€  
  â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆâ–Œ    â–€â–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–€         â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ  â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„      â–„â–ˆâ–ˆâ–ˆ         â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–ˆâ–ˆâ–€  â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„      â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„       â–ˆâ–ˆâ–ˆ        
â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–Œ    â–ˆâ–ˆâ–ˆâ–ˆâ–€â–ˆâ–ˆâ–„          â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€     â–€â–€â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–„  â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€â–€â–€   â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€     â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€     â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 
         â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–â–ˆâ–ˆâ–ˆ  â–€â–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–„    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–„    â–ˆâ–ˆâ–ˆ    â–ˆâ–„           â–ˆâ–ˆâ–ˆ 
   â–„â–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–„â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆâ–„       â–ˆâ–ˆâ–ˆ   â–„â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ    â–„â–ˆ    â–ˆâ–ˆâ–ˆ 
 â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€  â–ˆâ–€   â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–„      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€  
                                                                              â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ                                        
                                                                                                                          









~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 â–—â–„â–– â–—â––  â–—â––â–—â–„â–„â–„â––â–—â–„â–„â–– â–—â––  â–—â––â–—â–„â–„â–„â––â–—â–„â–„â–„â––â–—â–– â–—â––
â–â–Œ â–â–Œâ–â–Œ  â–â–Œâ–â–Œ   â–â–Œ â–â–Œâ–â–Œ  â–â–Œ  â–ˆ  â–â–Œ   â–â–Œ â–â–Œ
â–â–Œ â–â–Œâ–â–Œ  â–â–Œâ–â–›â–€â–€â–˜â–â–›â–€â–šâ––â–â–Œ  â–â–Œ  â–ˆ  â–â–›â–€â–€â–˜â–â–Œ â–â–Œ
â–â–šâ–„â–â–˜ â–â–šâ–â–˜ â–â–™â–„â–„â––â–â–Œ â–â–Œ â–â–šâ–â–˜ â–—â–„â–ˆâ–„â––â–â–™â–„â–„â––â–â–™â–ˆâ–Ÿâ–Œ

~ OVERVIEW ~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


========================
ABOUT
========================
Six Degrees is a game about word associations. It's a poetic-strategic exploration game where players traverse a network of interconnected words (like a Wiki) to reach a target word in the fewest steps possible. The game's structure is a hybrid between a logical taxonomy (like a tree of concepts) and a metaphorical social web (a network of associations). Each move is a click on a word within a definition, which reveals that word's own definition and related words. Every click forward counts as a step.


========================
GOAL OF THE GAME
========================
The objective is simple: reach the destination word in the fewest possible steps. The fewer the steps, the better your score. Six Degrees is both a reflective puzzle and a meditative journey â€” a game that rewards thoughtful connections and 'scalable / zoomable' traversable logic.


========================
CORE GAMEPLAY LOOP
========================
1. The player is shown a starting word and a destination word.
2. Clicking the starting word opens its natural-language definition.
3. In the definition, certain words are clickable: these may be parents, children, acquaintances, traits, or roles.
4. Clicking on a linked word opens that word's definition.
5. Every click forward increments the player's step count.
6. The goal is to reach the destination word in the least number of steps possible.

Polished features:
- Show visited path as a breadcrumb trail (Backtracking rewinds time: it removes the steps taken after that point, encouraging experimentation and puzzle-solving without penalty)
- Make it a "daily" game, with a new starting word and destination word each day. This is in sync globally, but allow a "Free Play" mode as well.
- Add a "Share" button once you finish to share a screenshot of your score with your friends, along with a link to the game, to encourage virality.
- Add a "Something missing in this definition?" button to allow players to suggest new words to add to the graph, to be used in later iteration of the game
- Add difficulty slider to allow players to choose how far away the starting word is from the destination word



========================
ENVISIONED PLAYER STRATEGIES
========================
There are two core player archetypes:

The Logician:
- Navigates up to parent categories to access siblings
- Uses structured inference: "If cat is an animal, and dog is too, I can get to dog by going up to animal first"
- Values hierarchy, clarity, and precision

The Poet:
- Follows acquaintances, traits, and roles through contextual resonance
- Moves by metaphor, emotion, cultural vibe: "Apple reminds me of knowledge... or Steve Jobs"
- Values surprise, meaning, and personal logic

Both Modes Can Succeed:
Most players will mix both styles during a single game.

Example Path: Destination = Stop Sign
Logician's path (6 steps): Cat â†’ Animal â†’ Human â†’ Civilization â†’ Rules â†’ Traffic Sign â†’ Stop Sign
Poet's path (6 steps): Cat â†’ Night â†’ Street â†’ City â†’ Crosswalk â†’ Red Light â†’ Stop Sign

Both are valid. Both are beautiful. I hypothesise that making the best of both results in the highest score.




========================
DIFFICULTY TUNING
========================
Difficulty is tuned by controlling the number of steps the origin word is away from the destination.

- A random destination word is selected first
- Then, the game walks X steps away in a random direction through the existing graph to select an origin word. This ensures that the destination word is always reachable from the origin word.
- X = difficulty level
- Starting rule: All paths are built with 6 degrees of separation to match the game title
- Later, modes like "Easy (4 degrees)" or "Hard (8+)" can be introduced











~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
â–—â–– â–—â–– â–—â–„â–– â–—â–„â–„â–– â–—â–„â–„â–„     â–—â–„â–„â–„â––â–—â––  â–—â––â–—â–„â–„â–– â–—â–„â–„â–„â–– â–—â–„â–„â––
â–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–Œ  â–ˆ      â–ˆ   â–â–šâ–â–˜ â–â–Œ â–â–Œâ–â–Œ   â–â–Œ   
â–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–›â–€â–šâ––â–â–Œ  â–ˆ      â–ˆ    â–â–Œ  â–â–›â–€â–˜ â–â–›â–€â–€â–˜ â–â–€â–šâ––
â–â–™â–ˆâ–Ÿâ–Œâ–â–šâ–„â–â–˜â–â–Œ â–â–Œâ–â–™â–„â–„â–€      â–ˆ    â–â–Œ  â–â–Œ   â–â–™â–„â–„â––â–—â–„â–„â–

~ WORD TYPES ~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



                                                          
--.--|   ||,   .,---.    . . .,---.,---.,--. 
  |  |---|||\  ||  _.    | | ||   ||---'|   |
  |  |   ||| \ ||   |    | | ||   ||  \ |   |
  `  `   '``  `'`---'    `-'-'`---'`   ``--' 
(THING WORD)  


========================
WHAT IT IS
========================
- In Six Degrees, all primary nodes in the graph are called â€œThing Wordsâ€ â€” nouns that represent distinct entities, objects, systems, or concepts.
- These words are the most common in the game and serve as both origin and destination points in traversal.
- While all Thing Words share the same structural format, they may be be called "parent, child, sibling, or acquaintance" to describe how they relate to each other.
  - A thing word is called a **parent** when it represents the most precise broader category of a thing.
  - A thing word is called a **child** when it is a more specific example of another thing.
  - A thing word is called a **sibling** when it is a parallel member of the same category (purely used for thinking about the game development and design)
  - A thing word is called an **acquaintance** when it shares thematic, symbolic, or commonly associated meaning with another thing.



========================
DEFINITION SYSTEM
========================
Every Thing Word includes the following metadata, with its own parent (1), children (7), traits (7), acquaintances (7), and purposes (7).


{
  "word": "Cat",
  "type": "thing"
  "parent": "Animal",
  "children": [
    "Siamese",
    "Tabby",
    "Persian",
    "Maine Coon",
    "Sphynx",
    "Ragdoll",
    "Bengal"
  ],
  "traits": [
    "independent",
    "agile",
    "curious",
    "nocturnal",
    "graceful",
    "mysterious",
    "affectionate"
  ],
  "acquaintances": [
    "Dog",
    "Mouse",
    "Litter Box",
    "Laser Pointer",
    "Fur",
    "Scratching Post",
    "Couch"
  ],
  "purposes": [
    "Companion",
    "Pet"
  ]
}  


========================
CHILD
========================
- Children are more specific examples, types, or subcategory 'thing words' generated from a parent
- Each child must satisfy the statement: "A [child] is a kind of [parent word]"
- In player traversal, Children support downward movement in abstraction and help players zoom into more concrete ideas

Examples:
- "Animal" â†’ Children: [in order of commonality] Dog, Cat, Bird, Fish, Horse, Cow, Elephant
- "Tool" â†’ Children: [in order of commonality] Hammer, Screwdriver, Wrench, Pliers, Saw, Drill, Tape Measure
- "Emotion" â†’ Children: [in order of commonality] Happy, Sad, Angry, Fear, Love, Surprise, Disgust



========================
PARENT
========================
- The Parent is the single most appropriate broader category that a a child word belongs to
- Unlike children, which are generated based on their parents, parents are never generated from children, as generation is a one-way process.
- In player traversal, clicking on a parent supports "upward" movement in abstraction and help players zoom out to more general words
- The first parent, which has no parent of its own, is our God Word: "Thing"

Examples:
- "Cat" â†’ Parent: Animal
- "Sadness" â†’ Parent: Emotion
- "Hammer" â†’ Parent: Tool
- "Democracy" â†’ Parent: System



========================
SIBLING
========================
- Siblings are words that share the same parent â€” they are parallel members of the same category.
- For example, â€œDogâ€ and â€œCatâ€ would both be siblings if they were  the direct children of "Mammal.â€ 
- Unlike parents, children, and acquaintances, siblings are never explicitly listed in a wordâ€™s metadata. Instead, they are inferred by moving up to a shared parent and then down into another child. This design encourages discovery and strategy: a player might wish to find a sibling in order to traverse 'horizontally', and may  â€œzoom outâ€ to a shared parent and then â€œzoom inâ€ to a different child.
- The concept of siblings is mentioned here simply because it's useful for thinking about game development, design, and traversal strategy, but is not a direct part of a thing wordâ€™s metadata.

Examples:
- â€œDogâ€ and â€œCatâ€ â†’ Siblings (both children of "Mammal")
- â€œHammerâ€ and â€œScrewdriverâ€ â†’ Siblings (both children of â€œToolâ€)
- â€œJoyâ€ and â€œSadnessâ€ â†’ Siblings (both children of â€œEmotionâ€)
- â€œDemocracyâ€ and â€œMonarchyâ€ â†’ Siblings (both children of â€œSystemâ€)



========================
ACQUAINTANCE
========================
- Acquaintances are a type of Thing word generated based on lateral associations â€” thing words that most frequently co-occur in thought or experience, while not being siblings, parents, children, or synonyms.
- Acquaintances are generated from the thing word and reflect intuitive, often emotional or symbolic associations â€” the kind of answer you might give to the question, â€œWhen I say X, you thinkâ€¦?â€ (as long as the answer is not a parent, child, sibling, or synonym). These connections arise from culture, metaphor, memory, story, or shared experience.
- A wordâ€™s acquaintances often include symbolic opposites, poetic complements, or things that frequently co-occur in thought or experience ("9/11 and Osama Bin Laden")
- Acquaintances are a key part of the â€œPoetâ€™s Traversal Strategy,â€ as they allow players to warp across conceptual space â€” following emotional resonance, metaphorical logic, or cultural symbolism rather than strict hierarchy. They enable surprising yet satisfying leaps between ideas that may not be categorically related, but feel meaningfully connected.

Examples:
- â€œ9/11â€ â†’ Acquaintances: Osama Bin Laden, Twin Towers, Airplane, Collapse, War on Terror, Pentagon, NYC  
- â€œRomeoâ€ â†’ Acquaintances: Juliet, Balcony, Poison, Tragedy, Youth, Mask, Dagger  
- â€œMirrorâ€ â†’ Acquaintances: Reflection, Glass, Vanity, Gaze, Makeup, Face, Dressing Table  
- â€œCrownâ€ â†’ Acquaintances: King, Queen, Throne, Jewels, Coronation, Scepter, Robe








/// NEEDS WORK
--.--,---.,---.|--.--    . . .,---.,---.,--. 
  |  |---'|---||  |      | | ||   ||---'|   |
  |  |  \ |   ||  |      | | ||   ||  \ |   |
  `  `   ``   '`  `      `-'-'`---'`   ``--' 
(TRAIT WORD)  
                                        


========================
WHAT IT IS
========================
- Traits are adjectives â€” descriptive qualities like "agile," "loud," or "resilient"
- Trait words function as BRIDGES between thing words, by nature of things that share those traits.
- As they are pure qualitative bridges, they do not have parents or children. 

========================
EXAMPLES
========================
- "Cat" â†’ Traits: [in order of commonality] Independent, Agile, Curious, Nocturnal, Graceful, Mysterious, Affectionate
- "Ocean" â†’ Traits: [in order of commonality] Vast, Deep, Powerful, Mysterious, Beautiful, Dangerous, Eternal
- "Book" â†’ Traits: [in order of commonality] Informative, Engaging, Portable, Durable, Valuable, Timeless, Transformative


========================
DEFINITION SYSTEM
========================

The metadata of traits comprise of a list of thing words within whose metadata the trait is listed. Unlike thing words which have pre-set metadata, metadata of traits are dynamically generated from the word graph, (on load or on demand?). The metadata of Traits words are not pre-defined entities but emerge from the graph. In order to generate the metadata of a trait, the system will search the graph for all words that list this trait.

Because of this, during the initial graph build, traits are passively generated in each word's metadata alongside the other components, but are not defined during the initial stage. 
- Only traits that are shared by two or more defined words will be promoted in a later pass to become formal bridge-nodes with their own trait definitions

Example:
{
  "word": "Agile",
  "type": "trait",
  "exemplars": [
    "Cat",
    "Sprinter",
    "Dancer",
    "Gymnast",
    "Falcon",
    "Fencer",
    "Parkour"
  ],

(Each of those words shared betweem two or more thing words' definitions)

Example of a Trait's Natural Language Definition:
"Agile describes a quality of quick, graceful movement. It's a trait shared by [Cats], known for their natural [Agility], [Sprinters] who train for [Speed], and [Dancers] who master [Grace]. This trait connects to ideas of [Movement] and [Precision]."
.
Note: The exact words and connections shown will depend on what exists in the current graph. As more words are added to the system, the trait's definition and connections will naturally grow richer.









/// NEEDS WORK                                                
,---.,---.|    ,---.    . . .,---.,---.,--. 
|---'|   ||    |---     | | ||   ||---'|   |
|  \ |   ||    |        | | ||   ||  \ |   |
`   ``---'`---'`---'    `-'-'`---'`   ``--' 
                                            
                                                           
========================
WHAT IT IS
========================
- Role Words are functional or purpose-centered concepts that describe what a Thing is for, or what it does in the world.
- They can take two common forms:
  â€¢ Gerund Nouns (e.g. â€œCutting,â€ â€œHealing,â€ â€œTeachingâ€) â€” action-like roles
  â€¢ Abstract Nouns (e.g. â€œCompanionship,â€ â€œProtection,â€ â€œGuidanceâ€) â€” value- or experience-based roles
- Role Words act as BRIDGES between Thing Words that fulfill similar purposes, even if they come from different branches of the taxonomy.
- Like Traits, Role Words are non-hierarchical: they do not have parents or children.
- However, unlike Traits, Role Words are defined by not only their EXEMPLARS, but also by their ACQUAINTANCES.

========================
EXAMPLES
========================
- "Scissors" â†’ Roles: Cutting, Separating
- "Dog" â†’ Roles: Companionship, Guarding, Herding
- "Therapist" â†’ Roles: Guiding, Healing
- "Sword" â†’ Roles: Fighting, Cutting, Protecting
- "Alarm Clock" â†’ Roles: Waking, Alerting

========================
DEFINITION SYSTEM
========================

Role Word metadata is composed of:
â€¢ exemplars â€” Thing Words that list the Role in their metadata
â€¢ acquaintances â€” symbolic, cultural, or experiential associations related to the role itself (optional, but encouraged)

Role Words are seeded during Phase 1: when a Thing Word is created, up to 7 salient Role Words may be generated to describe its purpose. These roles are initially stored as string literals within the Thingâ€™s metadata, just like Traits and Acquaintances.

Then, in a later phase, we promote each unique Role Word into its own node if it appears in two or more Thing Words. This ensures roles are real bridges, not one-off metadata fragments.

Example:
{
  "word": "Cutting",
  "type": "role",
  "exemplars": [
    "Scissors",
    "Knife",
    "Scalpel",
    "Sword",
    "Chainsaw"
  ],
  "acquaintances": [
    "Sharpness",
    "Separation",
    "Precision",
    "Motion",
    "Blade"
  ]
}

Example of a Role Wordâ€™s Natural Language Definition:
"Cutting is a purposeful action associated with sharpness, separation, and precision. Itâ€™s a function shared by tools like scissors, knives, scalpels, and swordsâ€”each designed to divide, shape, or sever materials."

Note: Abstract Role Words follow the same structure, but the natural language definition would have more emphasis on emotional or relational context.

Example:
{
  "word": "Companionship",
  "type": "role",
  "exemplars": [
    "Dog",
    "Friend",
    "Pet",
    "Caregiver"
  ],
  "acquaintances": [
    "Loyalty",
    "Presence",
    "Support",
    "Closeness",
    "Love"
  ]
}

"Companionship is a human-centered experience associated with loyalty, presence, and emotional closeness. Itâ€™s a role fulfilled by beings like dogs, friends, pets, and caregiversâ€”offering comfort, connection, and belonging."

As with Traits, Role definitions grow richer as the graph expands. Role Words help players traverse via function, purpose, or effectâ€”revealing why something exists, not just what it is.










~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
â–—â–„â–„â–„â––â–—â–– â–—â––â–—â–„â–„â–„â––    â–—â–– â–—â–– â–—â–„â–– â–—â–„â–„â–– â–—â–„â–„â–„     â–—â–„â–„â–„â––â–—â–„â–„â–– â–—â–„â–„â–„â––â–—â–„â–„â–„â––
  â–ˆ  â–â–Œ â–â–Œâ–â–Œ       â–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–Œ  â–ˆ      â–ˆ  â–â–Œ â–â–Œâ–â–Œ   â–â–Œ   
  â–ˆ  â–â–›â–€â–œâ–Œâ–â–›â–€â–€â–˜    â–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–›â–€â–šâ––â–â–Œ  â–ˆ      â–ˆ  â–â–›â–€â–šâ––â–â–›â–€â–€â–˜â–â–›â–€â–€â–˜
  â–ˆ  â–â–Œ â–â–Œâ–â–™â–„â–„â––    â–â–™â–ˆâ–Ÿâ–Œâ–â–šâ–„â–â–˜â–â–Œ â–â–Œâ–â–™â–„â–„â–€      â–ˆ  â–â–Œ â–â–Œâ–â–™â–„â–„â––â–â–™â–„â–„â––
  
  HOW IT WORKS
                                                                              
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

=================================
GROWING THE WORD GRAPH LIKE A TREE
==================================      
Building the Six Degrees word graph is the most technically and conceptually challeging part of this game's development. Itâ€™s the creation of a living semantic ecosystem that mirrors how people actually think. When you simply dump a list of disconnected words and bolt on relationships afterward (as I initially tried), the result is a patchwork collage. Definitions feel awkward or forced, with words shoehorned into places they donâ€™t naturally belong. Cohesion breaks down, meaningful paths disappear, and the system fails to honor the playerâ€™s intuition, strategies, or associations. Players quickly become disillusioned and abandon logical or poetic strategies in favor of random clicking, which feels more like a game of chance than a meaningful use of one's own associative thinking.

As such, the better approach is to GROW the word graph. Basically, we build every word by extending from what already exists, never by retrofitting or shoehorning. We start at the GOD WORD which has no parent (â€œThingâ€), generate that wordâ€™s children, then each childâ€™s children, and so on, exponentially.

This unidirectional root-to-leaf process, which mirrors real-life growth, ensures that:
	1.	Integration Is Automatic: Each new word slots into a coherent lineage rather than forcing you to manually link or justify every relationship retroactively.
	2.	Paths Are Real: This 'hidden consistency' allows the player to build intuition over how this~ underlying map works, because it follows natural organic patterns. This, in turn, allows the building of strategies, whether conscious or subconscious.
	3.	Player Intuition Is Honored: Whether youâ€™re scaling the hierarchy like a logician or leaping via poetic resonanceâ€”because the structure itself grew to reflect natural thought patterns.


But this is all well and godo when we're thinking about parents and children, but what about acquaintances, traits, and roles? 


==============================================================
THE AQUAINTANCE CHALLENGE (aka the Orphan Problem)
===============================================================
So as we mentioned, we want to build a beautiful, clean system for growing a semantic graph. It starts with a broad concept like "Thing," and then grows outward â€” parent to children, children become parents, and so on, exponentially. This approach is hierarchical, elegant, and very stable. It's like growing a tree in one direction: you always know where a node came from and where it sits in the larger structure.

But then comes Acquaintances. These are not part of the family tree structure of children and parents â€” they're based on how humans naturally associate ideas in the mind. When people think of "Yin," they inevitably think of "Yang." Not because "Yang" is a child or parent of "Yin," but because they co-exist in the same cultural or symbolic space. This is the kind of link that makes the game feel intuitive, human, and resonant (AKA The Poet's Strategy). It's what lets people feel like their minds are being seen.

The problem is: when you include an acquaintance like "Yang" in the definition for "Yin," you now have to create "Yang" as a node. But Yang doesn't yet exist. And worse â€” you didn't get to Yang through the normal parent-child growth process. So now you have an undefined word dangling off the side of your clean tree. And you're faced with a tricky decision: do you generate a new parent for Yang (which might create instability as you have to retroactively justify the new parent), or do you try to ADOPT Yang into a parent of the tree that already exists (which might not be semantically ideal)? Neither option is great â€” and the reason is that the structure wasn't designed to grow backward.

Acquaintances are essential for making the game feel alive, but they break the clean growth model unless we handle them carefully.

The Solution? FINDING ADOPTIVE PARENTS

When a Thing Word generates an Acquaintanceâ€”so â€œ9/11â€ generates Osama Bin Ladenâ€”it can leave us with a dangling node outside our tidy parent-child tree. Rather than inventing a new parent for â€œOsama Bin Ladenâ€ (which would force backwards growth and potentially threaten stability), we find an existing parent in our system to ADOPT every Acquaintance (Political Figures)

1.	Two-Phase Process
	â€¢	Phase One: As we expand from the root (â€œThingâ€) through roughly 2,000 Thing Words, each thing word generates seven Acquaintances in its metadata â€” but none enter the tree yet.
	â€¢	Phase Two: Once that core graph is complete, we revisit every unique Acquaintance term.

2.	Clean Adoption
	â€¢	Existing Node: If the Acquaintance already exists as a Thing Word, we simply link to itâ€”no new hierarchy is created.
	â€¢	New Node: If it doesnâ€™t exist, we generate the Acquaintance exactly once and find an adoptive  parent by asking the LLM to choose the most semantically precise category from our established list. 

At first glance, slapping a new Acquaintance under an existing parent sounds like the same retroactive patchwork we fearedâ€”but in practice, it may not be the case, because:
	â€¢	Rich Taxonomic Coverage: With a mature tree of potentially ~2,000 Thing Words spanning a diverse range of parents, it seems rare to land on an orphan that truly lacks a parent.
	â€¢	Specificity-First Matching: Our LLM prompt explicitly asks for the most precise parent available. Only when no close fit exists does the term fall back to a higher-level category. So worst case, there's still a relevant category even if it's not the most specific.
	â€¢	Constrained Choice: By limiting the adoption to an established list, we eliminate the â€œanything goesâ€ risk. The model canâ€™t invent random parents; it only picks from proven, player-tested categories.

By capturing Acquaintances early and adopting them only after the tree has matured, we preserve the forward-growing integrity of our taxonomy while still allowing the lateral, poetic connections that make the game feel alive and intuitive. Each new link then feels naturalâ€”whether youâ€™re scaling the hierarchy or following a resonant leapâ€”because itâ€™s been slotted exactly where it belongs.

This is a hypothesis that has yet to be tried. 



==============================================================
HOW ARE TRAITS HANDLED?
===============================================================
Traits in Six Degrees occupy a special â€œbridgeâ€ roleâ€”they connect Thing Words by shared qualities without becoming part of the strict parentâ€“child hierarchy. Hereâ€™s the end-to-end flow for traits:
1.	Passive Collection During Growth
	â€¢	As each Thing Word is added (parent â†’ children), up to seven descriptive adjectives are logged in its metadataâ€”e.g. â€œagile,â€ â€œmysterious,â€ â€œresilient.â€
	â€¢	At this stage, traits remain inert strings alongside children and acquaintances; they do not create branches in the tree.
2.	Synonym Normalization & Clustering
	â€¢	Once the core graph (~2,000 words) is complete, we gather all raw trait labels and run a semantic-similarity pass:
	â€¢	Embedding Clustering: Group labels whose embeddings exceed a similarity threshold.
	â€¢	Lexical Normalization: Apply a synonym dictionary (e.g. WordNet) to merge obvious equivalents (â€œagileâ€ â‰ˆ â€œnimbleâ€).
	â€¢	Within each cluster, we pick a single canonical labelâ€”the most frequent or clearest term.
3.	Promotion to Trait Words
	â€¢	We count occurrences of each canonical label. Only those appearing in two or more Thing Word metadata lists are elevated into standalone Trait Word nodes.
	â€¢	This guarantee of â‰¥2 links prevents single-useâ€”orphanâ€”traits.
4.	Trait Word Structure
	â€¢	No Parents or Children: Traits exist outside the taxonomy tree.
	â€¢	Metadata: A ranked list of all Thing Words that exhibit the trait.
	â€¢	Optional Lateral Links: Traits may list related traits as acquaintances to each other.
5.	Dynamic Definition Rendering
	â€¢	When a player clicks a Trait Word, the game engine:
    	â€¢	Retrieves and ranks all associated Thing Words by frequency.
    	â€¢	Generates a natural-language description on the fly using those examples.
	â€¢	As new Thing Words join the graph, their traits feed back into this pipelineâ€”Trait definitions evolve automatically without manual intervention.


/// NEEDS WORK                                                

==============================================================
HOW ARE ROLES HANDLED?
===============================================================
Roles in Six Degrees describe the functional or intentional use of a Thing â€” what it does, or what itâ€™s for. They, like traits, ALSO serve as semantic bridges between otherwise unrelated Thing Words that share a common purpose, and they enable functional traversal through the graph. Unlike Traits, which describe inherent qualities, Roles are about external intent or effect.

Critically, not every Thing Word has a salient role. Roles are only included for Thing Words whose function is a core part of their identity â€” e.g., â€œScissorsâ€ (cutting), â€œGuruâ€ (guiding), â€œDogâ€ (companionship). This helps avoid overfitting or forcing a purpose where none meaningfully exists.

1. Selective Role Generation During Growth  
    â€¢ In Phase 1, every Thing Word is evaluated for whether it warrants Roles at all.  
    â€¢ Only if a functional use is central to its meaning does the system generate 7 candidate Role labels (e.g. â€œcutting,â€ â€œhealing,â€ "companionship," "guidance" etc). Be picky about which words Role Words are generated for.  
    â€¢ These role strings are stored in the Thingâ€™s metadata under a `purposes` array, alongside traits and acquaintances.  
    â€¢ Many Thing Words â€” like â€œCloudâ€ or â€œBeautyâ€ â€” may have no roles at all, and thatâ€™s fine.

2. Canonicalization & Filtering  
    â€¢ After tree growth, all raw purpose strings are normalized:
        â€¢ Lowercased, singularized, deduplicated
        â€¢ Merged using embeddings and lexical synonym rules (e.g. â€œcuttingâ€ â‰ˆ â€œslicingâ€?)
    â€¢ Any Role label found in â‰¥2 distinct Thing Words is promoted to become a Role Word.
    â€¢ Roles with only one source are discarded â€” this avoids cluttering the graph with overly narrow or redundant concepts.

3. Role Word Structure  
    â€¢ Like Traits, Role Words are stand-alone bridge nodes. They have:
        â€¢ `exemplars`: all Thing Words that listed the Role in their metadata  
        â€¢ `acquaintances`: optional â€” symbolic or functional associates of the Role  
    â€¢ They do not have children or parents, and are not part of the parent-child hierarchy.

4. Acquaintance Handling for Roles  
    â€¢ Role Words can have their own symbolic or functional acquaintances (e.g. â€œCuttingâ€ â†’ Sharpness, Blade, Precision).
    â€¢ These are handled using the same adoption strategy as for Thing Words:  
        â€¢ If the acquaintance exists â†’ link it  
        â€¢ If not â†’ adopt it under a semantically suitable parent

5. Dynamic Definition Rendering  
    â€¢ Role definitions are written using:
        â€¢ Exemplars (e.g. â€œScissors,â€ â€œRazor,â€ â€œKnifeâ€)
        â€¢ Acquaintances (e.g. â€œSeparation,â€ â€œEdge,â€ â€œBladeâ€)
    â€¢ These produce natural-language definitions like:
        "Cutting is a purposeful act of separating material using an edge or blade. It's a role performed by tools like scissors, razors, and knivesâ€”each designed for precision and sharpness."

    â€¢ Abstract or social Roles (e.g. â€œCompanionship,â€ â€œHealingâ€) follow the same pattern, using emotionally resonant acquaintances:
        "Companionship is the role of offering presence, loyalty, and mutual connection. It's fulfilled by dogs, friends, and partners, often tied to ideas of warmth, trust, and shared life."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ§  Why This Matters

â€¢ Role Words introduce intent, use, and social function into the semantic map â€” enabling movement not just by category or trait, but by what things are for.
â€¢ Roles are especially important for tools, social beings, professions, and systems with an active function.
â€¢ But by requiring functional salience (i.e. only assigning Roles when they're central), the game avoids artificial or bloated metadata.
â€¢ Roles give players a new strategic traversal dimension: function.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ Notes

â€¢ Roles are only generated when the Thing Word is clearly purpose-centered.  
â€¢ Role Words do not have parents or children.  
â€¢ Their definitions are constructed from exemplars and acquaintances â€” not hierarchy.  
â€¢ They may have their own `acquaintances`, which flow through the same Phase 3 adoption process.  
â€¢ This makes them feel consistent with the system while enabling a distinct mode of navigation.




/// STILL NEEDS TO INCLUDE ROLES                                                
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
â–—â–„â–„â–– â–—â–– â–—â––â–—â–„â–„â–„â––â–—â––   â–—â–„â–„â–„     â–—â–„â–„â–„â––â–—â––    â–—â–„â–– â–—â–– â–—â––    â–—â–„â–„â–„â––â–—â–„â–„â–„ â–—â–„â–„â–„â–– â–—â–„â–– 
â–â–Œ â–â–Œâ–â–Œ â–â–Œ  â–ˆ  â–â–Œ   â–â–Œ  â–ˆ    â–â–Œ   â–â–Œ   â–â–Œ â–â–Œâ–â–Œ â–â–Œ      â–ˆ  â–â–Œ  â–ˆâ–â–Œ   â–â–Œ â–â–Œ
â–â–›â–€â–šâ––â–â–Œ â–â–Œ  â–ˆ  â–â–Œ   â–â–Œ  â–ˆ    â–â–›â–€â–€â–˜â–â–Œ   â–â–Œ â–â–Œâ–â–Œ â–â–Œ      â–ˆ  â–â–Œ  â–ˆâ–â–›â–€â–€â–˜â–â–›â–€â–œâ–Œ
â–â–™â–„â–â–˜â–â–šâ–„â–â–˜â–—â–„â–ˆâ–„â––â–â–™â–„â–„â––â–â–™â–„â–„â–€    â–â–Œ   â–â–™â–„â–„â––â–â–šâ–„â–â–˜â–â–™â–ˆâ–Ÿâ–Œ    â–—â–„â–ˆâ–„â––â–â–™â–„â–„â–€â–â–™â–„â–„â––â–â–Œ â–â–Œ
BUILD FLOW IDEA
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

I know this section seems very specific, but this is the overall idea for how to actually build all this. In practice, perhaps there are some things that need to be tweaked, or a few mistakes in here, or maybe it's not fully robust enough. But this is a good reference for the overall idea I'm thinking about for how to build flow, and at the very least, maybe it can serve as inspiration for the true strategy that might devel in the process of building it. Don't take it as gospel, use your own intelligence to really ensure everything will work, but use this as solid inspiration of how we might go about this.


---------------------

ğŸŒ³ PHASE 1: CORE TREE GROWTH

Goal: Grow a cohesive parentâ†’child taxonomy of main Thing Words (parents & children), while logging raw traits & acquaintances (no promotions yet).

1. Initialize Artifacts  
   ğŸ² master_words.json â†  
      [  
        {  
          word: "Thing",  
          type: "thing",  
          parent: null,  
          children: [],  
          traits: [],  
          acquaintances: [],  
          stages: {  
            childrenDone: false,  
            rawLogged:  false  
          }  
        }  
      ]  
   ğŸ“ raw_traits.csv        â† empty (columns: word,trait)  
   ğŸ“ raw_acquaintances.csv â† empty (columns: word,acquaintance)  

2. BFS Expansion Loop  
   â³ targetCount â† 2000  
   â¡ï¸ queue â† ["Thing"]  

   While (queue not empty) AND (master_words.json.length < targetCount):  
     a. Dequeue â†’ currentWord  
     b. Lookup record = master_words.json.find(w==currentWord)  
        â€¢ If record.stages.childrenDone == true â†’ continue  
     c. Build exclusion lists:  
        â€¢ parentTerm   = record.parent  
        â€¢ childTerms   = record.children  
        â€¢ traitTerms   = raw_traits.csv.filter(w==currentWord).map(t)  
        â€¢ synonymTerms = synonyms_of(currentWord)  # e.g. WordNet lookup  
     d. **Generate via three separate LLM calls (GPT-4 Turbo)**:  
        1) **Children Prompt**  
           â€œList exactly 7 common subtypes of [currentWord], ranked by commonality.  
            Return 7 comma-separated nouns only.â€  (actual prompt in prompts/children generation.txt)
           â†ªï¸ Retry up to 2Ã— if countâ‰ 7 or format invalid.  
        2) **Traits Prompt**  
           â€œList exactly 7 adjectives most people associate with [currentWord], ranked by frequency.  
            Return 7 comma-separated words only.â€   (actual prompt in prompts/trait generation.txt)
           â†ªï¸ Retry if needed.  
        3) **Acquaintances Prompt**  
           â€œList exactly 7 nouns that co-occur with [currentWord] in thought/experience,  
            excluding [parentTerm], [childTerms], [traitTerms], [synonymTerms].  
            Return 7 comma-separated nouns only.â€  (actual prompt in prompts/acquaintance generation.txt)
           â†ªï¸ Retry if exclusions appear or countâ‰ 7.  
     e. **Mark children as done**  
        â€¢ record.stages.childrenDone = true  
        â€¢ Persist master_words.json  
     f. **Normalize & Deduplicate Children**  
        For each childCandidate:  
          norm = lowercase(singularize(childCandidate))  
          if (!master_words.json.find(w â‡’ normalize(w.word)==norm)):  
            â€“ append newRecord {  
                word: childCandidate, type:"thing", parent:currentWord,  
                children:[], traits:[], acquaintances:[],  
                stages:{ childrenDone:false, rawLogged:false }  
              }  
            â€“ enqueue childCandidate  
          if (childCandidate âˆ‰ record.children):  
            â€“ record.children.push(childCandidate)  
     g. **Write Raw Metadata**  
        â€¢ append (currentWord, trait) â†’ raw_traits.csv  
        â€¢ append (currentWord, acq)   â†’ raw_acquaintances.csv  
     h. **Mark raw logging as done**  
        â€¢ record.stages.rawLogged = true  
        â€¢ Persist master_words.json  
     i. **Throttle & Back-off**  
        â€¢ Sleep 50â€“100 ms between calls  
        â€¢ On HTTP 429 â†’ exponential back-off  

3. Output after Phase 1  
   âœ… master_words.json (~2 000 entries), each:  
     {  
       "word":         "Cat",  
       "type":         "thing",  
       "parent":       "Animal",  
       "children":     ["Siamese","Tabby",â€¦],   # 7 items  
       "traits":       [],                      # placeholders  
       "acquaintances": [],                     # placeholders  
       "stages": {                              
         "childrenDone": true,  
         "rawLogged":    true  
       }  
     }  
   âœ… raw_traits.csv & raw_acquaintances.csv capture every (word,trait)/(word,acq) pair  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ›  TECH: Model, Why & Cost Estimate  
   â€¢ Model: **GPT-4 Turbo**  
     â€“ Crisp list-only output, massive 128 K token window, ~Â½ GPT-4o cost  
   â€¢ Cost Estimate: 6 000 calls Ã— ~100 tokens â‰ˆ 600 000 tokens â†’ â‰ˆ\$2.55 total for Phase 1  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ”„ Resumable & Idempotent Processing  
â€¢ All state lives in master_words.json under each nodeâ€™s `stages` flags.  
â€¢ On crash/restart, rebuild queue from any words where either flag == false.  
â€¢ No separate â€œseenâ€ set neededâ€”JSON is the single source of truth.  

ğŸš‘ Crash Recovery & Re-Runs  
â€¢ Safe to re-run; uncompleted nodes pick up exactly where they left off.  
â€¢ A small â€œresetâ€ script can clear all `stages` flags if you want a fresh run.  

âœ… Benefits  
â€¢ Guarantees no half-finished nodes are skipped or re-processed.  
â€¢ Workflow state & data remain tightly coupled.  
â€¢ Fully crash-safe, retry-safe, and transparent to any maintainer.  


ğŸ”— LINKING STRATEGY (applies from Phase 1 onward)
	â€¢	Front-end should dynamically render any word found in a nodeâ€™s:
â€¢ parent
â€¢ children
â€¢ traits
â€¢ acquaintances
as a clickable link in the definition.
	â€¢	These are guaranteed to exist in the metadata regardless of whether the node has received a Phase 4 natural-language definition or is still showing raw JSON metadata.
	â€¢	This eliminates the need to insert markup or hyperlinks into the raw text returned by the model.



------------------------------------



ğŸ§¬ PHASE 2: TRAIT SYNONYM NORMALIZATION & PROMOTION  

Goal: Collapse redundant raw adjectives into canonical Trait Words, then promote only those shared by â‰¥ 2 Thing Words â€” all in a resumable, idempotent pass.

1. Load Raw Traits  
   ğŸ“ Read `raw_traits.csv` â†’ list of `(word, rawTrait)` pairs.  

2. Ensure Stage Flag  
   ğŸ”– In `master_words.json`, every node must have:  
     `"stages": { â€¦, "traitsPromoted": false }`  

3. Preprocess & Cluster Labels  
   a. **Label Cleanup**  
      â€¢ Lowercase, strip punctuation, singularize/pluralize.  
      â€¢ Build unique set `allRawTraits`.  
   b. **Embedding Clustering**  
      â€¢ Compute embeddings for each label (e.g. `text-embedding-ada-002`).  
      â€¢ Run agglomerative clustering at similarity threshold ~0.8.  
   c. **Synonym Merge**  
      â€¢ Within each cluster, use WordNet or a custom map to merge obvious equivalents.  

4. Select Canonical Labels  
   ğŸ”„ For each cluster:  
     â€¢ Choose the label with highest raw frequency (tie â†’ shortest) â†’ `canonicalTrait`.  
     â€¢ Build map `rawToCanonical[rawTrait] = canonicalTrait`.  

5. Normalize & Attach  
   ğŸ”„ For each `(word, rawTrait)` in `raw_traits.csv`:  
     â€¢ `canonical = rawToCanonical[rawTrait]`  
     â€¢ Append `(word, canonical)` to an in-memory list or `traits_normalized.csv`.  

6. Count & Promote Traits  
   â¡ï¸ Group normalized pairs by `canonical`, count **distinct** words:  
     â€¢ If count â‰¥ 2 **and** for all exhibitor nodes `stages.traitsPromoted == false`:  
       a. Add a Trait node in `traits_master.json`:  
          ```json
          {
            "word":          "<canonicalTrait>",
            "type":          "trait",
            "exhibitors":    [/* sorted list of Thing Words */],
            "related_traits": []
          }
          ```  
       b. For each exhibitor in `master_words.json`, push `<canonicalTrait>` into its `traits` array.  
       c. Set each exhibitorâ€™s `stages.traitsPromoted = true`.  

7. Persist & Resume Safety  
   â€¢ After each cluster promotion, write out updated `traits_master.json` and `master_words.json`.  
   â€¢ On restart, skip any Thing Word where `stages.traitsPromoted == true`.  

8. Output after Phase 2  
   âœ… `traits_master.json` â€” final Trait Word definitions.  
   âœ… Updated `master_words.json` â€” all Thing Words now list canonical traits.  


ğŸ›  TECH: Approach, Why & Cost Estimate  

â€¢ **Embedding Model**  
  â€“ Model: `text-embedding-ada-002`  
  â€“ Why: Fast, cost-efficient semantic vectors ideal for clustering hundreds of short labels.  
  â€“ Cost: $0.0004 per 1K tokens â†’ ~500 labels Ã— 1 token â‰ˆ 500 tokens â†’ **\$0.0002** total.  

â€¢ **Clustering & Synonym Merge**  
  â€“ Library: scikit-learnâ€™s AgglomerativeClustering (no API cost).  
  â€“ Synonym map via WordNet or custom dictionary (zero extra cost).  

â€¢ **LLM Calls**  
  â€“ **None required** in Phase 2â€”everything runs locally after embeddings.  
  â€“ (Optional) GPT-4 Turbo validation: ~100 tokens/cluster Ã— ~50 clusters â‰ˆ 5 000 tokens â†’ **\$0.02**.  

â€¢ **Total Cost Estimate**  
  â€“ Embeddings pass: **\$0.0002**  
  â€“ Optional cluster-validation: **\$0.02**  
  â€“ **Grand Total:** < \$0.05  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ” **Key Essentials**  
1. **Idempotency via `traitsPromoted` flags**  
2. **Embedding-based clustering** (threshold ~0.8)  
3. **Promote only traits â‰¥ 2 exhibitors**  
4. **Persistent writes after each promotion**  
5. **Resumable on crash/restart**  

These steps ensure Phase 2 is reliable, repeatable, and crash-safe.  

------------------------------------

ğŸ¤ PHASE 3: ACQUAINTANCE ADOPTION & INTEGRATION

Goal: Ensure every â€œAcquaintanceâ€ word hangs from the main taxonomy rather than dangle as an orphan, while preserving the natural, organic hierarchy we built in Phase 1. We use an embeddings-to-shortlist + LLM final vote approach to guarantee both high coverage and semantic precision.

1. Load Raw Acquaintances  
   ğŸ“ Read `raw_acquaintances.csv` â†’ list of `(sourceWord, acqTerm)` pairs.  
   â€£ Rationale: Having all logged pairs in memory lets us see which Thing Words reference each acquaintance.  

2. Build Unique Acquaintance Set  
   ğŸ¯ `allAcqs = unique(acqTerm)`  
   â€£ Rationale: Deduplicating here avoids redundant workâ€”each new term only needs a parent assigned once.  

3. Seed Orphan Placeholders  
   For each `acq` in `allAcqs`:  
     â€¢ If `acq` already exists in `master_words.json` â†’ skip.  
     â€¢ Else â†’ append placeholder node to `master_words.json`:  
       ```json
       {
         "word":          "<acq>",
         "type":          "thing",
         "parent":        null,
         "children":      [],
         "traits":        [],
         "acquaintances": [],
         "stages":        { /* inherit or initialize flags */ }
       }
       ```  
     â€£ Rationale: Placeholders let us assign parents in bulk before wiring up lateral links.  

4. Prepare Embeddings Search  
   â€¢ `parentCandidates = master_words.json.map(w â‡’ w.word + " â€” parent:" + w.parent + "; children:" + w.children.slice(0,3).join(", "))`  
   â€¢ Compute embeddings for each extended candidate string **and** for each raw `acq` term using `text-embedding-ada-002`.  
   â€£ Rationale: Embedding the word plus minimal context (parent, a few children) biases similarity toward true categories.  

5. Build Shortlists via Nearest Neighbors  
   For each orphan `acq`:  
     a. Compute cosine similarity vs. all `parentCandidates`.  
     b. Select top 20 most similar candidates â†’ `shortlist[acq]`.  
   â€£ Rationale: A top-20 shortlist almost always contains the correct parentâ€”cuts LLM calls by ~95%.  

6. LLM-Guided Final Parent Selection  
   For each `acq` + its `shortlist`:  
     â€¢ Prompt GPT-4 Turbo:
       ```
       I have 20 potential parent categories for â€œ[acq]â€:
       â€¢ cand1, cand2, â€¦, cand20

       Which single category best serves as the parent for â€œ[acq]â€?  
       Reply with exactly one choice from the list.
       ```  
     â€¢ If response âˆ‰ shortlist â†’ retry once.  
     â€¢ Assign `chosenParent = response`.  
   â€£ Rationale: LLM picks the most precise fit from a semantically-filtered setâ€”minimizing hallucination.  

7. Persist Parent Assignments (Bidirectional)  
   â€¢ Update each orphan nodeâ€™s `parent = chosenParent` in `master_words.json`.  
   â€¢ Then locate the `chosenParent` node and push `acq` into its `children` array (if not already present).  
   â€¢ Write out updated JSON.  
   â€£ Rationale: Ensures bidirectional consistencyâ€”parents know their children and vice versa.  

8. Attach Acquaintance Edges  
   For each `(sourceWord, acq)` in `raw_acquaintances.csv`:  
     â€¢ Locate `sourceWord` node â†’ push `acq` onto its `acquaintances` array (if missing).  
   â€¢ Persist `master_words.json`.  
   â€£ Rationale: Now every Thing Word correctly references its lateral connections.  

9. (Optional) Prune Duplicate Acquaintances  
   â€¢ Cluster near-identical nodes (e.g. â€œColorâ€ vs. â€œColourâ€), merge them, rewrite links.  
   â€£ Rationale: Keeps graph cleanâ€”skip unless obvious duplicates appear.  

10. Output after Phase 3  
   âœ… `master_words.json` updated:  
     â€“ Every orphan â€œThing Wordâ€ has a valid `parent`.  
     â€“ Every original Thing Wordâ€™s `acquaintances` array is filled.  
     â€“ Every adopted child is correctly listed in its parentâ€™s `children` array.  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ›  TECH: Hybrid Approach, Why & Cost  

â€¢ **Embeddings Pass**  
  â€“ Model: `text-embedding-ada-002`  
  â€“ Purpose: Build a shortlist >95% likely to include true parent.  
  â€“ Cost: ~$0.0004 per 1K tokens â†’ 2 000 candidates + contexts â‰ˆ\$0.002 total.  

â€¢ **Nearest-Neighbor Search**  
  â€“ Library: Faiss or `sklearn.neighbors` (no API cost).  
  â€“ Purpose: Quickly retrieve top-K similar candidates per orphan.  

â€¢ **LLM Final Vote**  
  â€“ Model: GPT-4 Turbo  
  â€“ Prompt size: ~100 tokens/orphan â†’ 2 000 orphans â†’ 200 000 tokens.  
  â€“ Cost: 200 000 tokens Ã— \$0.004/token â‰ˆ **\$0.80**.  

â€¢ **Total Phase 3 Estimate**  
  â€“ Embeddings: ~\$0.002  
  â€“ LLM calls: ~\$0.80  
  â€“ **Grand Total:** ~\$0.80  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ” Why This Matters  

1. **Semantic Integrity**  
   â€“ Context-aware embeddings bias toward valid parent categories.  
   â€“ LLM then picks the exact best fitâ€”avoiding misclassification.  

2. **Full Graph Connectivity**  
   â€“ No more dangling wordsâ€”all links traverse cleanly.  
   â€“ New acquaintances adopt into the structure naturally.  

3. **Bidirectional Consistency**  
   â€“ Parents point to their new children.  
   â€“ Definitions and UI reflect a fully wired web.  

4. **Idempotency & Crash Safety**  
   â€“ Skip any orphan whose `parent != null`.  
   â€“ Clear stages logic ensures safe restarts with no duplicated work.  

With this hybrid approach, your taxonomy remains coherent, intuitive, and fully connectedâ€”ready for the playerâ€™s logical and poetic journeys.  


------------------------------------



ğŸ—£ï¸ PHASE 4: NATURAL-LANGUAGE DEFINITION PASS

Goal: Turn each wordâ€™s structured metadata (parent, children, traits, acquaintances, purposes) into a clean, human-readable definition that feels organic and engaging to players. This adds personality and immersion while preserving all navigational affordances.

1. Gather All Nodes  
   ğŸ” Load `master_words.json` and `traits_master.json` into memory.  
   â€¢ Includes all Thing Words and promoted Trait Words.  

2. Prepare Prompt Template  
   âœï¸ Use a system + user prompt (see: `prompts/Natural Language.txt`) that:  
     â€¢ Accepts a full metadata object for each word  
     â€¢ Generates a sentence wikipedia-esque definition  
     â€¢ References the wordâ€™s parent category  
     â€¢ Weaves in up to X children, Y traits, and Z acquaintances (as the dev, you can decide how many of each to include)  
     â€¢ Ends with: â€œUse no additional terms beyond whatâ€™s given.â€  
   â€£ This ensures coverage of every key relationship while minimizing hallucination or drift.

3. Batch & Generate  
   âš¡ï¸ Process definitions in batches of 10â€“20 words per API call:  
     â€¢ Send prompt + batch of JSON nodes to GPT-4 Turbo  
     â€¢ Receive a list of `{ word, definition }` responses  
     â€¢ Validate structure before saving  
   â€£ Batching reduces per-call overhead and keeps costs predictable.

4. Validate Output  
   âœ”ï¸ For each generated definition:  
     â€¢ Confirm the main word appears in its own definition  
     â€¢ Optionally run a keyword coverage script to ensure presence of required metadata (parent, â‰¥1 trait, etc.)  
     â€¢ Spot-check ~10% of definitions for quality  
   â€£ Prevents silent errors or unusable definitions from slipping into the final UI.

5. Persist Definitions  
   ğŸ’¾ Write to `definitions_master.json` in this format:  
     ```json
     [
       {
         "word": "Cat",
         "definition": "Cats are agile mammals known for their independence and graceful movements. They are often kept as pets and associated with traits like curiosity and nocturnality. Related ideas include dogs, litter boxes, and scratching posts."
       },
       â€¦
     ]
     ```
   â€£ Easy to look up and integrate on the front-end.

6. Linkability Support  
   ğŸ”— All terms in the generated definitions that match a known Thing Word or Trait Word should be linkable in the UI.  
   â€¢ Donâ€™t encode links in the definition text itself  
   â€¢ Instead, the front-end should match visible tokens against each wordâ€™s known metadata (parent, children, traits, acquaintances)  
   â€¢ If matched, style and bind the word as a clickable hyperlink  
   â€£ This allows for dynamic, robust linking even if the natural-language wording varies.

7. Optional: Regeneration Support  
   ğŸ”„ To incrementally re-render definitions later:  
     â€¢ Track a `definitionLastUpdatedAt` timestamp per word  
     â€¢ Only re-run the LLM if its metadata has changed since that time

8. Output after Phase 4  
   âœ… `definitions_master.json` with natural-language definitions for every Thing and Trait Word  
   âœ… Front-end now supports immersive, readable traversal without breaking linkability

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ›  TECH: Model, Cost & Format

â€¢ Model: GPT-4 Turbo  
  â€“ Cost: $0.003 / 1K tokens  
  â€“ Easily fits ~20 metadata objects + prompt + output in 128K context  

â€¢ Token & Cost Estimate  
  â€“ Avg: ~200 tokens per word (input + output)  
  â€“ 10,000 words = ~2M tokens â†’ $6  
  â€“ 50,000 words â†’ ~$30  
  â€“ 500,000 words â†’ ~$300  

â€¢ Batch Size  
  â€“ 10â€“20 words per request recommended  
  â€“ Keeps response latency low and throughput high

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ“Œ Why This Matters

â€¢ ğŸš€ Player Experience: Adds clarity, narrative, and intuition to each node  
â€¢ ğŸ” Future-Proof: Easy to re-run when definitions change  
â€¢ ğŸ§  Cognitive Glue: Makes the graph feel alive and explorable instead of abstract  
â€¢ ğŸ§© Front-End Ready: UI link logic is metadata-driven, not tied to string parsing



---------------

========================
TLDR: THE OVERALL STRATEGY
========================                                                

We grow a living word graph in four idempotent, crash-safe stages:
	1.	Phase 1: Core Tree Growth
Starting from a single root â€œThing,â€ we breadth-first generate exactly seven children, seven traits, and seven acquaintances per node via GPT-4 Turbo. Children become new nodes; traits and acquaintances are logged for later. Each node tracks completion flags so you can pause, crash, or resume without losing progress. At the end, you have ~2 000 Thing-Words with a bulletproof parentâ†’child backbone plus raw trait and acquaintance logs.
	2.	Phase 2: Trait Normalization & Promotion
We collapse redundant adjectives into canonical Trait-Words using fast, local embedding clustering (plus a simple WordNet / synonym pass) and only promote traits that appear on two or more Thing-Words. No further LLM calls are neededâ€”your trait layer organically emerges from the data at almost zero cost.
	3.	Phase 3: Acquaintance Adoption
Every raw acquaintance is first seeded as a placeholder node, then slotted under a real parent via a hybrid approach: compute context-enhanced embeddings for each candidate, retrieve a top-20 shortlist per orphan, and finally ask GPT-4 Turbo to pick the single best parent. After that, you link each source word back to its newly adopted acquaintances. The result is a fully connected, intuitive network with zero dangling nodes.
	4.	Phase 4: Natural-Language Definitions
With metadata locked in, we batch out 2â€“3-sentence encyclopedia-style definitions for every Thing and Trait Wordâ€”via GPT-4 Turboâ€”by feeding each nodeâ€™s full JSON (parent, children, traits, acquaintances, etc.) into a consistent prompt template. We validate output, persist to definitions_master.json, and let the front-end dynamically hyperlink any known keyword back into the graph. This final pass costs only a few dollars for thousands of words and gives players rich, linkable definitions that bring the entire semantic ecosystem to life.





---------



//needs work
-----------------------------------------
ğŸ‘©â€ğŸ’» DEVELOPER / DESIGNER USER EXPERIENCE
-----------------------------------------

Goal: to make it as easy as possible for me, the designer/developer, to:
1. Be able to tweak parameters to see how differnet it makes the game feel, with the ability to save configurations as "files", name them, and load them back up. One such parameter I want to be able to shift is the number of children, traits, acquaintances, and purposes per node (Remove from the end of each collection of words]. Another such change would be the vertical granularity of the graph, so I can dial the complexity down or up, which would in turn remove 'gradations' in between the graph, so I can test the game with different levels of detail and complexity.
2. Be able to easily run each phase of the build pipeline
3. Be able to get clear, transparent logs progress indicators and other real-time progress indicators and feedback as I run the phases, so I'm not in the dark as to what is happening
4. Be able to easily and intuitively test and see the output of a meaningful sample of each, so I can see the direction it's going in before committing to large LLM calls or expensive tasks.
5. Be able to, if I choose to, view a verbose logging and audit trail just in case, so I can always ask "Why didn't X?" happen and have a method to diagnose the issues and look back
6. have a resumable & idmpotent process, so I can pause, crash, or resume without losing progress, unless I explicitly ask to remove all progress.
7. Be able to easily change the prompts being used by switching the text of the .txt files in the Prompts folder
8. To see a default preset definition of each word using static text + variables, so I can see a rough approximation of what the game feels like even before doing the Natural Language Pass. (Ex. [word] is a type of 'parent' that is known for [traits]. Often, [word] may be connected in some way to [acquaintances]. Types of [word] may include[children], etc]
Every phase is a self-contained script/app you can launch from the repo root.  
9. To be able to work with dummy data and see how game looks with it
10. To be able to contduct a "sandbox test" or dryrun to simulate each prase without writing to disk



---------------------------------------------------------------------
ğŸ‘©â€ğŸ’» IDEAS FOR HOW TO IMPLEMENT THE ABOVE (just ideas to be inspired by)
----------------------------------------------------------------------

1. CONFIGURATION MANAGEMENT  
   â€¢ Store all adjustable parameters in a versioned `config` folder (e.g. `config/`), with JSON files you can name, save, and load.  
     â€“ e.g. `config/fast-test.json`, `config/full-run.json`  
     â€“ Override defaults by passing `--config myfile.json` to any script.  
   â€¢ Exposed parameters include: childrenPerNode, traitsPerNode, acqsPerNode, purposesPerNode, graphDepth, batch sizes, dryRun, etc.  
   â€¢ Dial â€œvertical granularityâ€ by adjusting graphDepth or a minParentDepth threshold to collapse intermediate nodes.  

2. ONE-COMMAND ENTRYPOINTS  
   â€¢ Each phase is a standalone CLI script in `scripts/`â€”e.g.  
     â€“ `scripts/phase1.js` â†’ npm run phase1  
     â€“ `scripts/phase2.js` â†’ npm run phase2  
     â€“ `scripts/phase3.js` â†’ npm run phase3  
     â€“ `scripts/phase4.js` â†’ npm run phase4  
   â€¢ Pass `--config` and other flags directly.  

3. REAL-TIME PROGRESS & FEEDBACK  
   â€¢ Terminal shows a live progress bar and summary line for each phase:  
     â€“ Phase1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘]  45% (900/2000) â€“ current="Vehicle"  
     â€“ Phase3: orphan="Shakespeare" shortlistReady(20/20)  
   â€¢ Warnings on errors or rate limits trigger automatic retry with exponential back-off and log entry.  

4. LIGHTWEIGHT SANDBOX & DRY-RUN  
   â€¢ `--dryRun` mode simulates the entire pipeline on a miniature data sample (e.g. 10 root nodes) without writing to disk.  
   â€¢ Use `config/test-sample.json` to surface one or two words, verify JSON outputs, inspect placeholder definitions, and stop before costly LLM calls.  
   â€¢ Swap real data for dummy fixtures by passing `--dataSample dummy/` if you want full pipeline logic on synthetic inputs.  

5. EASY SAMPLE INSPECTION  
   â€¢ After any phase, run `npm run sample -- phaseX` to print a curated subset of outputs:  
     â€“ Phase1 sample: list 5 generated nodes + their children/traits/acqs  
     â€“ Phase3 sample: show 3 orphans with shortlists and chosen parents  
   â€¢ This helps you preview outcomes before full-scale runs or LLM billing.  

6. VERBOSE LOGGING & AUDIT TRAIL  
   â€¢ By default, each run writes to `/logs/phaseX_YYYYMMDD.log` with timestamps, config snapshot, retry counts, token usage, and summary metrics.  
   â€¢ Enable `--verbose` to include full request/response bodies for edge-case debugging.  

7. RESUMABLE & IDEMPOTENT  
   â€¢ All scripts respect per-node `stages` flags in JSON data stores.  
   â€¢ Kill and restart any phase; it picks up where it left off.  
   â€¢ Pass `--resetPhase X` to clear only that phaseâ€™s flags if you truly want to re-run from scratch.  

8. PROMPT & MODEL SWAPPABILITY  
   â€¢ All LLM and embedding calls read from `prompts/` and `utils/embeddings.js`.  
   â€¢ Alter any `.txt` prompt file or swap model names in one config fieldâ€”scripts automatically use the updated text on next run.  

9. PRESET â€œROUGHâ€ DEFINITIONS  
   â€¢ When Phase4 has not yet run, by default generate placeholder definitions from static templates:  
     â€“ â€œ[Word] is a type of [parent] known for [trait1, trait2]. Commonly linked to [acq1, acq2]. Examples include [child1, child2]. etc etc.â€  
   â€¢ Controlled by `config/defaultDefs.json`; run with `npm run previewDefs` to see them in the UI.  

10. INTERACTIVE DEBUGGING SHELL  
    â€¢ `npm run phase1:inspect -- --node Banana` drops you into a REPL to:  
      â€“ Load record(`Banana`), view metadata, call `generateAcqs(record)`, etc.  
    â€¢ Supports one-off LLM prompt tests, embedding PCA, clustering thresholds, etc.  























