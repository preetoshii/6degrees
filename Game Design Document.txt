 
 
 
 
 
 
 
 
 
 
   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–„â–ˆ  â–€â–ˆâ–ˆâ–ˆâ–ˆ    â–â–ˆâ–ˆâ–ˆâ–ˆâ–€      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„     â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„     â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 
  â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–Œ   â–ˆâ–ˆâ–ˆâ–ˆâ–€       â–ˆâ–ˆâ–ˆ   â–€â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ 
  â–ˆâ–ˆâ–ˆ    â–ˆâ–€  â–ˆâ–ˆâ–ˆâ–Œ    â–ˆâ–ˆâ–ˆ  â–â–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–€  
  â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆâ–Œ    â–€â–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–€         â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ  â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„      â–„â–ˆâ–ˆâ–ˆ         â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–ˆâ–ˆâ–€  â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„      â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„       â–ˆâ–ˆâ–ˆ        
â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–Œ    â–ˆâ–ˆâ–ˆâ–ˆâ–€â–ˆâ–ˆâ–„          â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€     â–€â–€â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–„  â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€â–€â–€   â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€     â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€     â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 
         â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–â–ˆâ–ˆâ–ˆ  â–€â–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–„    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–„    â–ˆâ–ˆâ–ˆ    â–ˆâ–„           â–ˆâ–ˆâ–ˆ 
   â–„â–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–„â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆâ–„       â–ˆâ–ˆâ–ˆ   â–„â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ    â–„â–ˆ    â–ˆâ–ˆâ–ˆ 
 â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€  â–ˆâ–€   â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–„      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€  
                                                                              â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ                                        
                                                                                                                          









~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 â–—â–„â–– â–—â––  â–—â––â–—â–„â–„â–„â––â–—â–„â–„â–– â–—â––  â–—â––â–—â–„â–„â–„â––â–—â–„â–„â–„â––â–—â–– â–—â––
â–â–Œ â–â–Œâ–â–Œ  â–â–Œâ–â–Œ   â–â–Œ â–â–Œâ–â–Œ  â–â–Œ  â–ˆ  â–â–Œ   â–â–Œ â–â–Œ
â–â–Œ â–â–Œâ–â–Œ  â–â–Œâ–›â–€â–€â–˜â–â–›â–€â–šâ––â–â–Œ  â–â–Œ  â–ˆ  â–â–›â–€â–€â–˜â–â–Œ â–â–Œ
â–â–šâ–„â–â–˜ â–â–šâ–â–˜ â–â–™â–„â–„â––â–â–Œ â–â–Œ â–â–šâ–â–˜ â–—â–„â–ˆâ–„â––â–â–™â–„â–„â––â–â–™â–ˆâ–Ÿâ–Œ

~ OVERVIEW ~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


========================
ABOUT
========================
Six Degrees is a game about word associations. It's a poetic-strategic exploration game where players traverse a network of interconnected words (like a Wiki) to reach a target word in the fewest steps possible. The game's structure is a hybrid between a logical taxonomy (like a tree of concepts) and a metaphorical social web (a network of associations). Each move is a click on a word within a definition, which reveals that word's own definition and related words. Every click forward counts as a step.


========================
GOAL OF THE GAME
========================
The objective is simple: reach the destination word in the fewest possible steps. The fewer the steps, the better your score. Six Degrees is both a reflective puzzle and a meditative journey â€” a game that rewards thoughtful connections and 'scalable / zoomable' traversable logic.


========================
CORE GAMEPLAY LOOP
========================
1. The player is shown a starting word and a destination word.
2. Clicking the starting word opens its natural-language definition.
3. In the definition, certain words are clickable: these may be parents, children, acquaintances, traits, or roles.
4. Clicking on a linked word opens that word's definition.
5. Every click forward increments the player's step count.
6. The goal is to reach the destination word in the least number of steps possible.

Polished features:
- Show visited path as a breadcrumb trail (Backtracking rewinds time: it removes the steps taken after that point, encouraging experimentation and puzzle-solving without penalty)
- Make it a "daily" game, with a new starting word and destination word each day. This is in sync globally, but allow a "Free Play" mode as well.
- Add a "Share" button once you finish to share a screenshot of your score with your friends, along with a link to the game, to encourage virality.
- Add a "Something missing in this definition?" button to allow players to suggest new words to add to the graph, to be used in later iteration of the game
- Add difficulty slider to allow players to choose how far away the starting word is from the destination word



========================
ENVISIONED PLAYER STRATEGIES
========================
There are two core player archetypes:

The Logician:
- Navigates up to parent categories to access siblings
- Uses structured inference: "If cat is an animal, and dog is too, I can get to dog by going up to animal first"
- Values hierarchy, clarity, and precision

The Poet:
- Follows acquaintances, traits, and roles through contextual resonance
- Moves by metaphor, emotion, cultural vibe: "Apple reminds me of knowledge... or Steve Jobs"
- Values surprise, meaning, and personal logic

Both Modes Can Succeed:
Most players will mix both styles during a single game.

Example Path: Destination = Stop Sign
Logician's path (6 steps): Cat â†’ Animal â†’ Human â†’ Civilization â†’ Rules â†’ Traffic Sign â†’ Stop Sign
Poet's path (6 steps): Cat â†’ Night â†’ Street â†’ City â†’ Crosswalk â†’ Red Light â†’ Stop Sign

Both are valid. Both are beautiful. I hypothesise that making the best of both results in the highest score.




========================
DIFFICULTY TUNING
========================
Difficulty is tuned by controlling the number of steps the origin word is away from the destination.

- A random destination word is selected first
- Then, the game walks X steps away in a random direction through the existing graph to select an origin word. This ensures that the destination word is always reachable from the origin word.
- X = difficulty level
- Starting rule: All paths are built with 6 degrees of separation to match the game title
- Later, modes like "Easy (4 degrees)" or "Hard (8+)" can be introduced











~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
â–—â–– â–—â–– â–—â–„â–– â–—â–„â–„â–– â–—â–„â–„â–„     â–—â–„â–„â–„â––â–—â––  â–—â––â–—â–„â–„â–– â–—â–„â–„â–„â–– â–—â–„â–„â––
â–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–Œ  â–ˆ      â–ˆ   â–â–šâ–â–˜ â–â–Œ â–â–Œâ–â–Œ   â–â–Œ   
â–â–Œ â–â–Œâ–â–Œ â–â–Œâ–â–›â–€â–šâ––â–â–Œ  â–ˆ      â–ˆ    â–â–Œ  â–â–›â–€â–˜ â–â–›â–€â–€â–˜ â–â–€â–šâ––
â–â–™â–ˆâ–Ÿâ–Œâ–â–šâ–„â–â–˜â–â–Œ â–â–Œâ–â–™â–„â–„â–€      â–ˆ    â–â–Œ  â–â–Œ   â–â–™â–„â–„â––â–—â–„â–„â–

~ WORD TYPES ~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



                                                          
--.--|   ||,   .,---.    . . .,---.,---.,--. 
  |  |---|||\  ||  _.    | | ||   ||---'|   |
  |  |   ||| \ ||   |    | | ||   ||  \ |   |
  `  `   '``  `'`---'    `-'-'`---'`   ``--' 
(THING WORD)  


========================
WHAT IT IS
========================
- In Six Degrees, all primary nodes in the graph are called "Thing Words" â€” nouns that represent distinct entities, objects, systems, or concepts.
- These words are the most common in the game and serve as both origin and destination points in traversal.
- While all Thing Words share the same structural format, they may be be called "parent, child, sibling, or acquaintance" to describe how they relate to each other.
  - A thing word is called a **parent** when it represents the most precise broader category of a thing.
  - A thing word is called a **child** when it is a more specific example of another thing.
  - A thing word is called a **sibling** when it is a parallel member of the same category (purely used for thinking about the game development and design)
  - A thing word is called an **acquaintance** when it shares thematic, symbolic, or commonly associated meaning with another thing.



========================
DEFINITION SYSTEM
========================
Every Thing Word includes the following metadata, with its own parent (1), children (5), traits (5), acquaintances (5), and purposes if they exist (5).


{
  "word": "Cat",
  "type": "thing"
  "parent": "Animal",
  "children": [
    "Siamese",
    "Tabby",
    "Persian",
    "Maine Coon",
    "Sphynx",
    "Ragdoll",
    "Bengal"
  ],
  "traits": [
    "independent",
    "agile",
    "curious",
    "nocturnal",
    "graceful",
    "mysterious",
    "affectionate"
  ],
  "acquaintances": [
    "Dog",
    "Mouse",
    "Litter Box",
    "Laser Pointer",
    "Fur",
    "Scratching Post",
    "Couch"
  ],
  "purposes": [
    "Companion",
    "Pet"
  ]
}  

Note: I have only put X number of each type of word in this example, but it could be more or less.


========================
CHILD
========================
- Children are more specific examples, types, or subcategory 'thing words' generated from a parent
- Each child must satisfy the statement: "A [child] is a kind of [parent word]"
- In player traversal, Children support downward movement in abstraction and help players zoom into more concrete ideas
- They should be the MOST LIKELY thought of words that fits this crtieria, and put in order of most likely thought about (Not unlike the game of Jeopardy)

Examples:
- "Animal" â†’ Children: [in order of commonality] Dog, Cat, Bird, Fish, Horse, Cow, Elephant
- "Tool" â†’ Children: [in order of commonality] Hammer, Screwdriver, Wrench, Pliers, Saw, Drill, Tape Measure
- "Emotion" â†’ Children: [in order of commonality] Happy, Sad, Angry, Fear, Love, Surprise, Disgust



========================
PARENT
========================
- The Parent is the single most appropriate broader category that a a child word belongs to
- Unlike children, which are generated based on their parents, parents are never generated from children, as generation is a one-way process.
- In player traversal, clicking on a parent supports "upward" movement in abstraction and help players zoom out to more general words
- The first parent, which has no parent of its own, is our God Word: "Thing"

Examples:
- "Cat" â†’ Parent: Animal
- "Sadness" â†’ Parent: Emotion
- "Hammer" â†’ Parent: Tool
- "Democracy" â†’ Parent: System



========================
SIBLING
========================
- Siblings are words that share the same parent â€” they are parallel members of the same category.
- For example, "Dog" and "Cat" would both be siblings if they were the direct children of "Mammal." (FYI not sure if this example stands with the real word graph)
- Unlike parents, children, and acquaintances, siblings are never explicitly listed in a word's metadata. Instead, they are inferred by moving up to a shared parent and then down into another child. This design encourages discovery and strategy: a player might wish to find a sibling in order to traverse 'horizontally', and may "zoom out" to a shared parent and then "zoom in" to a different child.
- The concept of siblings is mentioned here simply because it's useful for thinking about game development, design, and traversal strategy, but is not a direct part of a thing word's metadata.

Examples:
- "Dog" and "Cat" â†’ Siblings (both potential children of "Mammal")
- "Hammer" and "Screwdriver" â†’ Siblings (both potential children of "Tool")
- "Joy" and "Sadness" â†’ Siblings (both potential children of "Emotion")
- "Democracy" and "Monarchy" â†’ Siblings (both potential children of "System")



========================
ACQUAINTANCE
========================
- Acquaintances are a type of Thing word generated based on lateral associations â€” thing words that most frequently co-occur in thought or experience, while not being a parent, sibling, children, roles, or synonyms. (Very important that acquaintances are explictly not these other types of words)
- Acquaintances are generated from the thing word and reflect intuitive, often emotional or symbolic associations â€” the kind of answer you might give to the question, "When I say X, you thinkâ€¦?" (as long as the answer is not a parent, sibling, children, roles, or synonyms). These connections arise from culture, metaphor, memory, story, or shared experience.
- A word's acquaintances often include symbolic opposites, poetic complements, or things that frequently co-occur in thought or experience ("9/11 and Osama Bin Laden")
- Acquaintances are a key part of the "Poet's Traversal Strategy," as they allow players to warp across conceptual space â€” following emotional resonance, metaphorical logic, or cultural symbolism rather than strict hierarchy. They enable surprising yet satisfying leaps between ideas that may not be categorically related, but feel meaningfully connected.
- They should be the MOST LIKELY thought of word that fits this crtieria, and put in order of most likely thought about (Not unlike the game of Jeopardy)

Examples:
- "9/11" â†’ Acquaintances: Osama Bin Laden, Twin Towers, Airplane, Collapse, War on Terror, Pentagon, NYC  
- "Romeo" â†’ Acquaintances: Juliet, Balcony, Poison, Tragedy, Youth, Mask, Dagger  
- "Mirror" â†’ Acquaintances: Reflection, Glass, Vanity, Gaze, Makeup, Face, Dressing Table  
- "Crown" â†’ Acquaintances: King, Queen, Throne, Jewels, Coronation, Scepter, Robe








/// NEEDS WORK
--.--,---.,---.|--.--    . . .,---.,---.,--. 
  |  |---'|---||  |      | | ||   ||---'|   |
  |  |  \ |   ||  |      | | ||   ||  \ |   |
  `  `   ``   '`  `      `-'-'`---'`   ``--' 
(TRAIT WORD)  
                                        


========================
WHAT IT IS
========================
- Traits are adjectives â€” descriptive qualities like "agile," "loud," or "resilient"
- Trait words function as BRIDGES between thing words, by nature of things that share those traits.
- As they are pure qualitative bridges, they do not have parents or children.
- They should be the MOST LIKELY thought of words that fits this crtieria, and put in order of most likely thought about (Not unlike the game of Jeopardy)


========================
EXAMPLES
========================
- "Cat" â†’ Traits: [in order of commonality] Independent, Agile, Curious, Nocturnal, Graceful, Mysterious, Affectionate
- "Ocean" â†’ Traits: [in order of commonality] Vast, Deep, Powerful, Mysterious, Beautiful, Dangerous, Eternal
- "Book" â†’ Traits: [in order of commonality] Informative, Engaging, Portable, Durable, Valuable, Timeless, Transformative


========================
DEFINITION SYSTEM
========================

The metadata of traits comprise of a list of thing words within whose metadata the trait is listed. Unlike thing words which have pre-set metadata, metadata of traits are dynamically generated from the word graph, (on load or on demand?). The metadata of Traits words are not pre-defined entities but emerge from the graph. In order to generate the metadata of a trait, the system will search the graph for all words that list this trait.

Because of this, during the initial graph build, traits are passively generated in each word's metadata alongside the other components, but are not defined during the initial stage. 
- Only traits that are shared by two or more defined words will be promoted in a later pass to become formal bridge-nodes with their own trait definitions

Example:
{
  "word": "Agile",
  "type": "trait",
  "exemplars": [
    "Cat",
    "Sprinter",
    "Dancer",
    "Gymnast",
    "Falcon",
    "Fencer",
    "Parkour"
  ],

(Each of those words shared betweem two or more thing words' definitions)

Example of a Trait's Natural Language Definition:
"Agile describes a quality of quick, graceful movement. It's a trait shared by [Cats], known for their natural [Agility], [Sprinters] who train for [Speed], and [Dancers] who master [Grace]. This trait connects to ideas of [Movement] and [Precision]."
.
Note: The exact words and connections shown will depend on what exists in the current graph. As more words are added to the system, the trait's definition and connections will naturally grow richer.









/// NEEDS WORK                                                
,---.,---.|    ,---.    . . .,---.,---.,--. 
|---'|   ||    |---     | | ||   ||---'|   |
|  \ |   ||    |        | | ||   ||  \ |   |
`   ``---'`---'`---'    `-'-'`---'`   ``--' 
                                            
                                                           
========================
WHAT IT IS
========================
- Role Words are functional or purpose-centered concepts that describe what a Thing is for, or what it does in the world.
- They can take two common forms:
  â€¢ Gerund Nouns (e.g. "Cutting," "Healing," "Teaching") â€” action-like roles
  â€¢ Abstract Nouns (e.g. "Companionship," "Protection," "Guidance") â€” value- or experience-based roles
- Role Words act as BRIDGES between Thing Words that fulfill similar purposes, even if they come from different branches of the taxonomy.
- Like Traits, Role Words are non-hierarchical: they do not have parents or children.
- However, unlike Traits, Role Words are defined by not only their EXEMPLARS, but also by their ACQUAINTANCES.
- **Critical Design Decision**: Not every Thing Word has roles. Roles are only generated when functional use is central to a word's identity.

========================
WHEN ROLES ARE GENERATED
========================
- **LLM-Based Decision**: For each Thing Word, an LLM determines whether roles are semantically appropriate on a case-by-case basis.
- **Semantic Salience**: Roles are only generated when the function or purpose of a Thing Word is a central part of its identity.
- **Examples of Role-Worthy Words**: "Scissors" (cutting), "Guru" (guiding), "Dog" (companionship), "Therapist" (healing)
- **Examples of Non-Role-Worthy Words**: "Cloud" (abstract concept), "Beauty" (aesthetic quality), "Feather" (natural object without clear function)
- **Rationale**: This approach avoids forcing roles onto concepts where purpose would feel artificial, keeping the graph clean and meaningful.

========================
EXAMPLES
========================
- "Scissors" â†’ Roles: Cutting, Trimming, Separating
- "Dog" â†’ Roles: Companionship, Guarding, Herding
- "Therapist" â†’ Roles: Guiding, Healing
- "Sword" â†’ Roles: Fighting, Cutting, Protecting
- "Alarm Clock" â†’ Roles: Waking, Alerting

========================
ROLE FORMAT & STRUCTURE
========================
- **Mixed Format Support**: Roles can be either gerund nouns ("Cutting," "Healing") or abstract nouns ("Companionship," "Protection")
- **Format Decision**: The LLM chooses whichever noun form feels most semantically appropriate for each Thing Word
- **Constraints**: 
  â€¢ Must be noun-like (no raw verbs like "cut" or "teach")
  â€¢ Must clearly reflect functional or purposeful use
  â€¢ Must be able to stand alone as a concept
- **Number of Roles**: Maximum of 3 roles per Thing Word, but often fewer. Quality over quantity.
- **Ordering**: Roles are always ordered by salience (most cognitively common first) to enable safe, deterministic pruning.

========================
DEFINITION SYSTEM
========================

Role Word metadata is composed of:
â€¢ exemplars â€” Thing Words that list the Role in their metadata
â€¢ acquaintances â€” symbolic, cultural, or experiential associations related to the role itself (5-7 items)

Role Words are seeded during Phase 1: when a Thing Word is created, the LLM evaluates whether it warrants roles at all. If so, it generates 1-3 salient Role Words to describe its purpose. These roles are initially stored as string literals within the Thing's metadata under a `purposes` array, alongside traits and acquaintances.

Then, in Phase 2.5, we promote each unique Role Word into its own node if it appears in two or more Thing Words. This ensures roles are real bridges, not one-off metadata fragments.

Example:
{
  "word": "Cutting",
  "type": "role",
  "exemplars": [
    "Scissors",
    "Knife",
    "Scalpel",
    "Sword",
    "Chainsaw"
  ],
  "acquaintances": [
    "Sharpness",
    "Separation",
    "Precision",
    "Motion",
    "Blade"
  ]
}

Example of a Role Word's Natural Language Definition:
"Cutting is a purposeful action associated with sharpness, separation, and precision. It's a function shared by tools like scissors, knives, scalpels, and swordsâ€”each designed to divide, shape, or sever materials."

Note: Abstract Role Words follow the same structure, but the natural language definition would have more emphasis on emotional or relational context.

Example:
{
  "word": "Companionship",
  "type": "role",
  "exemplars": [
    "Dog",
    "Friend",
    "Pet",
    "Caregiver"
  ],
  "acquaintances": [
    "Loyalty",
    "Presence",
    "Support",
    "Closeness",
    "Love"
  ]
}

"Companionship is a human-centered experience associated with loyalty, presence, and emotional closeness. It's a role fulfilled by beings like dogs, friends, pets, and caregiversâ€”offering comfort, connection, and belonging."

As with Traits, Role definitions grow richer as the graph expands. Role Words help players traverse via function, purpose, or effectâ€”revealing why something exists, not just what it is.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ§  Why This Matters

â€¢ Role Words introduce intent, use, and social function into the semantic map â€” enabling movement not just by category or trait, but by what things are for.
â€¢ Roles are especially important for tools, social beings, professions, and systems with an active function.
â€¢ But by requiring functional salience (i.e. only assigning Roles when they're central), the game avoids artificial or bloated metadata.
â€¢ Roles give players a new strategic traversal dimension: function.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ Notes

â€¢ Roles are only generated when the Thing Word is clearly purpose-centered.  
â€¢ Role Words do not have parents or children.  
â€¢ Their definitions are constructed from exemplars and acquaintances â€” not hierarchy.  
â€¢ They may have their own `acquaintances`, which are generated during Phase 2.5 promotion.
â€¢ This makes them feel consistent with the system while enabling a distinct mode of navigation.




==============================================================
HOW ARE ROLES HANDLED?
===============================================================
Roles in Six Degrees describe the functional or intentional use of a Thing â€” what it does, or what it's for. They, like traits, serve as semantic bridges between otherwise unrelated Thing Words that share a common purpose, and they enable functional traversal through the graph. Unlike Traits, which describe inherent qualities, Roles are about external intent or effect.

1. Selective Role Generation During Growth  
    â€¢ In Phase 1, every Thing Word is evaluated by an LLM to determine whether it warrants roles at all.
    â€¢ Only if a functional use is central to its meaning does the system generate 1-3 candidate Role labels.
    â€¢ These role strings are stored in the Thing's metadata under a `purposes` array, alongside traits and acquaintances.
    â€¢ Many Thing Words â€” like "Cloud" or "Beauty" â€” may have no roles at all, and that's fine.

2. Canonicalization & Filtering (Phase 2.5)
    â€¢ After tree growth, all raw purpose strings are normalized:
        â€¢ Lowercased, singularized, deduplicated
        â€¢ Merged using embeddings and lexical synonym rules (e.g. "cutting" â‰ˆ "slicing")
    â€¢ Any Role label found in â‰¥2 distinct Thing Words is promoted to become a Role Word.
    â€¢ Roles with only one source are discarded â€” this avoids cluttering the graph with overly narrow concepts.

3. Role Word Structure  
    â€¢ Like Traits, Role Words are stand-alone bridge nodes. They have:
        â€¢ `exemplars`: all Thing Words that listed the Role in their metadata  
        â€¢ `acquaintances`: 5-7 symbolic or functional associates of the Role (generated during promotion)
    â€¢ They do not have children or parents, and are not part of the parent-child hierarchy.

4. Acquaintance Generation for Roles  
    â€¢ Role Words have their own symbolic or functional acquaintances (e.g. "Cutting" â†’ Sharpness, Blade, Precision).
    â€¢ These are generated during Phase 2.5 promotion using the same LLM approach as Thing Word acquaintances.
    â€¢ Each Role Word gets 5-7 acquaintances that help ground it semantically and enable lateral traversal.

5. Dynamic Definition Rendering  
    â€¢ Role definitions are written using:
        â€¢ Exemplars (e.g. "Scissors," "Razor," "Knife")
        â€¢ Acquaintances (e.g. "Separation," "Edge," "Blade")
    â€¢ These produce natural-language definitions like:
        "Cutting is a purposeful act of separating material using an edge or blade. It's a role performed by tools like scissors, razors, and knivesâ€”each designed for precision and sharpness."

    â€¢ Abstract or social Roles (e.g. "Companionship," "Healing") follow the same pattern, using emotionally resonant acquaintances:
        "Companionship is the role of offering presence, loyalty, and mutual connection. It's fulfilled by dogs, friends, and partners, often tied to ideas of warmth, trust, and shared life."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ§  Why This Matters

â€¢ Role Words introduce intent, use, and social function into the semantic map â€” enabling movement not just by category or trait, but by what things are for.
â€¢ Roles are especially important for tools, social beings, professions, and systems with an active function.
â€¢ But by requiring functional salience (i.e. only assigning Roles when they're central), the game avoids artificial or bloated metadata.
â€¢ Roles give players a new strategic traversal dimension: function.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ Notes

â€¢ Roles are only generated when the Thing Word is clearly purpose-centered.  
â€¢ Role Words do not have parents or children.  
â€¢ Their definitions are constructed from exemplars and acquaintances â€” not hierarchy.  
â€¢ They may have their own `acquaintances`, which are generated during Phase 2.5 promotion.
â€¢ This makes them feel consistent with the system while enabling a distinct mode of navigation.




/// STILL NEEDS TO INCLUDE ROLES                                                
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
â–—â–„â–„â–– â–—â–– â–—â––â–—â–„â–„â–„â––â–—â––   â–—â–„â–„â–„     â–—â–„â–„â–„â––â–—â––    â–—â–„â–– â–—â–– â–—â––    â–—â–„â–„â–„â––â–—â–„â–„â–„ â–—â–„â–„â–„â–– â–—â–„â–– 
â–â–Œ â–â–Œâ–â–Œ â–â–Œ  â–ˆ  â–â–Œ   â–â–Œ  â–ˆ    â–â–Œ   â–â–Œ   â–â–Œ â–â–Œâ–â–Œ â–â–Œ      â–ˆ  â–â–Œ  â–ˆâ–â–Œ   â–â–Œ â–â–Œ
â–â–›â–€â–šâ––â–â–Œ â–â–Œ  â–ˆ  â–â–Œ   â–â–Œ  â–ˆ    â–â–›â–€â–€â–˜â–â–Œ   â–â–Œ â–â–Œâ–â–Œ â–â–Œ      â–ˆ  â–â–Œ  â–ˆâ–â–›â–€â–€â–˜â–â–›â–€â–œâ–Œ
â–â–™â–„â–â–˜â–â–šâ–„â–â–˜â–—â–„â–ˆâ–„â––â–â–™â–„â–„â––â–â–™â–„â–„â–€    â–â–Œ   â–â–™â–„â–„â––â–â–šâ–„â–â–˜â–â–™â–ˆâ–Ÿâ–Œ    â–—â–„â–ˆâ–„â––â–â–™â–„â–„â–€â–â–™â–„â–„â––â–â–Œ â–â–Œ
BUILD FLOW IDEA
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

I know this section seems very specific, but this is the overall idea for how to actually build all this. In practice, perhaps there are some things that need to be tweaked, or a few mistakes in here, or maybe it's not fully robust enough. But this is a good reference for the overall idea I'm thinking about for how to build flow, and at the very least, maybe it can serve as inspiration for the true strategy that might devel in the process of building it. Don't take it as gospel, use your own intelligence to really ensure everything will work, but use this as solid inspiration of how we might go about this.


---------------------

ğŸŒ³ PHASE 1: CORE TREE GROWTH

Goal: Grow a cohesive parentâ†’child taxonomy of main Thing Words (parents & children), while logging raw traits & acquaintances (no promotions yet).

1. Initialize Artifacts  
   ğŸ² master_words.json â†  
      [  
        {  
          word: "Thing",  
          type: "thing",  
          parent: null,  
          children: [],  
          traits: [],  
          acquaintances: [],  
          stages: {  
            childrenDone: false,  
            rawLogged:  false  
          }  
        }  
      ]  
   ğŸ“ raw_traits.csv        â† empty (columns: word,trait)  
   ğŸ“ raw_acquaintances.csv â† empty (columns: word,acquaintance)  

2. BFS Expansion Loop  
   â³ targetCount â† 2000  
   â¡ï¸ queue â† ["Thing"]  

   While (queue not empty) AND (master_words.json.length < targetCount):  
     a. Dequeue â†’ currentWord  
     b. Lookup record = master_words.json.find(w==currentWord)  
        â€¢ If record.stages.childrenDone == true â†’ continue  
     c. Build exclusion lists:  
        â€¢ parentTerm   = record.parent  
        â€¢ childTerms   = record.children  
        â€¢ traitTerms   = raw_traits.csv.filter(w==currentWord).map(t)  
        â€¢ synonymTerms = synonyms_of(currentWord)  # e.g. WordNet lookup  
     d. **Generate via three separate LLM calls (GPT-4 Turbo)**:  
        1) **Children Prompt**  
           "List exactly 7 common subtypes of [currentWord], ranked by commonality.  
            Return 7 comma-separated nouns only."  (actual prompt in prompts/children generation.txt)
           â†ªï¸ Retry up to 2Ã— if countâ‰ 7 or format invalid.  
        2) **Traits Prompt**  
           "List exactly 7 adjectives most people associate with [currentWord], ranked by frequency.  
            Return 7 comma-separated words only."   (actual prompt in prompts/trait generation.txt)
           â†ªï¸ Retry if needed.  
        3) **Acquaintances Prompt**  
           "List exactly 7 nouns that co-occur with [currentWord] in thought/experience,  
            excluding [parentTerm], [childTerms], [traitTerms], [synonymTerms].  
            Return 7 comma-separated nouns only."  (actual prompt in prompts/acquaintance generation.txt)
           â†ªï¸ Retry if exclusions appear or countâ‰ 7.  
        4) **Roles Prompt**  
           "Determine if [currentWord] has a clear functional purpose or role. If yes, list 1-3 role words (gerund or abstract nouns) that describe what [currentWord] does or is for. If no clear purpose, return NONE.  
            Return comma-separated nouns or NONE only."  (actual prompt in prompts/role generation.txt)
           â†ªï¸ Retry if format invalid.
     e. **Mark children as done**  
        â€¢ record.stages.childrenDone = true  
        â€¢ Persist master_words.json  
     f. **Normalize & Deduplicate Children**  
        For each childCandidate:  
          norm = lowercase(singularize(childCandidate))  
          if (!master_words.json.find(w â‡’ normalize(w.word)==norm)):  
            â€“ append newRecord {  
                word: childCandidate, type:"thing", parent:currentWord,  
                children:[], traits:[], acquaintances:[],  
                stages:{ childrenDone:false, rawLogged:false }  
              }  
            â€“ enqueue childCandidate  
          if (childCandidate âˆ‰ record.children):  
            â€“ record.children.push(childCandidate)  
     g. **Write Raw Metadata**  
        â€¢ append (currentWord, trait) â†’ raw_traits.csv  
        â€¢ append (currentWord, acq)   â†’ raw_acquaintances.csv  
        â€¢ append (currentWord, role)  â†’ raw_purposes.csv (if role != NONE)
     h. **Mark raw logging as done**  
        â€¢ record.stages.rawLogged = true  
        â€¢ Persist master_words.json  
     i. **Throttle & Back-off**  
        â€¢ Sleep 50â€“100 ms between calls  
        â€¢ On HTTP 429 â†’ exponential back-off  

3. Output after Phase 1  
   âœ… master_words.json (~2 000 entries), each:  
     {  
       "word":         "Cat",  
       "type":         "thing",  
       "parent":       "Animal",  
       "children":     ["Siamese","Tabby",â€¦],   # 7 items  
       "traits":       [],                      # placeholders  
       "purposes":     [],                      # placeholders  
       "stages": {                              
         "childrenDone": true,  
         "rawLogged":    true  
       }  
     }  
   âœ… raw_traits.csv & raw_acquaintances.csv & raw_purposes.csv capture every (word,trait)/(word,acq)/(word,role) pair  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ›  TECH: Model, Why & Cost Estimate  
   â€¢ Model: **GPT-4 Turbo**  
     â€“ Crisp list-only output, massive 128 K token window, ~Â½ GPT-4o cost  
   â€¢ Cost Estimate: 8 000 calls Ã— ~100 tokens â‰ˆ 800 000 tokens â†’ â‰ˆ\$3.40 total for Phase 1

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ”„ Resumable & Idempotent Processing  
â€¢ All state lives in master_words.json under each node's `stages` flags.  
â€¢ On crash/restart, rebuild queue from any words where either flag == false.  
â€¢ No separate "seen" set neededâ€”JSON is the single source of truth.  

ğŸš‘ Crash Recovery & Re-Runs  
â€¢ Safe to re-run; uncompleted nodes pick up exactly where they left off.  
â€¢ A small "reset" script can clear all `stages` flags if you want a fresh run.  

âœ… Benefits  
â€¢ Guarantees no half-finished nodes are skipped or re-processed.  
â€¢ Workflow state & data remain tightly coupled.  
â€¢ Fully crash-safe, retry-safe, and transparent to any maintainer.  


ğŸ”— LINKING STRATEGY (applies from Phase 1 onward)
	â€¢	Front-end should dynamically render any word found in a node's:
â€¢ parent
â€¢ children
â€¢ traits
â€¢ acquaintances
â€¢ purposes
as a clickable link in the definition.
	â€¢	These are guaranteed to exist in the metadata regardless of whether the node has received a Phase 4 natural-language definition or is still showing raw JSON metadata.
	â€¢	This eliminates the need to insert markup or hyperlinks into the raw text returned by the model.



------------------------------------



ğŸ§¬ PHASE 2: TRAIT SYNONYM NORMALIZATION & PROMOTION  

Goal: Collapse redundant raw adjectives into canonical Trait Words, then promote only those shared by â‰¥ 2 Thing Words â€” all in a resumable, idempotent pass.

1. Load Raw Traits  
   ğŸ“ Read `raw_traits.csv` â†’ list of `(word, rawTrait)` pairs.  

2. Ensure Stage Flag  
   ğŸ”– In `master_words.json`, every node must have:  
     `"stages": { â€¦, "traitsPromoted": false }`  

3. Preprocess & Cluster Labels  
   a. **Label Cleanup**  
      â€¢ Lowercase, strip punctuation, singularize/pluralize.  
      â€¢ Build unique set `allRawTraits`.  
   b. **Embedding Clustering**  
      â€¢ Compute embeddings for each label (e.g. `text-embedding-ada-002`).  
      â€¢ Run agglomerative clustering at similarity threshold ~0.8.  
   c. **Synonym Merge**  
      â€¢ Within each cluster, use WordNet or a custom map to merge obvious equivalents.  

4. Select Canonical Labels  
   ğŸ”„ For each cluster:  
     â€¢ Choose the label with highest raw frequency (tie â†’ shortest) â†’ `canonicalTrait`.  
     â€¢ Build map `rawToCanonical[rawTrait] = canonicalTrait`.  

5. Normalize & Attach  
   ğŸ”„ For each `(word, rawTrait)` in `raw_traits.csv`:  
     â€¢ `canonical = rawToCanonical[rawTrait]`  
     â€¢ Append `(word, canonical)` to an in-memory list or `traits_normalized.csv`.  

6. Count & Promote Traits  
   â¡ï¸ Group normalized pairs by `canonical`, count **distinct** words:  
     â€¢ If count â‰¥ 2 **and** for all exhibitor nodes `stages.traitsPromoted == false`:  
       a. Add a Trait node in `traits_master.json`:  
          ```json
          {
            "word":          "<canonicalTrait>",
            "type":          "trait",
            "exhibitors":    [/* sorted list of Thing Words */],
            "related_traits": []
          }
          ```  
       b. For each exhibitor in `master_words.json`, push `<canonicalTrait>` into its `traits` array.  
       c. Set each exhibitor's `stages.traitsPromoted = true`.  

7. Persist & Resume Safety  
   â€¢ After each cluster promotion, write out updated `traits_master.json` and `master_words.json`.  
   â€¢ On restart, skip any Thing Word where `stages.traitsPromoted == true`.  

8. Output after Phase 2  
   âœ… `traits_master.json` â€” final Trait Word definitions.  
   âœ… Updated `master_words.json` â€” all Thing Words now list canonical traits.  


ğŸ›  TECH: Approach, Why & Cost Estimate  

â€¢ **Embedding Model**  
  â€“ Model: `text-embedding-ada-002`  
  â€“ Why: Fast, cost-efficient semantic vectors ideal for clustering hundreds of short labels.  
  â€“ Cost: $0.0004 per 1K tokens â†’ ~500 labels Ã— 1 token â‰ˆ 500 tokens â†’ **\$0.0002** total.  

â€¢ **Clustering & Synonym Merge**  
  â€“ Library: scikit-learn's AgglomerativeClustering (no API cost).  
  â€“ Synonym map via WordNet or custom dictionary (zero extra cost).  

â€¢ **LLM Calls**  
  â€“ **None required** in Phase 2â€”everything runs locally after embeddings.  
  â€“ (Optional) GPT-4 Turbo validation: ~100 tokens/cluster Ã— ~50 clusters â‰ˆ 5 000 tokens â†’ **\$0.02**.  

â€¢ **Total Cost Estimate**  
  â€“ Embeddings pass: **\$0.0002**  
  â€“ Optional cluster-validation: **\$0.02**  
  â€“ **Grand Total:** < \$0.05  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ” **Key Essentials**  
1. **Idempotency via `traitsPromoted` flags**  
2. **Embedding-based clustering** (threshold ~0.8)  
3. **Promote only traits â‰¥ 2 exhibitors**  
4. **Persistent writes after each promotion**  
5. **Resumable on crash/restart**  

These steps ensure Phase 2 is reliable, repeatable, and crash-safe.  

------------------------------------

ğŸ­ PHASE 2.5: ROLE NORMALIZATION & PROMOTION

Goal: Convert raw role strings into canonical Role Words, promote only those shared by â‰¥ 2 Thing Words, and generate acquaintances for each promoted role â€” all in a resumable, idempotent pass.

1. Load Raw Roles  
   ğŸ“ Read `raw_purposes.csv` â†’ list of `(word, rawRole)` pairs.  

2. Ensure Stage Flag  
   ğŸ”– In `master_words.json`, every node must have:  
     `"stages": { â€¦, "rolesPromoted": false }`  

3. Preprocess & Cluster Labels  
   a. **Label Cleanup**  
      â€¢ Lowercase, strip punctuation, singularize/pluralize.  
      â€¢ Build unique set `allRawRoles`.  
   b. **Embedding Clustering**  
      â€¢ Compute embeddings for each label (e.g. `text-embedding-ada-002`).  
      â€¢ Run agglomerative clustering at similarity threshold ~0.8.  
   c. **Synonym Merge**  
      â€¢ Within each cluster, use WordNet or a custom map to merge obvious equivalents ("cutting" â‰ˆ "slicing").

4. Select Canonical Labels  
   ğŸ”„ For each cluster:  
     â€¢ Choose the label with highest raw frequency (tie â†’ shortest) â†’ `canonicalRole`.  
     â€¢ Build map `rawToCanonical[rawRole] = canonicalRole`.  

5. Normalize & Attach  
   ğŸ”„ For each `(word, rawRole)` in `raw_purposes.csv`:  
     â€¢ `canonical = rawToCanonical[rawRole]`  
     â€¢ Append `(word, canonical)` to an in-memory list or `roles_normalized.csv`.  

6. Count & Promote Roles  
   â¡ï¸ Group normalized pairs by `canonical`, count **distinct** words:  
     â€¢ If count â‰¥ 2 **and** for all exhibitor nodes `stages.rolesPromoted == false`:  
       a. Add a Role node in `roles_master.json`:  
          ```json
          {
            "word":          "<canonicalRole>",
            "type":          "role",
            "exemplars":     [/* sorted list of Thing Words */],
            "acquaintances": []
          }
          ```  
       b. For each exhibitor in `master_words.json`, push `<canonicalRole>` into its `purposes` array.  
       c. Set each exhibitor's `stages.rolesPromoted = true`.  

7. Generate Role Acquaintances  
   ğŸ”„ For each promoted Role Word:  
     â€¢ Use LLM to generate 5-7 acquaintances that co-occur with the role concept.
     â€¢ Store these in the Role Word's `acquaintances` array.
     â€¢ These acquaintances will be processed in Phase 3 (Acquaintance Adoption).

8. Persist & Resume Safety  
   â€¢ After each cluster promotion, write out updated `roles_master.json` and `master_words.json`.  
   â€¢ On restart, skip any Thing Word where `stages.rolesPromoted == true`.  

9. Output after Phase 2.5  
   âœ… `roles_master.json` â€” final Role Word definitions with acquaintances.  
   âœ… Updated `master_words.json` â€” all Thing Words now list canonical roles.  


ğŸ›  TECH: Approach, Why & Cost Estimate  

â€¢ **Embedding Model**  
  â€“ Model: `text-embedding-ada-002`  
  â€“ Why: Fast, cost-efficient semantic vectors ideal for clustering role labels.  
  â€“ Cost: $0.0004 per 1K tokens â†’ ~200 labels Ã— 1 token â‰ˆ 200 tokens â†’ **\$0.0001** total.  

â€¢ **Clustering & Synonym Merge**  
  â€“ Library: scikit-learn's AgglomerativeClustering (no API cost).  
  â€“ Synonym map via WordNet or custom dictionary (zero extra cost).  

â€¢ **LLM Calls for Acquaintances**  
  â€“ Model: GPT-4 Turbo  
  â€“ Purpose: Generate 5-7 acquaintances per promoted Role Word.  
  â€“ Cost: ~100 tokens/role Ã— ~100 roles â‰ˆ 10 000 tokens â†’ **\$0.04**.  

â€¢ **Total Cost Estimate**  
  â€“ Embeddings pass: **\$0.0001**  
  â€“ LLM acquaintance generation: **\$0.04**  
  â€“ **Grand Total:** ~\$0.04  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ” **Key Essentials**  
1. **Idempotency via `rolesPromoted` flags**  
2. **Embedding-based clustering** (threshold ~0.8)  
3. **Promote only roles â‰¥ 2 exhibitors**  
4. **Generate acquaintances during promotion**  
5. **Persistent writes after each promotion**  
6. **Resumable on crash/restart**  

These steps ensure Phase 2.5 is reliable, repeatable, and crash-safe.  

------------------------------------

ğŸ¤ PHASE 3: ACQUAINTANCE ADOPTION & INTEGRATION

Goal: Ensure every "Acquaintance" word hangs from the main taxonomy rather than dangle as an orphan, while preserving the natural, organic hierarchy we built in Phase 1. We use an embeddings-to-shortlist + LLM final vote approach to guarantee both high coverage and semantic precision.

1. Load Raw Acquaintances  
   ğŸ“ Read `raw_acquaintances.csv` â†’ list of `(sourceWord, acqTerm)` pairs.  
   â€£ Rationale: Having all logged pairs in memory lets us see which Thing Words reference each acquaintance.  

2. Build Unique Acquaintance Set  
   ğŸ¯ `allAcqs = unique(acqTerm)`  
   â€£ Rationale: Deduplicating here avoids redundant workâ€”each new term only needs a parent assigned once.  

3. Seed Orphan Placeholders  
   For each `acq` in `allAcqs`:  
     â€¢ If `acq` already exists in `master_words.json` â†’ skip.  
     â€¢ Else â†’ append placeholder node to `master_words.json`:  
       ```json
       {
         "word":          "<acq>",
         "type":          "thing",
         "parent":        null,
         "children":      [],
         "traits":        [],
         "acquaintances": [],
         "stages":        { /* inherit or initialize flags */ }
       }
       ```  
     â€£ Rationale: Placeholders let us assign parents in bulk before wiring up lateral links.  

4. Prepare Embeddings Search  
   â€¢ `parentCandidates = master_words.json.map(w â‡’ w.word + " â€” parent:" + w.parent + "; children:" + w.children.slice(0,3).join(", "))`  
   â€¢ Compute embeddings for each extended candidate string **and** for each raw `acq` term using `text-embedding-ada-002`.  
   â€£ Rationale: Embedding the word plus minimal context (parent, a few children) biases similarity toward true categories.  

5. Build Shortlists via Nearest Neighbors  
   For each orphan `acq`:  
     a. Compute cosine similarity vs. all `parentCandidates`.  
     b. Select top 20 most similar candidates â†’ `shortlist[acq]`.  
   â€£ Rationale: A top-20 shortlist almost always contains the correct parentâ€”cuts LLM calls by ~95%.  

6. LLM-Guided Final Parent Selection  
   For each `acq` + its `shortlist`:  
     â€¢ Prompt GPT-4 Turbo:
       ```
       I have 20 potential parent categories for "[acq]":
       â€¢ cand1, cand2, â€¦, cand20

       Which single category best serves as the parent for "[acq]"?  
       Reply with exactly one choice from the list.
       ```  
     â€¢ If response âˆ‰ shortlist â†’ retry once.  
     â€¢ Assign `chosenParent = response`.  
   â€£ Rationale: LLM picks the most precise fit from a semantically-filtered setâ€”minimizing hallucination.  

7. Persist Parent Assignments (Bidirectional)  
   â€¢ Update each orphan node's `parent = chosenParent` in `master_words.json`.  
   â€¢ Then locate the `chosenParent` node and push `acq` into its `children` array (if not already present).  
   â€¢ Write out updated JSON.  
   â€£ Rationale: Ensures bidirectional consistencyâ€”parents know their children and vice versa.  

8. Attach Acquaintance Edges  
   For each `(sourceWord, acq)` in `raw_acquaintances.csv`:  
     â€¢ Locate `sourceWord` node â†’ push `acq` onto its `acquaintances` array (if missing).  
   â€¢ Persist `master_words.json`.  
   â€£ Rationale: Now every Thing Word correctly references its lateral connections.  

9. (Optional) Prune Duplicate Acquaintances  
   â€¢ Cluster near-identical nodes (e.g. "Color" vs. "Colour"), merge them, rewrite links.  
   â€£ Rationale: Keeps graph cleanâ€”skip unless obvious duplicates appear.  

10. Output after Phase 3  
   âœ… `master_words.json` updated:  
     â€“ Every orphan "Thing Word" has a valid `parent`.  
     â€“ Every original Thing Word's `acquaintances` array is filled.  
     â€“ Every adopted child is correctly listed in its parent's `children` array.  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ›  TECH: Hybrid Approach, Why & Cost  

â€¢ **Embeddings Pass**  
  â€“ Model: `text-embedding-ada-002`  
  â€“ Purpose: Build a shortlist >95% likely to include true parent.  
  â€“ Cost: ~$0.0004 per 1K tokens â†’ 2 000 candidates + contexts â‰ˆ\$0.002 total.  

â€¢ **Nearest-Neighbor Search**  
  â€“ Library: Faiss or `sklearn.neighbors` (no API cost).  
  â€“ Purpose: Quickly retrieve top-K similar candidates per orphan.  

â€¢ **LLM Final Vote**  
  â€“ Model: GPT-4 Turbo  
  â€“ Prompt size: ~100 tokens/orphan â†’ 2 000 orphans â†’ 200 000 tokens.  
  â€“ Cost: 200 000 tokens Ã— \$0.004/token â‰ˆ **\$0.80**.  

â€¢ **Total Phase 3 Estimate**  
  â€“ Embeddings: ~\$0.002  
  â€“ LLM calls: ~\$0.80  
  â€“ **Grand Total:** ~\$0.80  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ” Why This Matters  

1. **Semantic Integrity**  
   â€“ Context-aware embeddings bias toward valid parent categories.  
   â€“ LLM then picks the exact best fitâ€”avoiding misclassification.  

2. **Full Graph Connectivity**  
   â€“ No more dangling wordsâ€”all links traverse cleanly.  
   â€“ New acquaintances adopt into the structure naturally.  

3. **Bidirectional Consistency**  
   â€“ Parents point to their new children.  
   â€“ Definitions and UI reflect a fully wired web.  

4. **Idempotency & Crash Safety**  
   â€“ Skip any orphan whose `parent != null`.  
   â€“ Clear stages logic ensures safe restarts with no duplicated work.  

With this hybrid approach, your taxonomy remains coherent, intuitive, and fully connectedâ€”ready for the player's logical and poetic journeys.  


------------------------------------



ğŸ—£ï¸ PHASE 4: NATURAL-LANGUAGE DEFINITION PASS (WE WONT DO THIS RIGHT NOW. WE WILL JUST HAVE THE WORDS / LINKS WITHOUT A NATURAL LANGUAGE PASS FOR PLAYTESTING)

Goal: Turn each word's structured metadata (parent, children, traits, acquaintances, purposes) into a clean, human-readable definition that feels organic and engaging to players. This adds personality and immersion while preserving all navigational affordances.

1. Gather All Nodes  
   ğŸ” Load `master_words.json` and `traits_master.json` into memory.  
   â€¢ Includes all Thing Words and promoted Trait Words.  

2. Prepare Prompt Template  
   âœï¸ Use a system + user prompt (see: `prompts/Natural Language.txt`) that:  
     â€¢ Accepts a full metadata object for each word  
     â€¢ Generates a sentence wikipedia-esque definition  
     â€¢ References the word's parent category  
     â€¢ Weaves in up to X children, Y traits, and Z acquaintances (as the dev, you can decide how many of each to include)  
     â€¢ Ends with: "Use no additional terms beyond what's given."  
   â€£ This ensures coverage of every key relationship while minimizing hallucination or drift.

3. Batch & Generate  
   âš¡ï¸ Process definitions in batches of 10â€“20 words per API call:  
     â€¢ Send prompt + batch of JSON nodes to GPT-4 Turbo  
     â€¢ Receive a list of `{ word, definition }` responses  
     â€¢ Validate structure before saving  
   â€£ Batching reduces per-call overhead and keeps costs predictable.

4. Validate Output  
   âœ”ï¸ For each generated definition:  
     â€¢ Confirm the main word appears in its own definition  
     â€¢ Optionally run a keyword coverage script to ensure presence of required metadata (parent, â‰¥1 trait, etc.)  
     â€¢ Spot-check ~10% of definitions for quality  
   â€£ Prevents silent errors or unusable definitions from slipping into the final UI.

5. Persist Definitions  
   ğŸ’¾ Write to `definitions_master.json` in this format:  
     ```json
     [
       {
         "word": "Cat",
         "definition": "Cats are agile mammals known for their independence and graceful movements. They are often kept as pets and associated with traits like curiosity and nocturnality. Related ideas include dogs, litter boxes, and scratching posts."
       },
       â€¦
     ]
     ```
   â€£ Easy to look up and integrate on the front-end.

6. Linkability Support  
   ğŸ”— All terms in the generated definitions that match a known Thing Word or Trait Word should be linkable in the UI.  
   â€¢ Don't encode links in the definition text itself  
   â€¢ Instead, the front-end should match visible tokens against each word's known metadata (parent, children, traits, acquaintances)  
   â€¢ If matched, style and bind the word as a clickable hyperlink  
   â€£ This allows for dynamic, robust linking even if the natural-language wording varies.

7. Optional: Regeneration Support  
   ğŸ”„ To incrementally re-render definitions later:  
     â€¢ Track a `definitionLastUpdatedAt` timestamp per word  
     â€¢ Only re-run the LLM if its metadata has changed since that time

8. Output after Phase 4  
   âœ… `definitions_master.json` with natural-language definitions for every Thing and Trait Word  
   âœ… Front-end now supports immersive, readable traversal without breaking linkability

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ›  TECH: Model, Cost & Format

â€¢ Model: GPT-4 Turbo  
  â€“ Cost: $0.003 / 1K tokens  
  â€“ Easily fits ~20 metadata objects + prompt + output in 128K context  

â€¢ Token & Cost Estimate  
  â€“ Avg: ~200 tokens per word (input + output)  
  â€“ 10,000 words = ~2M tokens â†’ $6  
  â€“ 50,000 words â†’ ~$30  
  â€“ 500,000 words â†’ ~$300  

â€¢ Batch Size  
  â€“ 10â€“20 words per request recommended  
  â€“ Keeps response latency low and throughput high

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ“Œ Why This Matters

â€¢ ğŸš€ Player Experience: Adds clarity, narrative, and intuition to each node  
â€¢ ğŸ” Future-Proof: Easy to re-run when definitions change  
â€¢ ğŸ§  Cognitive Glue: Makes the graph feel alive and explorable instead of abstract  
â€¢ ğŸ§© Front-End Ready: UI link logic is metadata-driven, not tied to string parsing



---------------

========================
TLDR: THE OVERALL STRATEGY
========================                                                

We grow a living word graph in four idempotent, crash-safe stages:
	1.	Phase 1: Core Tree Growth
Starting from a single root "Thing," we breadth-first generate exactly seven children, seven traits, and seven acquaintances per node via GPT-4 Turbo. For roles, we use an LLM to determine whether each Thing Word warrants roles at all, and if so, generate 1-3 salient role labels. Children become new nodes; traits, acquaintances, and roles are logged for later. Each node tracks completion flags so you can pause, crash, or resume without losing progress. At the end, you have ~2 000 Thing-Words with a bulletproof parentâ†’child backbone plus raw trait, acquaintance, and role logs.
	2.	Phase 2: Trait Normalization & Promotion
We collapse redundant adjectives into canonical Trait-Words using fast, local embedding clustering (plus a simple WordNet / synonym pass) and only promote traits that appear on two or more Thing-Words. No further LLM calls are neededâ€”your trait layer organically emerges from the data at almost zero cost.
	3.	Phase 2.5: Role Normalization & Promotion
We collapse redundant role strings into canonical Role-Words using the same embedding clustering approach as traits. Only roles shared by â‰¥2 Thing Words are promoted. For each promoted Role Word, we generate 5-7 acquaintances using GPT-4 Turbo to ground the role semantically and enable lateral traversal.
	4.	Phase 3: Acquaintance Adoption
Every raw acquaintance (from both Thing Words and Role Words) is first seeded as a placeholder node, then slotted under a real parent via a hybrid approach: compute context-enhanced embeddings for each candidate, retrieve a top-20 shortlist per orphan, and finally ask GPT-4 Turbo to pick the single best parent. After that, you link each source word back to its newly adopted acquaintances. The result is a fully connected, intuitive network with zero dangling nodes.
	5.	Phase 4: Natural-Language Definitions
With metadata locked in, we batch out 2â€“3-sentence encyclopedia-style definitions for every Thing, Trait, and Role Wordâ€”via GPT-4 Turboâ€”by feeding each node's full JSON (parent, children, traits, acquaintances, purposes, etc.) into a consistent prompt template. We validate output, persist to definitions_master.json, and let the front-end dynamically hyperlink any known keyword back into the graph. This final pass costs only a few dollars for thousands of words and gives players rich, linkable definitions that bring the entire semantic ecosystem to life.





---------



//needs work
-----------------------------------------
ğŸ‘©â€ğŸ’» DEVELOPER / DESIGNER USER EXPERIENCE
-----------------------------------------

Goal: to make it as easy as possible for me, the designer/developer, to:
1. Be able to tweak parameters to see how differnet it makes the game feel, with the ability to save configurations as "files", name them, and load them back up. One such parameter I want to be able to shift is the number of children, traits, acquaintances, and purposes per node (Remove from the end of each collection of words]. Another such change would be the vertical granularity of the graph, so I can dial the complexity down or up, which would in turn remove 'gradations' in between the graph, so I can test the game with different levels of detail and complexity.
2. Be able to easily run each phase of the build pipeline
3. Be able to get clear, transparent logs progress indicators and other real-time progress indicators and feedback as I run the phases, so I'm not in the dark as to what is happening
4. Be able to easily and intuitively test and see the output of a meaningful sample of each, so I can see the direction it's going in before committing to large LLM calls or expensive tasks.
5. Be able to, if I choose to, view a verbose logging and audit trail just in case, so I can always ask "Why didn't X?" happen and have a method to diagnose the issues and look back
6. have a resumable & idmpotent process, so I can pause, crash, or resume without losing progress, unless I explicitly ask to remove all progress.
7. Be able to easily change the prompts being used by switching the text of the .txt files in the Prompts folder
8. To see a default preset definition of each word using static text + variables, so I can see a rough approximation of what the game feels like even before doing the Natural Language Pass. (Ex. [word] is a type of 'parent' that is known for [traits]. Often, [word] may be connected in some way to [acquaintances]. Types of [word] may include[children], etc]
Every phase is a self-contained script/app you can launch from the repo root.  
9. To be able to work with dummy data and see how game looks with it
10. To be able to contduct a "sandbox test" or dryrun to simulate each prase without writing to disk



---------------------------------------------------------------------
ğŸ‘©â€ğŸ’» IDEAS FOR HOW TO IMPLEMENT THE ABOVE (just ideas to be inspired by)
----------------------------------------------------------------------

1. CONFIGURATION MANAGEMENT  
   â€¢ Store all adjustable parameters in a versioned `config` folder (e.g. `config/`), with JSON files you can name, save, and load.  
     â€“ e.g. `config/fast-test.json`, `config/full-run.json`  
     â€“ Override defaults by passing `--config myfile.json` to any script.  
   â€¢ Exposed parameters include: childrenPerNode, traitsPerNode, acqsPerNode, rolesPerNode, graphDepth, batch sizes, dryRun, etc.  
   â€¢ Dial "vertical granularity" by adjusting graphDepth or a minParentDepth threshold to collapse intermediate nodes.  

2. ONE-COMMAND ENTRYPOINTS  
   â€¢ Each phase is a standalone CLI script in `scripts/`â€”e.g.  
     â€“ `scripts/phase1.js` â†’ npm run phase1  
     â€“ `scripts/phase2.js` â†’ npm run phase2  
     â€“ `scripts/phase3.js` â†’ npm run phase3  
     â€“ `scripts/phase4.js` â†’ npm run phase4  
   â€¢ Pass `--config` and other flags directly.  

3. REAL-TIME PROGRESS & FEEDBACK  
   â€¢ Terminal shows a live progress bar and summary line for each phase:  
     â€“ Phase1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘]  45% (900/2000) â€“ current="Vehicle"  
     â€“ Phase3: orphan="Shakespeare" shortlistReady(20/20)  
   â€¢ Warnings on errors or rate limits trigger automatic retry with exponential back-off and log entry.  

4. LIGHTWEIGHT SANDBOX & DRY-RUN  
   â€¢ `--dryRun` mode simulates the entire pipeline on a miniature data sample (e.g. 10 root nodes) without writing to disk.  
   â€¢ Use `config/test-sample.json` to surface one or two words, verify JSON outputs, inspect placeholder definitions, and stop before costly LLM calls.  
   â€¢ Swap real data for dummy fixtures by passing `--dataSample dummy/` if you want full pipeline logic on synthetic inputs.  

5. EASY SAMPLE INSPECTION  
   â€¢ After any phase, run `npm run sample -- phaseX` to print a curated subset of outputs:  
     â€“ Phase1 sample: list 5 generated nodes + their children/traits/acqs  
     â€“ Phase3 sample: show 3 orphans with shortlists and chosen parents  
   â€¢ This helps you preview outcomes before full-scale runs or LLM billing.  

6. VERBOSE LOGGING & AUDIT TRAIL  
   â€¢ By default, each run writes to `/logs/phaseX_YYYYMMDD.log` with timestamps, config snapshot, retry counts, token usage, and summary metrics.  
   â€¢ Enable `--verbose` to include full request/response bodies for edge-case debugging.  

7. RESUMABLE & IDEMPOTENT  
   â€¢ All scripts respect per-node `stages` flags in JSON data stores.  
   â€¢ Kill and restart any phase; it picks up where it left off.  
   â€¢ Pass `--resetPhase X` to clear only that phase's flags if you truly want to re-run from scratch.  

8. PROMPT & MODEL SWAPPABILITY  
   â€¢ All LLM and embedding calls read from `prompts/` and `utils/embeddings.js`.  
   â€¢ Alter any `.txt` prompt file or swap model names in one config fieldâ€”scripts automatically use the updated text on next run.  

9. PRESET "ROUGH" DEFINITIONS  
   â€¢ When Phase4 has not yet run, by default generate placeholder definitions from static templates:  
     â€“ "[Word] is a type of [parent] known for [trait1, trait2]. Commonly linked to [acq1, acq2]. Examples include [child1, child2]. [Word] serves purposes such as [purpose1, purpose2]."  
   â€¢ Controlled by `config/defaultDefs.json`; run with `npm run previewDefs` to see them in the UI.

10. INTERACTIVE DEBUGGING SHELL  
    â€¢ `npm run phase1:inspect -- --node Banana` drops you into a REPL to:  
      â€“ Load record(`Banana`), view metadata, call `generateAcqs(record)`, etc.  
    â€¢ Supports one-off LLM prompt tests, embedding PCA, clustering thresholds, etc.  























