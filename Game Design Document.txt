 
 
 
 
 
 
 
 
 
 
 
   ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÑ‚ñà  ‚ñÄ‚ñà‚ñà‚ñà‚ñà    ‚ñê‚ñà‚ñà‚ñà‚ñà‚ñÄ      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ     ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ     ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 
  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñå   ‚ñà‚ñà‚ñà‚ñà‚ñÄ       ‚ñà‚ñà‚ñà   ‚ñÄ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà 
  ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ  ‚ñà‚ñà‚ñà‚ñå    ‚ñà‚ñà‚ñà  ‚ñê‚ñà‚ñà‚ñà         ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ  
  ‚ñà‚ñà‚ñà        ‚ñà‚ñà‚ñà‚ñå    ‚ñÄ‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñÄ         ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà  ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ      ‚ñÑ‚ñà‚ñà‚ñà         ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÄ  ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ      ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ       ‚ñà‚ñà‚ñà        
‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñå    ‚ñà‚ñà‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñÑ          ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ     ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñÑ  ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñÄ   ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ     ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ     ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 
         ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñê‚ñà‚ñà‚ñà  ‚ñÄ‚ñà‚ñà‚ñà         ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñÑ    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñÑ    ‚ñà‚ñà‚ñà    ‚ñà‚ñÑ           ‚ñà‚ñà‚ñà 
   ‚ñÑ‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñÑ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà‚ñÑ       ‚ñà‚ñà‚ñà   ‚ñÑ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà    ‚ñÑ‚ñà    ‚ñà‚ñà‚ñà 
 ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ  ‚ñà‚ñÄ   ‚ñà‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà‚ñÑ      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ  
                                                                              ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà                                        
                                                                                                                          









~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 ‚ñó‚ñÑ‚ññ ‚ñó‚ññ  ‚ñó‚ññ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ñÑ‚ñÑ‚ññ ‚ñó‚ññ  ‚ñó‚ññ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ññ ‚ñó‚ññ
‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå  ‚ñê‚ñå‚ñê‚ñå   ‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå  ‚ñê‚ñå  ‚ñà  ‚ñê‚ñå   ‚ñê‚ñå ‚ñê‚ñå
‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå  ‚ñê‚ñå‚ñõ‚ñÄ‚ñÄ‚ñò‚ñê‚ñõ‚ñÄ‚ñö‚ññ‚ñê‚ñå  ‚ñê‚ñå  ‚ñà  ‚ñê‚ñõ‚ñÄ‚ñÄ‚ñò‚ñê‚ñå ‚ñê‚ñå
‚ñù‚ñö‚ñÑ‚ñû‚ñò ‚ñù‚ñö‚ñû‚ñò ‚ñê‚ñô‚ñÑ‚ñÑ‚ññ‚ñê‚ñå ‚ñê‚ñå ‚ñù‚ñö‚ñû‚ñò ‚ñó‚ñÑ‚ñà‚ñÑ‚ññ‚ñê‚ñô‚ñÑ‚ñÑ‚ññ‚ñê‚ñô‚ñà‚ñü‚ñå

~ OVERVIEW ~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


========================
ABOUT
========================
Six Degrees is a game about word associations. It's a poetic-strategic exploration game where players traverse a network of interconnected words (like a Wiki) to reach a target word in the fewest steps possible. The game's structure is a hybrid between a logical taxonomy (like a tree of concepts) and a metaphorical social web (a network of associations). Each move is a click on a word within a definition, which reveals that word's own definition and related words. Every click forward counts as a step.


========================
GOAL OF THE GAME
========================
The objective is simple: reach the destination word in the fewest possible steps. The fewer the steps, the better your score. Six Degrees is both a reflective puzzle and a meditative journey ‚Äî a game that rewards thoughtful connections and 'scalable / zoomable' traversable logic.


========================
CORE GAMEPLAY LOOP
========================
1. The player is shown a starting word and a destination word.
2. Clicking the starting word opens its natural-language definition.
3. In the definition, certain words are clickable: these may be parents, children, acquaintances, traits, or roles.
4. Clicking on a linked word opens that word's definition.
5. Every click forward increments the player's step count.
6. The goal is to reach the destination word in the least number of steps possible.

Polished features:
- Show visited path as a breadcrumb trail (Backtracking rewinds time: it removes the steps taken after that point, encouraging experimentation and puzzle-solving without penalty)
- Make it a "daily" game, with a new starting word and destination word each day. This is in sync globally, but allow a "Free Play" mode as well.
- Add a "Share" button once you finish to share a screenshot of your score with your friends, along with a link to the game, to encourage virality.
- Add a "Something missing in this definition?" button to allow players to suggest new words to add to the graph, to be used in later iteration of the game
- Add difficulty slider to allow players to choose how far away the starting word is from the destination word



========================
ENVISIONED PLAYER STRATEGIES
========================
There are two core player archetypes:

The Logician:
- Navigates up to parent categories to access siblings
- Uses structured inference: "If cat is an animal, and dog is too, I can get to dog by going up to animal first"
- Values hierarchy, clarity, and precision

The Poet:
- Follows acquaintances, traits, and roles through contextual resonance
- Moves by metaphor, emotion, cultural vibe: "Apple reminds me of knowledge... or Steve Jobs"
- Values surprise, meaning, and personal logic

Both Modes Can Succeed:
Most players will mix both styles during a single game.

Example Path: Destination = Stop Sign
Logician's path (6 steps): Cat ‚Üí Animal ‚Üí Human ‚Üí Civilization ‚Üí Rules ‚Üí Traffic Sign ‚Üí Stop Sign
Poet's path (6 steps): Cat ‚Üí Night ‚Üí Street ‚Üí City ‚Üí Crosswalk ‚Üí Red Light ‚Üí Stop Sign

Both are valid. Both are beautiful. I hypothesise that making the best of both results in the highest score.




========================
DIFFICULTY TUNING
========================
Difficulty is tuned by controlling the number of steps the origin word is away from the destination.

- A random destination word is selected first
- Then, the game walks X steps away in a random direction through the existing graph to select an origin word. This ensures that the destination word is always reachable from the origin word.
- X = difficulty level
- Starting rule: All paths are built with 6 degrees of separation to match the game title
- Later, modes like "Easy (4 degrees)" or "Hard (8+)" can be introduced











~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
‚ñó‚ññ ‚ñó‚ññ ‚ñó‚ñÑ‚ññ ‚ñó‚ñÑ‚ñÑ‚ññ ‚ñó‚ñÑ‚ñÑ‚ñÑ     ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ññ  ‚ñó‚ññ‚ñó‚ñÑ‚ñÑ‚ññ ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ ‚ñó‚ñÑ‚ñÑ‚ññ
‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå  ‚ñà      ‚ñà   ‚ñù‚ñö‚ñû‚ñò ‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå   ‚ñê‚ñå   
‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñõ‚ñÄ‚ñö‚ññ‚ñê‚ñå  ‚ñà      ‚ñà    ‚ñê‚ñå  ‚ñê‚ñõ‚ñÄ‚ñò ‚ñê‚ñõ‚ñÄ‚ñÄ‚ñò ‚ñù‚ñÄ‚ñö‚ññ
‚ñê‚ñô‚ñà‚ñü‚ñå‚ñù‚ñö‚ñÑ‚ñû‚ñò‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñô‚ñÑ‚ñÑ‚ñÄ      ‚ñà    ‚ñê‚ñå  ‚ñê‚ñå   ‚ñê‚ñô‚ñÑ‚ñÑ‚ññ‚ñó‚ñÑ‚ñÑ‚ñû

~ WORD TYPES ~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



                                                          
--.--|   ||,   .,---.    . . .,---.,---.,--. 
  |  |---|||\  ||  _.    | | ||   ||---'|   |
  |  |   ||| \ ||   |    | | ||   ||  \ |   |
  `  `   '``  `'`---'    `-'-'`---'`   ``--' 
(THING WORD)  


========================
WHAT IT IS
========================
- In Six Degrees, all primary nodes in the graph are called "Thing Words" ‚Äî nouns that represent distinct entities, objects, systems, or concepts.
- These words are the most common in the game and serve as both origin and destination points in traversal.
- While all Thing Words share the same structural format, they may be be called "parent, child, sibling, or acquaintance" to describe how they relate to each other.
  - A thing word is called a **parent** when it represents the most precise broader category of a thing.
  - A thing word is called a **child** when it is a more specific example of another thing.
  - A thing word is called a **sibling** when it is a parallel member of the same category (purely used for thinking about the game development and design)
  - A thing word is called an **acquaintance** when it shares thematic, symbolic, or commonly associated meaning with another thing.



========================
DEFINITION SYSTEM
========================
Every Thing Word includes the following metadata, with its own parent (1), children (5), traits (5), acquaintances (5), and purposes if they exist (5).


{
  "word": "Cat",
  "type": "thing"
  "parent": "Animal",
  "children": [
    "Siamese",
    "Tabby",
    "Persian",
    "Maine Coon",
    "Sphynx",
    "Ragdoll",
    "Bengal"
  ],
  "traits": [
    "independent",
    "agile",
    "curious",
    "nocturnal",
    "graceful",
    "mysterious",
    "affectionate"
  ],
  "acquaintances": [
    "Dog",
    "Mouse",
    "Litter Box",
    "Laser Pointer",
    "Fur",
    "Scratching Post",
    "Couch"
  ],
  "purposes": [
    "Companion",
    "Pet"
  ]
}  

Note: I have only put X number of each type of word in this example, but it could be more or less.


========================
CHILD
========================
- Children are more specific examples, types, or subcategory 'thing words' generated from a parent
- Each child must satisfy the statement: "A [child] is a kind of [parent word]"
- In player traversal, Children support downward movement in abstraction and help players zoom into more concrete ideas
- They should be the MOST LIKELY thought of words that fits this crtieria, and put in order of most likely thought about (Not unlike the game of Jeopardy)

Examples:
- "Animal" ‚Üí Children: [in order of commonality] Dog, Cat, Bird, Fish, Horse, Cow, Elephant
- "Tool" ‚Üí Children: [in order of commonality] Hammer, Screwdriver, Wrench, Pliers, Saw, Drill, Tape Measure
- "Emotion" ‚Üí Children: [in order of commonality] Happy, Sad, Angry, Fear, Love, Surprise, Disgust



========================
PARENT
========================
- The Parent is the single most appropriate broader category that a a child word belongs to
- Unlike children, which are generated based on their parents, parents are never generated from children, as generation is a one-way process.
- In player traversal, clicking on a parent supports "upward" movement in abstraction and help players zoom out to more general words
- The first parent, which has no parent of its own, is our God Word: "Thing"

Examples:
- "Cat" ‚Üí Parent: Animal
- "Sadness" ‚Üí Parent: Emotion
- "Hammer" ‚Üí Parent: Tool
- "Democracy" ‚Üí Parent: System



========================
SIBLING
========================
- Siblings are words that share the same parent ‚Äî they are parallel members of the same category.
- For example, "Dog" and "Cat" would both be siblings if they were the direct children of "Mammal." (FYI not sure if this example stands with the real word graph)
- Unlike parents, children, and acquaintances, siblings are never explicitly listed in a word's metadata. Instead, they are inferred by moving up to a shared parent and then down into another child. This design encourages discovery and strategy: a player might wish to find a sibling in order to traverse 'horizontally', and may "zoom out" to a shared parent and then "zoom in" to a different child.
- The concept of siblings is mentioned here simply because it's useful for thinking about game development, design, and traversal strategy, but is not a direct part of a thing word's metadata.

Examples:
- "Dog" and "Cat" ‚Üí Siblings (both potential children of "Mammal")
- "Hammer" and "Screwdriver" ‚Üí Siblings (both potential children of "Tool")
- "Joy" and "Sadness" ‚Üí Siblings (both potential children of "Emotion")
- "Democracy" and "Monarchy" ‚Üí Siblings (both potential children of "System")



========================
ACQUAINTANCE
========================
- Acquaintances are a type of Thing word generated based on lateral associations ‚Äî thing words that most frequently co-occur in thought or experience, while not being a parent, sibling, children, roles, or synonyms. (Very important that acquaintances are explictly not these other types of words)
- Acquaintances are generated from the thing word and reflect intuitive, often emotional or symbolic associations ‚Äî the kind of answer you might give to the question, "When I say X, you think‚Ä¶?" (as long as the answer is not a parent, sibling, children, roles, or synonyms). These connections arise from culture, metaphor, memory, story, or shared experience.
- A word's acquaintances often include symbolic opposites, poetic complements, or things that frequently co-occur in thought or experience ("9/11 and Osama Bin Laden")
- Acquaintances are a key part of the "Poet's Traversal Strategy," as they allow players to warp across conceptual space ‚Äî following emotional resonance, metaphorical logic, or cultural symbolism rather than strict hierarchy. They enable surprising yet satisfying leaps between ideas that may not be categorically related, but feel meaningfully connected.
- They should be the MOST LIKELY thought of word that fits this crtieria, and put in order of most likely thought about (Not unlike the game of Jeopardy)

========================
ACQUAINTANCE SYMMETRY
========================
- **One-Directional by Default**: Acquaintances are generated based on salience from one side only. If "Romeo" lists "Juliet" as an acquaintance, this does not automatically mean "Juliet" lists "Romeo."
- **Salience-Based Generation**: Each word's acquaintances reflect what people think of when they hear that specific word. "Romeo" ‚Üí "Juliet" because people think of Juliet when hearing Romeo, but "Juliet" might evoke "Tragedy" or "Balcony" more prominently than "Romeo."
- **Natural Bidirectional Emergence**: If both directions independently emerge during generation (e.g., both "Romeo" and "Juliet" list each other), this represents a strong, naturally symmetric association.
- **Deduplication Strategy**: During Phase 3, the system checks for existing bidirectional pairs and prevents redundant metadata while preserving the natural strength of mutual associations.
- **Rationale**: Enforcing artificial symmetry would dilute signal quality and inflate noise in the graph. Letting associations emerge naturally preserves the authentic, salience-based nature of human associative thinking.

Examples:
- "9/11" ‚Üí Acquaintances: Osama Bin Laden, Twin Towers, Airplane, Collapse, War on Terror, Pentagon, NYC  
- "Romeo" ‚Üí Acquaintances: Juliet, Balcony, Poison, Tragedy, Youth, Mask, Dagger  
- "Mirror" ‚Üí Acquaintances: Reflection, Glass, Vanity, Gaze, Makeup, Face, Dressing Table  
- "Crown" ‚Üí Acquaintances: King, Queen, Throne, Jewels, Coronation, Scepter, Robe








/// NEEDS WORK
--.--,---.,---.|--.--    . . .,---.,---.,--. 
  |  |---'|---||  |      | | ||   ||---'|   |
  |  |  \ |   ||  |      | | ||   ||  \ |   |
  `  `   ``   '`  `      `-'-'`---'`   ``--' 
(TRAIT WORD)  
                                        


========================
WHAT IT IS
========================
- Traits are adjectives ‚Äî descriptive qualities like "agile," "loud," or "resilient"
- Trait words function as BRIDGES between thing words, by nature of things that share those traits.
- As they are pure qualitative bridges, they do not have parents or children.
- They should be the MOST LIKELY thought of words that fits this crtieria, and put in order of most likely thought about (Not unlike the game of Jeopardy)


========================
EXAMPLES
========================
- "Cat" ‚Üí Traits: [in order of commonality] Independent, Agile, Curious, Nocturnal, Graceful, Mysterious, Affectionate
- "Ocean" ‚Üí Traits: [in order of commonality] Vast, Deep, Powerful, Mysterious, Beautiful, Dangerous, Eternal
- "Book" ‚Üí Traits: [in order of commonality] Informative, Engaging, Portable, Durable, Valuable, Timeless, Transformative


========================
DEFINITION SYSTEM
========================

The metadata of traits comprise of a list of thing words within whose metadata the trait is listed. Unlike thing words which have pre-set metadata, metadata of traits are dynamically generated from the word graph, (on load or on demand?). The metadata of Traits words are not pre-defined entities but emerge from the graph. In order to generate the metadata of a trait, the system will search the graph for all words that list this trait.

Because of this, during the initial graph build, traits are passively generated in each word's metadata alongside the other components, but are not defined during the initial stage. 
- Only traits that are shared by two or more defined words will be promoted in a later pass to become formal bridge-nodes with their own trait definitions

Example:
{
  "word": "Agile",
  "type": "trait",
  "exemplars": [
    "Cat",
    "Sprinter",
    "Dancer",
    "Gymnast",
    "Falcon",
    "Fencer",
    "Parkour"
  ],

(Each of those words shared betweem two or more thing words' definitions)

Example of a Trait's Natural Language Definition:
"Agile describes a quality of quick, graceful movement. It's a trait shared by [Cats], known for their natural [Agility], [Sprinters] who train for [Speed], and [Dancers] who master [Grace]. This trait connects to ideas of [Movement] and [Precision]."
.
Note: The exact words and connections shown will depend on what exists in the current graph. As more words are added to the system, the trait's definition and connections will naturally grow richer.









/// NEEDS WORK                                                
,---.,---.|    ,---.    . . .,---.,---.,--. 
|---'|   ||    |---     | | ||   ||---'|   |
|  \ |   ||    |        | | ||   ||  \ |   |
`   ``---'`---'`---'    `-'-'`---'`   ``--' 
                                            
                                                           
========================
WHAT IT IS
========================
- Role Words are functional or purpose-centered concepts that describe what a Thing is for, or what it does in the world.
- They can take two common forms:
  ‚Ä¢ Gerund Nouns (e.g. "Cutting," "Healing," "Teaching") ‚Äî action-like roles
  ‚Ä¢ Abstract Nouns (e.g. "Companionship," "Protection," "Guidance") ‚Äî value- or experience-based roles
- Role Words act as BRIDGES between Thing Words that fulfill similar purposes, even if they come from different branches of the taxonomy.
- Like Traits, Role Words are non-hierarchical: they do not have parents or children.
- However, unlike Traits, Role Words are defined by not only their EXEMPLARS, but also by their ACQUAINTANCES.
- **Critical Design Decision**: Not every Thing Word has roles. Roles are only generated when functional use is central to a word's identity.

========================
WHEN ROLES ARE GENERATED
========================
- **LLM-Based Decision**: For each Thing Word, an LLM determines whether roles are semantically appropriate on a case-by-case basis.
- **Semantic Salience**: Roles are only generated when the function or purpose of a Thing Word is a central part of its identity.
- **Decision Criteria**: The LLM asks: "Does this word represent an entity whose primary identity is defined by a function, service, or purpose in the world?"
- **Key Signals**: concrete tools, professions, social roles, systems with active effects.
- **Examples of Role-Worthy Words**: "Scissors" (cutting), "Guru" (guiding), "Dog" (companionship), "Therapist" (healing)
- **Examples of Non-Role-Worthy Words**: "Cloud" (abstract concept), "Beauty" (aesthetic quality), "Feather" (natural object without clear function)
- **Conditional Generation**: Roles are the only metadata type generated conditionally. Children, traits, and acquaintances are always generated for every Thing Word.
- **Rationale**: This approach avoids forcing roles onto concepts where purpose would feel artificial, keeping the graph clean and meaningful.

========================
EXAMPLES
========================
- "Scissors" ‚Üí Roles: Cutting, Trimming, Separating
- "Dog" ‚Üí Roles: Companionship, Guarding, Herding
- "Therapist" ‚Üí Roles: Guiding, Healing
- "Sword" ‚Üí Roles: Fighting, Cutting, Protecting
- "Alarm Clock" ‚Üí Roles: Waking, Alerting

========================
ROLE FORMAT & STRUCTURE
========================
- **Mixed Format Support**: Roles can be either gerund nouns ("Cutting," "Healing") or abstract nouns ("Companionship," "Protection")
- **Format Decision**: The LLM chooses whichever noun form feels most semantically appropriate for each Thing Word
- **Constraints**: 
  ‚Ä¢ Must be noun-like (no raw verbs like "cut" or "teach")
  ‚Ä¢ Must clearly reflect functional or purposeful use
  ‚Ä¢ Must be able to stand alone as a concept
- **Number of Roles**: Maximum of 3 roles per Thing Word, but often fewer. Quality over quantity.
- **Selection Criteria**: The LLM is instructed to "List the three most salient roles ‚Äî the ones most people intuitively think of first when they consider [WORD]. Order them by likelihood."
- **Salience-Based Ordering**: Roles are always ordered by cognitive salience (most likely to be thought of first) to enable safe, deterministic pruning.
- **Rationale**: This phrasing biases the model toward its strongest candidates, and because we truncate at 3 and then later canonicalize/promote only roles appearing in ‚â•2 words, the graph naturally favors the most cognitive "wins."

========================
DEFINITION SYSTEM
========================

Role Word metadata is composed of:
‚Ä¢ exemplars ‚Äî Thing Words that listed the Role in their metadata
‚Ä¢ acquaintances ‚Äî symbolic, cultural, or experiential associations related to the role itself (5-7 items)

Role Words are seeded during Phase 1: when a Thing Word is created, the LLM evaluates whether it warrants roles at all. If so, it generates 1-3 salient Role Words to describe its purpose. These roles are initially stored as string literals within the Thing's metadata under a `purposes` array, alongside traits and acquaintances.

Then, in Phase 2.5, we promote each unique Role Word into its own node if it appears in two or more Thing Words. This ensures roles are real bridges, not one-off metadata fragments.

Example:
{
  "word": "Cutting",
  "type": "role",
  "exemplars": [
    "Scissors",
    "Knife",
    "Scalpel",
    "Sword",
    "Chainsaw"
  ],
  "acquaintances": [
    "Sharpness",
    "Separation",
    "Precision",
    "Motion",
    "Blade"
  ]
}

Example of a Role Word's Natural Language Definition:
"Cutting is a purposeful action associated with sharpness, separation, and precision. It's a function shared by tools like scissors, knives, scalpels, and swords‚Äîeach designed to divide, shape, or sever materials."

Note: Abstract Role Words follow the same structure, but the natural language definition would have more emphasis on emotional or relational context.

Example:
{
  "word": "Companionship",
  "type": "role",
  "exemplars": [
    "Dog",
    "Friend",
    "Pet",
    "Caregiver"
  ],
  "acquaintances": [
    "Loyalty",
    "Presence",
    "Support",
    "Closeness",
    "Love"
  ]
}

"Companionship is a human-centered experience associated with loyalty, presence, and emotional closeness. It's a role fulfilled by beings like dogs, friends, pets, and caregivers‚Äîoffering comfort, connection, and belonging."

As with Traits, Role definitions grow richer as the graph expands. Role Words help players traverse via function, purpose, or effect‚Äîrevealing why something exists, not just what it is.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üß† Why This Matters

‚Ä¢ Role Words introduce intent, use, and social function into the semantic map ‚Äî enabling movement not just by category or trait, but by what things are for.
‚Ä¢ Roles are especially important for tools, social beings, professions, and systems with an active function.
‚Ä¢ But by requiring functional salience (i.e. only assigning Roles when they're central), the game avoids artificial or bloated metadata.
‚Ä¢ Roles give players a new strategic traversal dimension: function.
‚Ä¢ **Bidirectional consistency** ensures that clicking on a Role shows all Things that perform that function, and clicking on a Thing shows its Roles with confidence that the Role node truly connects back.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìé Notes

‚Ä¢ Roles are only generated when the Thing Word is clearly purpose-centered.  
‚Ä¢ Role Words do not have parents or children.  
‚Ä¢ Their definitions are constructed from exemplars and acquaintances ‚Äî not hierarchy.  
‚Ä¢ They may have their own `acquaintances`, which are generated during Phase 2.5 promotion.
‚Ä¢ This makes them feel consistent with the system while enabling a distinct mode of navigation.




==============================================================
HOW ARE ROLES HANDLED?
===============================================================
Roles in Six Degrees describe the functional or intentional use of a Thing ‚Äî what it does, or what it's for. They, like traits, serve as semantic bridges between otherwise unrelated Thing Words that share a common purpose, and they enable functional traversal through the graph. Unlike Traits, which describe inherent qualities, Roles are about external intent or effect.

1. Selective Role Generation During Growth  
    ‚Ä¢ In Phase 1, every Thing Word is evaluated by an LLM to determine whether it warrants roles at all.
    ‚Ä¢ **Conditional Generation**: Roles are the only metadata type generated conditionally. Children, traits, and acquaintances are always generated for every Thing Word.
    ‚Ä¢ Only if a functional use is central to its meaning does the system generate 1-3 candidate Role labels.
    ‚Ä¢ These role strings are stored in the Thing's metadata under a `purposes` array, alongside traits and acquaintances.
    ‚Ä¢ Many Thing Words ‚Äî like "Cloud" or "Beauty" ‚Äî may have no roles at all, and that's fine.
    ‚Ä¢ **Decision Logging**: The LLM's internal rationale for returning NONE is not captured or persisted to avoid parsing complexity.

2. Canonicalization & Filtering (Phase 2.5)
    ‚Ä¢ After tree growth, all raw purpose strings are normalized:
        ‚Ä¢ Lowercased, singularized, deduplicated
        ‚Ä¢ Merged using embeddings and lexical synonym rules (e.g. "cutting" ‚âà "slicing")
    ‚Ä¢ Any Role label found in ‚â•2 distinct Thing Words is promoted to become a Role Word.
    ‚Ä¢ Roles with only one source are discarded ‚Äî this avoids cluttering the graph with overly narrow concepts.

3. Role Word Structure  
    ‚Ä¢ Like Traits, Role Words are stand-alone bridge nodes. They have:
        ‚Ä¢ `exemplars`: all Thing Words that listed the Role in their metadata (populated during promotion)
        ‚Ä¢ `acquaintances`: 5-7 symbolic or functional associates of the Role (generated during promotion)
    ‚Ä¢ They do not have children or parents, and are not part of the parent-child hierarchy.

4. Acquaintance Generation for Roles  
    ‚Ä¢ Role Words have their own symbolic or functional acquaintances (e.g. "Cutting" ‚Üí Sharpness, Blade, Precision).
    ‚Ä¢ These are generated during Phase 2.5 promotion using the same LLM approach as Thing Word acquaintances.
    ‚Ä¢ Each Role Word gets 5-7 acquaintances that help ground it semantically and enable lateral traversal.

5. Dynamic Definition Rendering  
    ‚Ä¢ Role definitions are written using:
        ‚Ä¢ Exemplars (e.g. "Scissors," "Razor," "Knife")
        ‚Ä¢ Acquaintances (e.g. "Separation," "Edge," "Blade")
    ‚Ä¢ These produce natural-language definitions like:
        "Cutting is a purposeful act of separating material using an edge or blade. It's a role performed by tools like scissors, razors, and knives‚Äîeach designed for precision and sharpness."

    ‚Ä¢ Abstract or social Roles (e.g. "Companionship," "Healing") follow the same pattern, using emotionally resonant acquaintances:
        "Companionship is the role of offering presence, loyalty, and mutual connection. It's fulfilled by dogs, friends, and partners, often tied to ideas of warmth, trust, and shared life."

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üß† Why This Matters

‚Ä¢ Role Words introduce intent, use, and social function into the semantic map ‚Äî enabling movement not just by category or trait, but by what things are for.
‚Ä¢ Roles are especially important for tools, social beings, professions, and systems with an active function.
‚Ä¢ But by requiring functional salience (i.e. only assigning Roles when they're central), the game avoids artificial or bloated metadata.
‚Ä¢ Roles give players a new strategic traversal dimension: function.
‚Ä¢ **Bidirectional consistency** ensures that clicking on a Role shows all Things that perform that function, and clicking on a Thing shows its Roles with confidence that the Role node truly connects back.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìé Notes

‚Ä¢ Roles are only generated when the Thing Word is clearly purpose-centered.  
‚Ä¢ Role Words do not have parents or children.  
‚Ä¢ Their definitions are constructed from exemplars and acquaintances ‚Äî not hierarchy.  
‚Ä¢ They may have their own `acquaintances`, which are generated during Phase 2.5 promotion.
‚Ä¢ This makes them feel consistent with the system while enabling a distinct mode of navigation.




                                            
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
‚ñó‚ñÑ‚ñÑ‚ññ ‚ñó‚ññ ‚ñó‚ññ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ññ   ‚ñó‚ñÑ‚ñÑ‚ñÑ     ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ññ    ‚ñó‚ñÑ‚ññ ‚ñó‚ññ ‚ñó‚ññ    ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ñÑ‚ñÑ‚ñÑ ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ ‚ñó‚ñÑ‚ññ 
‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå ‚ñê‚ñå  ‚ñà  ‚ñê‚ñå   ‚ñê‚ñå  ‚ñà    ‚ñê‚ñå   ‚ñê‚ñå   ‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå ‚ñê‚ñå      ‚ñà  ‚ñê‚ñå  ‚ñà‚ñê‚ñå   ‚ñê‚ñå ‚ñê‚ñå
‚ñê‚ñõ‚ñÄ‚ñö‚ññ‚ñê‚ñå ‚ñê‚ñå  ‚ñà  ‚ñê‚ñå   ‚ñê‚ñå  ‚ñà    ‚ñê‚ñõ‚ñÄ‚ñÄ‚ñò‚ñê‚ñå   ‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå ‚ñê‚ñå      ‚ñà  ‚ñê‚ñå  ‚ñà‚ñê‚ñõ‚ñÄ‚ñÄ‚ñò‚ñê‚ñõ‚ñÄ‚ñú‚ñå
‚ñê‚ñô‚ñÑ‚ñû‚ñò‚ñù‚ñö‚ñÑ‚ñû‚ñò‚ñó‚ñÑ‚ñà‚ñÑ‚ññ‚ñê‚ñô‚ñÑ‚ñÑ‚ññ‚ñê‚ñô‚ñÑ‚ñÑ‚ñÄ    ‚ñê‚ñå   ‚ñê‚ñô‚ñÑ‚ñÑ‚ññ‚ñù‚ñö‚ñÑ‚ñû‚ñò‚ñê‚ñô‚ñà‚ñü‚ñå    ‚ñó‚ñÑ‚ñà‚ñÑ‚ññ‚ñê‚ñô‚ñÑ‚ñÑ‚ñÄ‚ñê‚ñô‚ñÑ‚ñÑ‚ññ‚ñê‚ñå ‚ñê‚ñå
BUILD FLOW IDEA
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

I know this section seems very specific, but this is the overall idea for how to actually build all this. In practice, perhaps there are some things that need to be tweaked, or a few mistakes in here, or maybe it's not fully robust enough. But this is a good reference for the overall idea I'm thinking about for how to build flow, and at the very least, maybe it can serve as inspiration for the true strategy that might devel in the process of building it. Don't take it as gospel, use your own intelligence to really ensure everything will work, but use this as solid inspiration of how we might go about this.


---------------------

üå≥ PHASE 1: CORE TREE GROWTH

Goal: Grow a cohesive parent‚Üíchild taxonomy of main Thing Words (parents & children), while logging raw traits & acquaintances (no promotions yet).

1. Initialize Artifacts  
   üé≤ master_words.json ‚Üê  
      [  
        {  
          word: "Thing",  
          type: "thing",  
          parent: null,  
          children: [],  
          traits: [],  
          acquaintances: [],  
          stages: {  
            childrenDone: false,  
            rawLogged:  false  
          }  
        }  
      ]  
   üìù raw_traits.csv        ‚Üê empty (columns: word,trait)  
   üìù raw_acquaintances.csv ‚Üê empty (columns: word,acquaintance)  

   **File Locations (per implementation structure):**
   ‚Ä¢ `data/processed/master_words.json`
   ‚Ä¢ `data/raw/raw_traits.csv`
   ‚Ä¢ `data/raw/raw_acquaintances.csv`
   ‚Ä¢ `data/raw/raw_purposes.csv`

2. BFS Expansion Loop  
   ‚è≥ targetCount ‚Üê 2000  
   ‚û°Ô∏è queue ‚Üê ["Thing"]  

   **Configuration-driven parameters:**
   ‚Ä¢ `targetCount` from config file (e.g., 10 for test-sample.json, 2000 for full-run.json)
   ‚Ä¢ `childrenPerNode`, `traitsPerNode`, `acquaintancesPerNode`, `rolesPerNode` from config
   ‚Ä¢ `dryRun` mode for testing without API calls

   While (queue not empty) AND (master_words.json.length < targetCount):  
     a. Dequeue ‚Üí currentWord  
     b. Lookup record = master_words.json.find(w==currentWord)  
        ‚Ä¢ If record.stages.childrenDone == true ‚Üí continue  
     c. Build exclusion lists:  
        ‚Ä¢ parentTerm   = record.parent  
        ‚Ä¢ childTerms   = record.children  
        ‚Ä¢ traitTerms   = raw_traits.csv.filter(w==currentWord).map(t)  
        ‚Ä¢ synonymTerms = synonyms_of(currentWord)  # e.g. WordNet lookup  
     d. **Generate via three separate LLM calls (GPT-4 Turbo)**:  
        1) **Children Prompt**  
           "List exactly 7 common subtypes of [currentWord], ranked by commonality.  
            Return 7 comma-separated nouns only."  (actual prompt in prompts/children generation.txt)
           ‚Ü™Ô∏è Retry up to 2√ó if count‚â†7 or format invalid.  
        2) **Traits Prompt**  
           "List exactly 7 adjectives most people associate with [currentWord], ranked by frequency.  
            Return 7 comma-separated words only."   (actual prompt in prompts/trait generation.txt)
           ‚Ü™Ô∏è Retry if needed.  
        3) **Acquaintances Prompt**  
           "List exactly 7 nouns that co-occur with [currentWord] in thought/experience,  
            excluding [parentTerm], [childTerms], [traitTerms], [synonymTerms].  
            Return 7 comma-separated nouns only."  (actual prompt in prompts/acquaintance generation.txt)
           ‚Ü™Ô∏è Retry if exclusions appear or count‚â†7.  
        4) **Roles Prompt**  
           "Determine if [currentWord] has a clear functional purpose or role. If yes, list 1-3 role words (gerund or abstract nouns) that describe what [currentWord] does or is for. If no clear purpose, return NONE.  
            Return comma-separated nouns or NONE only."  (actual prompt in prompts/role generation.txt)
           ‚Ü™Ô∏è Retry if format invalid.

           **Enhanced Prompt Guidance:**
           "Does [currentWord] represent an entity whose primary identity is defined by a function, service, or purpose in the world? Key signals: concrete tools, professions, social roles, systems with active effects. If yes, list the three most salient roles ‚Äî the ones most people intuitively think of first when they consider [currentWord]. Order them by likelihood. If no clear purpose, return NONE."

     d.1. **Exclusion-List Validation** (After each LLM call)
        ‚Ä¢ For acquaintances generation, verify no excluded terms appear in the response
        ‚Ä¢ Excluded terms: parent, children, traits, synonyms
        ‚Ä¢ If violation detected: append "do not include [excludedTerm]" clause to prompt and retry once
        ‚Ä¢ If still invalid: drop the excluded term and log warning
        ‚Ä¢ Rationale: Prevents semantic pollution that could cascade downstream and break graph integrity

     e. **Mark children as done**  
        ‚Ä¢ record.stages.childrenDone = true  
        ‚Ä¢ Persist master_words.json  
     f. **Normalize & Deduplicate Children**  
        For each childCandidate:  
          norm = lowercase(singularize(childCandidate))  
          if (!master_words.json.find(w ‚áí normalize(w.word)==norm)):  
            ‚Äì append newRecord {  
                word: childCandidate, type:"thing", parent:currentWord,  
                children:[], traits:[], acquaintances:[],  
                stages:{ childrenDone:false, rawLogged:false }  
              }  
            ‚Äì enqueue childCandidate  
          if (childCandidate ‚àâ record.children):  
            ‚Äì record.children.push(childCandidate)  

     f.1. **Cross-Reference Validation** (After metadata assembly)
        ‚Ä¢ Verify all referenced words exist in the graph:
          - Children: If missing, enqueue for generation
          - Traits/Acquaintances/Roles: If missing, drop and warn
        ‚Ä¢ This prevents broken JSON links and UI errors from missing nodes
        ‚Ä¢ Rationale: Ensures all metadata references point to valid, existing nodes
     g. **Write Raw Metadata**  
        ‚Ä¢ append (currentWord, trait) ‚Üí raw_traits.csv  
        ‚Ä¢ append (currentWord, acq)   ‚Üí raw_acquaintances.csv  
        ‚Ä¢ append (currentWord, role)  ‚Üí raw_purposes.csv (if role != NONE)
     h. **Mark raw logging as done**  
        ‚Ä¢ record.stages.rawLogged = true  
        ‚Ä¢ Persist master_words.json  
     i. **Throttle & Back-off**  
        ‚Ä¢ Sleep 50‚Äì100 ms between calls  
        ‚Ä¢ On HTTP 429 ‚Üí exponential back-off  

3. Output after Phase 1  
   ‚úÖ master_words.json (~2 000 entries), each:  
     {  
       "word":         "Cat",  
       "type":         "thing",  
       "parent":       "Animal",  
       "children":     ["Siamese","Tabby",‚Ä¶],   # 7 items  
       "traits":       [],                      # placeholders  
       "purposes":     [],                      # placeholders  
       "stages": {                              
         "childrenDone": true,  
         "rawLogged":    true  
       }  
     }  
   ‚úÖ raw_traits.csv & raw_acquaintances.csv & raw_purposes.csv capture every (word,trait)/(word,acq)/(word,role) pair  

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üõ† TECH: Model, Why & Cost Estimate  
   ‚Ä¢ Model: **GPT-4 Turbo**  
     ‚Äì Crisp list-only output, massive 128 K token window, ~¬Ω GPT-4o cost  
   ‚Ä¢ Cost Estimate: 8 000 calls √ó ~100 tokens ‚âà 800 000 tokens ‚Üí ‚âà\$3.40 total for Phase 1

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üîÑ Resumable & Idempotent Processing  
‚Ä¢ All state lives in master_words.json under each node's `stages` flags.  
‚Ä¢ On crash/restart, rebuild queue from any words where either flag == false.  
‚Ä¢ No separate "seen" set needed‚ÄîJSON is the single source of truth.  

üöë Crash Recovery & Re-Runs  
‚Ä¢ Safe to re-run; uncompleted nodes pick up exactly where they left off.  
‚Ä¢ A small "reset" script can clear all `stages` flags if you want a fresh run.  

‚úÖ Benefits  
‚Ä¢ Guarantees no half-finished nodes are skipped or re-processed.  
‚Ä¢ Workflow state & data remain tightly coupled.  
‚Ä¢ Fully crash-safe, retry-safe, and transparent to any maintainer.  


üîó LINKING STRATEGY (applies from Phase 1 onward)
	‚Ä¢	Front-end should dynamically render any word found in a node's:
‚Ä¢ parent
‚Ä¢ children
‚Ä¢ traits
‚Ä¢ acquaintances
‚Ä¢ purposes
as a clickable link in the definition.
	‚Ä¢	These are guaranteed to exist in the metadata regardless of whether the node has received a Phase 4 natural-language definition or is still showing raw JSON metadata.
	‚Ä¢	This eliminates the need to insert markup or hyperlinks into the raw text returned by the model.
	‚Ä¢	**Acquaintance Directionality**: Acquaintances are one-directional by default (A‚ÜíB doesn't imply B‚ÜíA). Natural bidirectional associations emerge independently and are deduplicated during Phase 3.



------------------------------------



üß¨ PHASE 2: TRAIT SYNONYM NORMALIZATION & PROMOTION  

Goal: Collapse redundant raw adjectives into canonical Trait Words, then promote only those shared by ‚â• 2 Thing Words ‚Äî all in a resumable, idempotent pass.

1. Load Raw Traits  
   üìù Read `raw_traits.csv` ‚Üí list of `(word, rawTrait)` pairs.  

2. Ensure Stage Flag  
   üîñ In `master_words.json`, every node must have:  
     `"stages": { ‚Ä¶, "traitsPromoted": false }`  

3. Preprocess & Cluster Labels  
   a. **Label Cleanup**  
      ‚Ä¢ Lowercase, strip punctuation, singularize/pluralize.  
      ‚Ä¢ Build unique set `allRawTraits`.  
   b. **Embedding Clustering**  
      ‚Ä¢ Compute embeddings for each label (e.g. `text-embedding-ada-002`).  
      ‚Ä¢ Run agglomerative clustering at similarity threshold ~0.8.  
   c. **Synonym Merge**  
      ‚Ä¢ Within each cluster, use WordNet or a custom map to merge obvious equivalents.  

4. Select Canonical Labels  
   üîÑ For each cluster:  
     ‚Ä¢ Choose the label with highest raw frequency (tie ‚Üí shortest) ‚Üí `canonicalTrait`.  
     ‚Ä¢ Build map `rawToCanonical[rawTrait] = canonicalTrait`.  

5. Normalize & Attach  
   üîÑ For each `(word, rawTrait)` in `raw_traits.csv`:  
     ‚Ä¢ `canonical = rawToCanonical[rawTrait]`  
     ‚Ä¢ Append `(word, canonical)` to an in-memory list or `traits_normalized.csv`.  

6. Count & Promote Traits  
   ‚û°Ô∏è Group normalized pairs by `canonical`, count **distinct** words:  
     ‚Ä¢ If count ‚â• 2 **and** for all exemplar nodes `stages.traitsPromoted == false`:  
       a. Add a Trait node in `traits_master.json`:  
          ```json
          {
            "word":          "<canonicalTrait>",
            "type":          "trait",
            "exemplars":     [/* sorted list of Thing Words */],
            "related_traits": []
          }
          ```  
       b. For each exemplar in `master_words.json`, push `<canonicalTrait>` into its `traits` array.  
       c. Set each exemplar's `stages.traitsPromoted = true`.  

7. Persist & Resume Safety  
   ‚Ä¢ After each cluster promotion, write out updated `traits_master.json` and `master_words.json`.  
   ‚Ä¢ On restart, skip any Thing Word where `stages.traitsPromoted == true`.  

8. Output after Phase 2  
   ‚úÖ `traits_master.json` ‚Äî final Trait Word definitions.  
   ‚úÖ Updated `master_words.json` ‚Äî all Thing Words now list canonical traits.  


üõ† TECH: Approach, Why & Cost Estimate  

‚Ä¢ **Embedding Model**  
  ‚Äì Model: `text-embedding-ada-002`  
  ‚Äì Why: Fast, cost-efficient semantic vectors ideal for clustering hundreds of short labels.  
  ‚Äì Cost: $0.0004 per 1K tokens ‚Üí ~500 labels √ó 1 token ‚âà 500 tokens ‚Üí **\$0.0002** total.  

‚Ä¢ **Clustering & Synonym Merge**  
  ‚Äì Library: scikit-learn's AgglomerativeClustering (no API cost).  
  ‚Äì Synonym map via WordNet or custom dictionary (zero extra cost).  

‚Ä¢ **LLM Calls**  
  ‚Äì **None required** in Phase 2‚Äîeverything runs locally after embeddings.  
  ‚Äì (Optional) GPT-4 Turbo validation: ~100 tokens/cluster √ó ~50 clusters ‚âà 5 000 tokens ‚Üí **\$0.02**.  

‚Ä¢ **Total Cost Estimate**  
  ‚Äì Embeddings pass: **\$0.0002**  
  ‚Äì Optional cluster-validation: **\$0.02**  
  ‚Äì **Grand Total:** < \$0.05  

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üîç **Key Essentials**  
1. **Idempotency via `traitsPromoted` flags**  
2. **Embedding-based clustering** (threshold ~0.8)  
3. **Promote only traits ‚â• 2 exemplars**  
4. **Persistent writes after each promotion**  
5. **Resumable on crash/restart**  

These steps ensure Phase 2 is reliable, repeatable, and crash-safe.  

------------------------------------

üé≠ PHASE 2.5: ROLE NORMALIZATION & PROMOTION

Goal: Convert raw role strings into canonical Role Words, promote only those shared by ‚â• 2 Thing Words, and generate acquaintances for each promoted role ‚Äî all in a resumable, idempotent pass.

1. Load Raw Roles  
   üìù Read `raw_purposes.csv` ‚Üí list of `(word, rawRole)` pairs.  

2. Ensure Stage Flag  
   üîñ In `master_words.json`, every node must have:  
     `"stages": { ‚Ä¶, "rolesPromoted": false }`  

3. Preprocess & Cluster Labels  
   a. **Label Cleanup**  
      ‚Ä¢ Lowercase, strip punctuation, singularize/pluralize.  
      ‚Ä¢ Build unique set `allRawRoles`.  
   b. **Embedding Clustering**  
      ‚Ä¢ Compute embeddings for each label (e.g. `text-embedding-ada-002`).  
      ‚Ä¢ Run agglomerative clustering at similarity threshold ~0.8.  
   c. **Synonym Merge**  
      ‚Ä¢ Within each cluster, use WordNet or a custom map to merge obvious equivalents ("cutting" ‚âà "slicing").

4. Select Canonical Labels  
   üîÑ For each cluster:  
     ‚Ä¢ Choose the label with highest raw frequency (tie ‚Üí shortest) ‚Üí `canonicalRole`.  
     ‚Ä¢ Build map `rawToCanonical[rawRole] = canonicalRole`.  

5. Normalize & Attach  
   üîÑ For each `(word, rawRole)` in `raw_purposes.csv`:  
     ‚Ä¢ `canonical = rawToCanonical[rawRole]`  
     ‚Ä¢ Append `(word, canonical)` to an in-memory list or `roles_normalized.csv`.  

6. Count & Promote Roles  
   ‚û°Ô∏è Group normalized pairs by `canonical`, count **distinct** words:  
     ‚Ä¢ If count ‚â• 2 **and** for all exemplar nodes `stages.rolesPromoted == false`:  
       a. Add a Role node in `roles_master.json`:  
          ```json
          {
            "word":          "<canonicalRole>",
            "type":          "role",
            "exemplars":     [/* sorted list of Thing Words */],
            "acquaintances": []
          }
          ```  
       b. Populate the Role node's exemplars array with all Thing Words that listed this role (from the normalized list).
       c. For each exemplar in `master_words.json`, push `<canonicalRole>` into its `purposes` array.  
       d. Set each exemplar's `stages.rolesPromoted = true`.

7. Generate Role Acquaintances  
   üîÑ For each promoted Role Word:  
     ‚Ä¢ Use LLM to generate 5-7 acquaintances that co-occur with the role concept.
     ‚Ä¢ Store these in the Role Word's `acquaintances` array.
     ‚Ä¢ These acquaintances will be processed in Phase 3 (Acquaintance Adoption).

8. Persist & Resume Safety  
   ‚Ä¢ After each cluster promotion, write out updated `roles_master.json` and `master_words.json`.  
   ‚Ä¢ On restart, skip any Thing Word where `stages.rolesPromoted == true`.  

9. Output after Phase 2.5  
   ‚úÖ `roles_master.json` ‚Äî final Role Word definitions with acquaintances.  
   ‚úÖ Updated `master_words.json` ‚Äî all Thing Words now list canonical roles.  


üõ† TECH: Approach, Why & Cost Estimate  

‚Ä¢ **Embedding Model**  
  ‚Äì Model: `text-embedding-ada-002`  
  ‚Äì Why: Fast, cost-efficient semantic vectors ideal for clustering role labels.  
  ‚Äì Cost: $0.0004 per 1K tokens ‚Üí ~200 labels √ó 1 token ‚âà 200 tokens ‚Üí **\$0.0001** total.  

‚Ä¢ **Clustering & Synonym Merge**  
  ‚Äì Library: scikit-learn's AgglomerativeClustering (no API cost).  
  ‚Äì Synonym map via WordNet or custom dictionary (zero extra cost).  

‚Ä¢ **LLM Calls for Acquaintances**  
  ‚Äì Model: GPT-4 Turbo  
  ‚Äì Purpose: Generate 5-7 acquaintances per promoted Role Word.  
  ‚Äì Cost: ~100 tokens/role √ó ~100 roles ‚âà 10 000 tokens ‚Üí **\$0.04**.  

‚Ä¢ **Total Cost Estimate**  
  ‚Äì Embeddings pass: **\$0.0001**  
  ‚Äì LLM acquaintance generation: **\$0.04**  
  ‚Äì **Grand Total:** ~\$0.04  

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üîç **Key Essentials**  
1. **Idempotency via `rolesPromoted` flags**  
2. **Embedding-based clustering** (threshold ~0.8)  
3. **Promote only roles ‚â• 2 exemplars**  
4. **Generate acquaintances during promotion**  
5. **Persistent writes after each promotion**  
6. **Resumable on crash/restart**  

These steps ensure Phase 2.5 is reliable, repeatable, and crash-safe.  

------------------------------------

ü§ù PHASE 3: ACQUAINTANCE ADOPTION & INTEGRATION

Goal: Ensure every "Acquaintance" word hangs from the main taxonomy rather than dangle as an orphan, while preserving the natural, organic hierarchy we built in Phase 1. We use an embeddings-to-shortlist + LLM final vote approach to guarantee both high coverage and semantic precision.

1. Load Raw Acquaintances  
   üìù Read `raw_acquaintances.csv` ‚Üí list of `(sourceWord, acqTerm)` pairs.  
   ‚Ä£ Rationale: Having all logged pairs in memory lets us see which Thing Words reference each acquaintance.  

2. Build Unique Acquaintance Set  
   üéØ `allAcqs = unique(acqTerm)`  
   ‚Ä£ Rationale: Deduplicating here avoids redundant work‚Äîeach new term only needs a parent assigned once.  

3. Seed Orphan Placeholders  
   For each `acq` in `allAcqs`:  
     ‚Ä¢ If `acq` already exists in `master_words.json` ‚Üí skip.  
     ‚Ä¢ Else ‚Üí append placeholder node to `master_words.json`:  
       ```json
       {
         "word":          "<acq>",
         "type":          "thing",
         "parent":        null,
         "children":      [],
         "traits":        [],
         "acquaintances": [],
         "stages":        { /* inherit or initialize flags */ }
       }
       ```  
     ‚Ä£ Rationale: Placeholders let us assign parents in bulk before wiring up lateral links.  

4. Prepare Embeddings Search  
   ‚Ä¢ `parentCandidates = master_words.json.map(w ‚áí w.word + " ‚Äî parent:" + w.parent + "; children:" + w.children.slice(0,3).join(", "))`  
   ‚Ä¢ Compute embeddings for each extended candidate string **and** for each raw `acq` term using `text-embedding-ada-002`.  
   ‚Ä£ Rationale: Embedding the word plus minimal context (parent, a few children) biases similarity toward true categories.  

5. Build Shortlists via Nearest Neighbors  
   For each orphan `acq`:  
     a. Compute cosine similarity vs. all `parentCandidates`.  
     b. Select top 20 most similar candidates ‚Üí `shortlist[acq]`.  
   ‚Ä£ Rationale: A top-20 shortlist almost always contains the correct parent‚Äîcuts LLM calls by ~95%.  

6. LLM-Guided Final Parent Selection  
   For each `acq` + its `shortlist`:  
     ‚Ä¢ Prompt GPT-4 Turbo:
       ```
       I have 20 potential parent categories for "[acq]":
       ‚Ä¢ cand1, cand2, ‚Ä¶, cand20

       Which single category best serves as the parent for "[acq]"?  
       Reply with exactly one choice from the list.
       ```  
     ‚Ä¢ If response ‚àâ shortlist ‚Üí retry once.  
     ‚Ä¢ Assign `chosenParent = response`.  
   ‚Ä£ Rationale: LLM picks the most precise fit from a semantically-filtered set‚Äîminimizing hallucination.  

6.a. Cycle Detection & Prevention
   ‚Ä¢ Before assigning `chosenParent`, perform cycle detection:
     ```
     function isDescendant(potentialParent, node):
       current = potentialParent
       while current.parent:
         if current.parent === node: return true
         current = current.parent
       return false
     ```
   ‚Ä¢ If `isDescendant(chosenParent, acq)` returns true:
     - Remove `chosenParent` from shortlist
     - Re-prompt LLM with remaining candidates
     - If shortlist exhausted, fall back to safe high-level parent (e.g., "Thing")
   ‚Ä¢ This prevents circular hierarchies (A‚ÜíB‚ÜíC‚ÜíA) that would break tree traversal algorithms.
   ‚Ä£ Rationale: Maintains acyclic graph structure essential for difficulty tuning, analytics, and debugging.

6.b. Parent-Child Validation  
   ‚Ä¢ After LLM selects a parent, run a secondary validation check:
     ```
     "Is [orphan] a kind of [parent]? Answer yes or no only."
     ```
   ‚Ä¢ If validation fails (answer = "no"), log the case for review and consider retry with different shortlist.
   ‚Ä¢ This lightweight check catches edge-case misclassifications due to polysemy or fuzzy category boundaries.
   ‚Ä¢ Example failure cases: "Mercury" (planet vs. metal vs. deity), "Democracy" under "Food".
   ‚Ä£ Rationale: Prevents semantic violations that could disrupt traversal and confuse players.

7. Persist Parent Assignments (Bidirectional)  
   ‚Ä¢ Update each orphan node's `parent = chosenParent` in `master_words.json`.  
   ‚Ä¢ Then locate the `chosenParent` node and push `acq` into its `children` array (if not already present).  
   ‚Ä¢ Write out updated JSON.  
   ‚Ä£ Rationale: Ensures bidirectional consistency‚Äîparents know their children and vice versa.  

8. Attach Acquaintance Edges  
   For each `(sourceWord, acq)` in `raw_acquaintances.csv`:  
     ‚Ä¢ Locate `sourceWord` node ‚Üí push `acq` onto its `acquaintances` array (if missing).  
   ‚Ä¢ Persist `master_words.json`.  
   ‚Ä£ Rationale: Now every Thing Word correctly references its lateral connections.  

8.1. **Cross-Reference Validation** (After acquaintance adoption)
    ‚Ä¢ Re-validate all acquaintance references point to real nodes
    ‚Ä¢ Any missing references should be logged for manual review
    ‚Ä¢ This ensures no broken links exist after the adoption process
    ‚Ä¢ Rationale: Prevents UI errors from references to non-existent nodes

8.2. **De-dupe & Merge Existing Edges**
    ‚Ä¢ Before creating an A‚ÜíB acquaintance link, check whether B's acquaintances already contains A (and/or A's acquaintances contains B).
    ‚Ä¢ If so, skip creating a new edge‚Äîtreat it as a naturally symmetric association that has already emerged.
    ‚Ä¢ This prevents redundant metadata (e.g. both "Romeo‚ÜíJuliet" and "Juliet‚ÜíRomeo" only materializing once), while still allowing true bidirectional pairs that both sides generated independently.
    ‚Ä£ Rationale: Preserves the natural strength of mutual associations without artificial inflation.

9. (Optional) Prune Duplicate Acquaintances  
   ‚Ä¢ Cluster near-identical nodes (e.g. "Color" vs. "Colour"), merge them, rewrite links.  
   ‚Ä£ Rationale: Keeps graph clean‚Äîskip unless obvious duplicates appear.  

10. Output after Phase 3  
   ‚úÖ `master_words.json` updated:  
     ‚Äì Every orphan "Thing Word" has a valid `parent`.  
     ‚Äì Every original Thing Word's `acquaintances` array is filled.  
     ‚Äì Every adopted child is correctly listed in its parent's `children` array.  
     ‚Äì Natural bidirectional associations are preserved without redundancy.
     ‚Äì All parent-child relationships have been validated for semantic correctness.

11. **Graph Connectivity Validation** (Final integrity check)
    ‚Ä¢ Run BFS from root "Thing" using only parent‚Üíchild relationships
    ‚Ä¢ Log any unvisited nodes for manual inspection
    ‚Ä¢ Does not auto-fix‚Äîjust identifies unreachable islands
    ‚Ä¢ Rationale: Guarantees no completely orphaned subgraphs exist
    ‚Ä¢ Implementation: Simple breadth-first search with visited set tracking

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üõ† TECH: Hybrid Approach, Why & Cost  

‚Ä¢ **Embeddings Pass**  
  ‚Äì Model: `text-embedding-ada-002`  
  ‚Äì Purpose: Build a shortlist >95% likely to include true parent.  
  ‚Äì Cost: ~$0.0004 per 1K tokens ‚Üí 2 000 candidates + contexts ‚âà\$0.002 total.  

‚Ä¢ **Nearest-Neighbor Search**  
  ‚Äì Library: Faiss or `sklearn.neighbors` (no API cost).  
  ‚Äì Purpose: Quickly retrieve top-K similar candidates per orphan.  

‚Ä¢ **LLM Final Vote**  
  ‚Äì Model: GPT-4 Turbo  
  ‚Äì Prompt size: ~100 tokens/orphan ‚Üí 2 000 orphans ‚Üí 200 000 tokens.  
  ‚Äì Cost: 200 000 tokens √ó \$0.004/token ‚âà **\$0.80**.  
  ‚Äì **Additional cost for cycle detection retries**: ~10% of orphans may need retry ‚Üí 200 retries √ó 100 tokens ‚âà 20 000 tokens ‚Üí **\$0.08**.

‚Ä¢ **LLM Validation**  
  ‚Äì Model: GPT-4 Turbo  
  ‚Äì Purpose: Verify parent-child semantic correctness.  
  ‚Äì Cost: ~20 tokens/orphan √ó 2 000 orphans ‚âà 40 000 tokens ‚Üí **\$0.16**.  

‚Ä¢ **Total Phase 3 Estimate**  
  ‚Äì Embeddings: ~\$0.002  
  ‚Äì LLM calls: ~\$0.80  
  ‚Äì Cycle detection retries: ~\$0.08  
  ‚Äì Validation: ~\$0.16  
  ‚Äì **Grand Total:** ~\$1.04

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üîç Why This Matters  

1. **Semantic Integrity**  
   ‚Äì Context-aware embeddings bias toward valid parent categories.  
   ‚Äì LLM then picks the exact best fit‚Äîavoiding misclassification.  
   ‚Äì Validation step catches edge-case misclassifications due to polysemy or fuzzy boundaries.

2. **Full Graph Connectivity**  
   ‚Äì No more dangling words‚Äîall links traverse cleanly.  
   ‚Äì New acquaintances adopt into the structure naturally.  

3. **Bidirectional Consistency**  
   ‚Äì Parents point to their new children.  
   ‚Äì Definitions and UI reflect a fully wired web.  

4. **Idempotency & Crash Safety**  
   ‚Äì Skip any orphan whose `parent != null`.  
   ‚Äì Clear stages logic ensures safe restarts with no duplicated work.  

5. **Quality Assurance**  
   ‚Äì Lightweight validation prevents semantic violations that could disrupt traversal.
   ‚Äì Catches edge cases like "Mercury" (planet vs. metal vs. deity) or "Democracy" under "Food".
   ‚Äì **Cycle detection prevents circular hierarchies that would break tree traversal algorithms and analytics.**

With this hybrid approach, your taxonomy remains coherent, intuitive, and fully connected‚Äîready for the player's logical and poetic journeys.  


------------------------------------

üîó PHASE 3.5: BUILD UNIFIED INDEX

Goal: Create a unified index of all words (Thing, Trait, Role) for frontend navigation and deduplication validation.

1. Load All Word Collections  
   üìù Read `master_words.json`, `traits_master.json`, and `roles_master.json` into memory.  
   ‚Ä¢ Collect all word entries from each file with their types and metadata.  

2. Build Unified Index  
   üîÑ Create `unified_master.json` with structure:  
     ```json
     {
       "all_words": {
         "Cat": { 
           "type": "thing", 
           "file": "master_words.json",
           "metadata": { /* full thing word data */ }
         },
         "Agile": { 
           "type": "trait", 
           "file": "traits_master.json",
           "metadata": { /* full trait word data */ }
         },
         "Cutting": { 
           "type": "role", 
           "file": "roles_master.json",
           "metadata": { /* full role word data */ }
         }
       },
       "stats": {
         "total_words": 2500,
         "thing_words": 2000,
         "trait_words": 300,
         "role_words": 200
       }
     }
     ```  

3. Deduplication Validation  
   ‚Ä¢ Check for any word that appears in multiple files with different types.  
   ‚Ä¢ Log any duplicates for manual review (e.g., "Cutting" as both Thing and Role).  
   ‚Ä¢ This ensures data integrity and prevents frontend confusion.  

4. Frontend Optimization  
   ‚Ä¢ Create a lightweight index file for fast lookups:  
     ```json
     {
       "Cat": "thing",
       "Agile": "trait", 
       "Cutting": "role"
     }
     ```  
   ‚Ä¢ This enables O(1) word existence checks without loading full metadata.  

5. Output after Phase 3.5  
   ‚úÖ `unified_master.json` ‚Äî complete index of all words with full metadata  
   ‚úÖ `word_index.json` ‚Äî lightweight lookup table for frontend performance  
   ‚úÖ Deduplication report ‚Äî any cross-type duplicates logged for review  

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üõ† TECH: Approach, Why & Cost  

‚Ä¢ **No LLM Calls Required** ‚Äî purely data processing operation  
‚Ä¢ **Local Processing** ‚Äî reads existing JSON files and creates new indexes  
‚Ä¢ **Cost**: ~$0.00 (no API calls)  
‚Ä¢ **Performance**: Fast file I/O operations only  

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üîç Why This Matters  

1. **Frontend Performance**  
   ‚Äì Single file lookup instead of searching across 3 separate files  
   ‚Äì O(1) word existence checks for clickable links  

2. **Data Integrity**  
   ‚Äì Prevents accidental duplicates across word types  
   ‚Äì Ensures each word has a single, unambiguous identity  

3. **Developer Experience**  
   ‚Äì Simplified frontend integration  
   ‚Äì Clear separation between build process and runtime data  

4. **Game Navigation**  
   ‚Äì Players can click any word regardless of its type  
   ‚Äì Unified traversal experience across all word categories  

This phase bridges the gap between the modular build process and the unified frontend experience, ensuring optimal performance and data consistency.


------------------------------------

üó£Ô∏è PHASE 4: NATURAL-LANGUAGE DEFINITION PASS (WE WONT DO THIS RIGHT NOW. WE WILL JUST HAVE THE WORDS / LINKS WITHOUT A NATURAL LANGUAGE PASS FOR PLAYTESTING)

Goal: Turn each word's structured metadata (parent, children, traits, acquaintances, purposes) into a clean, human-readable definition that feels organic and engaging to players. This adds personality and immersion while preserving all navigational affordances.

1. Gather All Nodes  
   üîç Load `master_words.json` and `traits_master.json` into memory.  
   ‚Ä¢ Includes all Thing Words and promoted Trait Words.  

2. Prepare Prompt Template  
   ‚úèÔ∏è Use a system + user prompt (see: `prompts/Natural Language.txt`) that:  
     ‚Ä¢ Accepts a full metadata object for each word  
     ‚Ä¢ Generates a sentence wikipedia-esque definition  
     ‚Ä¢ References the word's parent category  
     ‚Ä¢ Weaves in up to X children, Y traits, and Z acquaintances (as the dev, you can decide how many of each to include)  
     ‚Ä¢ Ends with: "Use no additional terms beyond what's given."  
   ‚Ä£ This ensures coverage of every key relationship while minimizing hallucination or drift.

3. Batch & Generate  
   ‚ö°Ô∏è Process definitions in batches of 10‚Äì20 words per API call:  
     ‚Ä¢ Send prompt + batch of JSON nodes to GPT-4 Turbo  
     ‚Ä¢ Receive a list of `{ word, definition }` responses  
     ‚Ä¢ Validate structure before saving  
   ‚Ä£ Batching reduces per-call overhead and keeps costs predictable.

4. Validate Output  
   ‚úîÔ∏è For each generated definition:  
     ‚Ä¢ Confirm the main word appears in its own definition  
     ‚Ä¢ Optionally run a keyword coverage script to ensure presence of required metadata (parent, ‚â•1 trait, etc.)  
     ‚Ä¢ Spot-check ~10% of definitions for quality  
   ‚Ä£ Prevents silent errors or unusable definitions from slipping into the final UI.

5. Persist Definitions  
   üíæ Write to `definitions_master.json` in this format:  
     ```json
     [
       {
         "word": "Cat",
         "definition": "Cats are agile mammals known for their independence and graceful movements. They are often kept as pets and associated with traits like curiosity and nocturnality. Related ideas include dogs, litter boxes, and scratching posts."
       },
       ‚Ä¶
     ]
     ```
   ‚Ä£ Easy to look up and integrate on the front-end.

6. Linkability Support  
   üîó All terms in the generated definitions that match a known Thing Word or Trait Word should be linkable in the UI.  
   ‚Ä¢ Don't encode links in the definition text itself  
   ‚Ä¢ Instead, the front-end should match visible tokens against each word's known metadata (parent, children, traits, acquaintances)  
   ‚Ä¢ If matched, style and bind the word as a clickable hyperlink  
   ‚Ä£ This allows for dynamic, robust linking even if the natural-language wording varies.

7. Optional: Regeneration Support  
   üîÑ To incrementally re-render definitions later:  
     ‚Ä¢ Track a `definitionLastUpdatedAt` timestamp per word  
     ‚Ä¢ Only re-run the LLM if its metadata has changed since that time

8. Output after Phase 4  
   ‚úÖ `definitions_master.json` with natural-language definitions for every Thing and Trait Word  
   ‚úÖ Front-end now supports immersive, readable traversal without breaking linkability

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üõ† TECH: Model, Cost & Format

‚Ä¢ Model: GPT-4 Turbo  
  ‚Äì Cost: $0.003 / 1K tokens  
  ‚Äì Easily fits ~20 metadata objects + prompt + output in 128K context  

‚Ä¢ Token & Cost Estimate  
  ‚Äì Avg: ~200 tokens per word (input + output)  
  ‚Äì 10,000 words = ~2M tokens ‚Üí $6  
  ‚Äì 50,000 words ‚Üí ~$30  
  ‚Äì 500,000 words ‚Üí ~$300  

‚Ä¢ Batch Size  
  ‚Äì 10‚Äì20 words per request recommended  
  ‚Äì Keeps response latency low and throughput high

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üìå Why This Matters

‚Ä¢ üöÄ Player Experience: Adds clarity, narrative, and intuition to each node  
‚Ä¢ üîÅ Future-Proof: Easy to re-run when definitions change  
‚Ä¢ üß† Cognitive Glue: Makes the graph feel alive and explorable instead of abstract  
‚Ä¢ üß© Front-End Ready: UI link logic is metadata-driven, not tied to string parsing

------------------------------------

üõ°Ô∏è QUALITY ASSURANCE & VALIDATION

Goal: Ensure data integrity, prevent common failure modes, and maintain graph consistency throughout the build pipeline.

========================
CORE SAFEGUARDS (MUST-DO)
========================                                                

1. **Cycle Detection & Prevention** (Phase 3)
   ‚Ä¢ Implemented in Phase 3 Step 6.a
   ‚Ä¢ Prevents circular hierarchies that break tree traversal algorithms
   ‚Ä¢ Uses `isDescendant()` function to check parent chains before assignment

2. **Exclusion-List Validation** (Phase 1)
   ‚Ä¢ After each acquaintances generation, verify no excluded terms appear
   ‚Ä¢ Excluded terms: parent, children, traits, synonyms
   ‚Ä¢ If violation detected: append exclusion clause to prompt and retry once
   ‚Ä¢ If still invalid: drop the term and log warning
   ‚Ä¢ Rationale: Prevents semantic pollution that could cascade downstream

3. **Cross-Reference Existence Validation** (Phase 1 & 3)
   ‚Ä¢ **Phase 1**: When assembling metadata, verify all referenced words exist
     - Children: If missing, enqueue for generation
     - Traits/Acquaintances/Roles: If missing, drop and warn
   ‚Ä¢ **Phase 3**: After adoption, re-validate all acquaintance references point to real nodes
   ‚Ä¢ Rationale: Prevents broken JSON links and UI errors from missing nodes

4. **Graph Connectivity Validation** (After Phase 3)
   ‚Ä¢ Run BFS from root "Thing" using only parent‚Üíchild relationships
   ‚Ä¢ Log any unvisited nodes for manual inspection
   ‚Ä¢ Does not auto-fix‚Äîjust identifies unreachable islands
   ‚Ä¢ Rationale: Guarantees no completely orphaned subgraphs exist

========================
OPTIONAL SAFEGUARDS (NICE-TO-HAVE)
========================

5. **Metadata Completeness Validation** (Soft Enforcement)
   ‚Ä¢ Check for nodes with very sparse metadata (< 3 children, < 3 traits)
   ‚Ä¢ If below soft threshold, retry generation once
   ‚Ä¢ If still sparse, accept but log warning
   ‚Ä¢ Rationale: Some concepts naturally have few subtypes‚Äîrigid enforcement stalls builds

6. **Synonym/Duplicate Detection** (Postpone)
   ‚Ä¢ Basic normalization (lowercase, singularize) catches ~90% of cases
   ‚Ä¢ Sophisticated fuzzy matching adds complexity and risk of over-merging
   ‚Ä¢ If needed later: implement as "Phase 3.5" with hand-curated alias maps
   ‚Ä¢ Rationale: Premature optimization that could accidentally merge distinct concepts

========================
IMPLEMENTATION GUIDELINES
========================

‚Ä¢ **Lightweight & Fast**: All validation should be quick checks, not expensive operations
‚Ä¢ **Fail-Safe**: When validation fails, log warnings but don't crash the pipeline
‚Ä¢ **Retry Logic**: Give LLM one retry opportunity with clearer constraints
‚Ä¢ **Manual Review**: Log issues for human review rather than auto-fixing complex cases
‚Ä¢ **Cost-Aware**: Validation should not significantly increase API costs

========================
VALIDATION TIMING
========================

‚Ä¢ **Phase 1**: Exclusion-list checks after each LLM call, cross-reference validation after metadata assembly
‚Ä¢ **Phase 3**: Cycle detection during parent assignment, cross-reference validation after adoption
‚Ä¢ **Post-Phase 3**: Connectivity validation as final integrity check
‚Ä¢ **Optional**: Metadata completeness checks after each phase

========================
LOGGING & MONITORING
========================

‚Ä¢ **Warning Logs**: Track validation failures for review
‚Ä¢ **Error Logs**: Document pipeline-breaking issues
‚Ä¢ **Success Metrics**: Count successful validations vs. failures
‚Ä¢ **Manual Review Queue**: List items requiring human intervention

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üß† Why These Safeguards Matter

‚Ä¢ **Data Integrity**: Prevents corrupted or inconsistent graph structures
‚Ä¢ **Debugging**: Makes it easier to identify and fix issues
‚Ä¢ **Reliability**: Ensures the pipeline produces usable results consistently
‚Ä¢ **Maintainability**: Reduces technical debt and future debugging time
‚Ä¢ **User Experience**: Prevents broken links and traversal errors in the game

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üìé Implementation Notes

‚Ä¢ Start with the four core safeguards‚Äîthey provide maximum protection with minimal complexity
‚Ä¢ Add optional safeguards only when real issues emerge in testing
‚Ä¢ Keep validation logic separate from core generation logic for maintainability
‚Ä¢ Use consistent logging format across all validation steps

------------------------------------

üîß CONCRETE IMPLEMENTATION DETAILS

Goal: Provide specific technical guidance for implementing the build pipeline with proper file organization, data formats, API configuration, error handling, batching, progress tracking, and development vs production modes.

========================
FILE/FOLDER STRUCTURE
========================

```
6degrees/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ default.json         # Base configuration
‚îÇ   ‚îú‚îÄ‚îÄ test-sample.json     # Small test config (10 nodes)
‚îÇ   ‚îú‚îÄ‚îÄ fast-test.json       # Medium test (100 nodes)
‚îÇ   ‚îî‚îÄ‚îÄ full-run.json        # Production (2000+ nodes)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # CSV files from Phase 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw_traits.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw_acquaintances.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ raw_purposes.csv
‚îÇ   ‚îú‚îÄ‚îÄ processed/           # JSON files for each phase
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ master_words.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ traits_master.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ roles_master.json
‚îÇ   ‚îú‚îÄ‚îÄ unified/             # Phase 3.5 outputs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unified_master.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ word_index.json
‚îÇ   ‚îî‚îÄ‚îÄ definitions/         # Phase 4 outputs
‚îÇ       ‚îî‚îÄ‚îÄ definitions_master.json
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îú‚îÄ‚îÄ phase1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ children-generation.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trait-generation.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ acquaintance-generation.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ role-generation.txt
‚îÇ   ‚îî‚îÄ‚îÄ phase4/
‚îÇ       ‚îî‚îÄ‚îÄ natural-language.txt
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ phase1.js
‚îÇ   ‚îú‚îÄ‚îÄ phase2.js
‚îÇ   ‚îú‚îÄ‚îÄ phase2-5.js
‚îÇ   ‚îú‚îÄ‚îÄ phase3.js
‚îÇ   ‚îú‚îÄ‚îÄ phase3-5.js
‚îÇ   ‚îî‚îÄ‚îÄ phase4.js
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ phase1_20240625.log
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ embeddings.js
    ‚îú‚îÄ‚îÄ validation.js
    ‚îî‚îÄ‚îÄ api-client.js
```

========================
DATA FILE FORMATS
========================

**Mixed approach based on use case:**

‚Ä¢ **CSV for raw collections (Phase 1)**
  - Simple, append-friendly for logging traits/acquaintances/roles
  - Easy to process and validate
  - Efficient for large datasets

‚Ä¢ **JSON for structured data**
  - `master_words.json` with full node metadata and stage flags
  - `traits_master.json` and `roles_master.json` for bridge nodes
  - Preserves complex nested structures

‚Ä¢ **JSON for unified access**
  - `unified_master.json` for complete index of all words regardless of type
  - `word_index.json` for lightweight O(1) lookups
  - Enables frontend to quickly verify any word exists and is clickable

‚Ä¢ **JSON for definitions**
  - `definitions_master.json` for natural language definitions
  - Easy front-end integration
  - Maintains linkability metadata

========================
API CONFIGURATION
========================

**Environment Variables (.env file - gitignored):**
```
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=gpt-4-turbo
```

**API Client Configuration (utils/api-client.js):**
```javascript
const config = {
  model: process.env.ANTHROPIC_MODEL || 'gpt-4-turbo',
  maxRetries: 2,  // As per GDD retry logic
  backoffMs: 100   // 50-100ms as specified
};
```

========================
ERROR HANDLING STRATEGY
========================

**Fail-Safe Principle (as specified in GDD):**

‚Ä¢ **Warnings, not crashes**: Log and continue for non-critical errors
‚Ä¢ **Retry logic**: 2 attempts for format violations or exclusion violations
‚Ä¢ **Exponential backoff**: For HTTP 429 rate limits
‚Ä¢ **Manual review queue**: Log unresolvable issues for human intervention

**Implementation Example:**
```javascript
async function callLLMWithRetry(prompt, validator, maxRetries = 2) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      const result = await llm.generate(prompt);
      if (validator(result)) return result;

      // Add clarification to prompt for retry
      prompt += '\nPlease ensure the response follows the exact format requested.';
    } catch (e) {
      if (e.status === 429) {
        await sleep(Math.pow(2, i) * 1000); // Exponential backoff
      }
    }
  }
  // Log warning and return partial result
  logger.warn(`Failed after ${maxRetries} attempts`);
  return null;
}
```

========================
BATCHING LOGIC
========================

**Phase-specific batching (from GDD recommendations):**

‚Ä¢ **Phase 1**: No batching (BFS one word at a time)
‚Ä¢ **Phase 4**: 10-20 words per batch for definitions
‚Ä¢ **Configurable via config**: `batchSizes: { phase4: 15 }`

**Configuration Example:**
```json
{
  "batchSizes": {
    "phase1": 1,
    "phase4": 15
  }
}
```

========================
PROGRESS TRACKING
========================

**Enhanced stage flags with metrics (as shown in GDD):**
```json
{
  "stages": {
    "childrenDone": false,
    "rawLogged": false,
    "traitsPromoted": false,
    "rolesPromoted": false,
    "acquaintancesAdopted": false,
    "definitionGenerated": false
  },
  "metrics": {
    "attemptCount": 0,
    "lastModified": "2024-06-25T10:00:00Z",
    "tokensUsed": 0
  }
}
```

**Real-time Progress Indicators:**
‚Ä¢ Terminal progress bars: `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 45% (900/2000) - current="Vehicle"`
‚Ä¢ Success/failure rates per phase
‚Ä¢ Token usage tracking
‚Ä¢ Time elapsed per phase

========================
DEVELOPMENT VS PRODUCTION
========================

**Configurable modes (as emphasized in GDD):**

**Test Sample Configuration (config/test-sample.json):**
```json
{
  "targetCount": 10,
  "childrenPerNode": 3,
  "traitsPerNode": 3,
  "acquaintancesPerNode": 3,
  "rolesPerNode": 1,
  "dryRun": true,
  "skipValidation": false
}
```

**Full Production Configuration (config/full-run.json):**
```json
{
  "targetCount": 2000,
  "childrenPerNode": 7,
  "traitsPerNode": 7,
  "acquaintancesPerNode": 7,
  "rolesPerNode": 3,
  "dryRun": false,
  "skipValidation": false
}
```

**Development Features:**
‚Ä¢ **Dry-run mode**: Simulate pipeline without API calls
‚Ä¢ **Sample inspection**: `npm run sample -- phaseX` for curated output preview
‚Ä¢ **Interactive debugging**: `npm run phase1:inspect -- --node Banana`
‚Ä¢ **Verbose logging**: Full request/response bodies for debugging

========================
SCRIPT ENTRY POINTS
========================

**One-command entry points (as specified in GDD):**
```bash
npm run phase1 -- --config test-sample.json
npm run phase2 -- --config full-run.json
npm run phase2-5 -- --config full-run.json
npm run phase3 -- --config full-run.json
npm run phase3-5 -- --config full-run.json
npm run phase4 -- --config full-run.json
```

**Utility Commands:**
```bash
npm run sample -- phase1          # Preview outputs
npm run reset -- phase1           # Clear stage flags
npm run validate -- phase3        # Run validation only
npm run dry-run -- phase1         # Simulate without API calls
```

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üß† Why These Implementation Details Matter

‚Ä¢ **Developer Experience**: Easy configuration switching and progress monitoring
‚Ä¢ **Reliability**: Robust error handling and resumability
‚Ä¢ **Cost Management**: Token tracking and dry-run capabilities
‚Ä¢ **Maintainability**: Clear file organization and separation of concerns
‚Ä¢ **Scalability**: Configurable parameters for different deployment scenarios

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üìé Implementation Guidelines

‚Ä¢ Start with test-sample.json for initial development and validation
‚Ä¢ Use dry-run mode to verify pipeline logic before API costs
‚Ä¢ Monitor token usage and success rates during development
‚Ä¢ Keep configuration files version-controlled but API keys in .env
‚Ä¢ Implement all validation steps before moving to production runs





~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ñÑ‚ñÑ‚ññ  ‚ñó‚ñÑ‚ññ ‚ñó‚ññ  ‚ñó‚ññ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ    ‚ñó‚ñÑ‚ñÑ‚ñÑ‚ññ‚ñó‚ññ  ‚ñó‚ññ‚ñó‚ñÑ‚ñÑ‚ñÑ 
‚ñê‚ñå   ‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñõ‚ñö‚ññ‚ñê‚ñå  ‚ñà      ‚ñê‚ñå   ‚ñê‚ñõ‚ñö‚ññ‚ñê‚ñå‚ñê‚ñå  ‚ñà
‚ñê‚ñõ‚ñÄ‚ñÄ‚ñò‚ñê‚ñõ‚ñÄ‚ñö‚ññ‚ñê‚ñå ‚ñê‚ñå‚ñê‚ñå ‚ñù‚ñú‚ñå  ‚ñà      ‚ñê‚ñõ‚ñÄ‚ñÄ‚ñò‚ñê‚ñå ‚ñù‚ñú‚ñå‚ñê‚ñå  ‚ñà
‚ñê‚ñå   ‚ñê‚ñå ‚ñê‚ñå‚ñù‚ñö‚ñÑ‚ñû‚ñò‚ñê‚ñå  ‚ñê‚ñå  ‚ñà      ‚ñê‚ñô‚ñÑ‚ñÑ‚ññ‚ñê‚ñå  ‚ñê‚ñå‚ñê‚ñô‚ñÑ‚ñÑ‚ñÄ
FRONT END
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                  

üéÆ FRONT-END USER EXPERIENCE

1. Launch & Intro Screen
	‚Ä¢	Splash / Title
	‚Ä¢	Display a clean "6 Degrees" logo or title.
	‚Ä¢	Centered "Play Now" button invites the player to begin.
	‚Ä¢	Settings / Difficulty (optional)
	‚Ä¢	If you support multiple step-counts (e.g. 4-, 6-, 8-degree puzzles), allow the player to choose here.
	‚Ä¢	Store this value as the game's "X" (number of clicks from origin ‚Üí destination).

2. Game Start
	‚Ä¢	Generate Puzzle
	‚Ä¢	Pick a random destination word from the graph.
	‚Ä¢	Walk exactly X hops away (parent/child/acquaintance/trait/role links) to select an origin word that is guaranteed to reach the destination in ‚â§ X clicks.
	‚Ä¢	Show Origin Definition
	‚Ä¢	Navigate immediately to the origin word's "definition page."

3. Definition Page Layout

Each word (Thing, Trait or Role) has its own page template:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ [Back ‚Üê]   DESTINATION: [Stop Sign]     ‚îÇ ‚Üê always visible bar at top or side
‚îÇ Steps:  0 | Path: [Cat]                ‚îÇ ‚Üê step counter + breadcrumb trail
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ              CAT                       ‚îÇ ‚Üê Word title (large headline)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ PARENT: Animal                         ‚îÇ ‚Üê clickable link
‚îÇ CHILDREN: Dog  ‚Ä¢ Bird ‚Ä¢ Fish ‚Ä¢ ‚Ä¶       ‚îÇ ‚Üê comma-separated or bullet list
‚îÇ TRAITS: Independent ‚Ä¢ Agile ‚Ä¢ Curious   ‚îÇ ‚Üê clickable adjectives
‚îÇ ACQUAINTANCES: Mouse ‚Ä¢ Litter Box ‚Ä¢ ‚Ä¶   ‚îÇ ‚Üê poetic links
‚îÇ PURPOSES: Companion ‚Ä¢ Pet              ‚îÇ ‚Üê roles/purposes
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ [Footer or Sidebar: "Quit" | "Help"]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

	‚Ä¢	Header / Sidebar
	‚Ä¢	Destination Reminder: Always show the goal word.
	‚Ä¢	Step Counter: "Steps: N" increments on each forward click.
	‚Ä¢	Breadcrumb Trail:
	‚Ä¢	Shows each visited word in order.
	‚Ä¢	Clicking any breadcrumb rewinds the game back to that point (resets step count and path accordingly).
	‚Ä¢	Main Content
	‚Ä¢	Word Title: Large, centered.
	‚Ä¢	Metadata Sections (each with a clear label):
	1.	Parent (single link)
	2.	Children (list)
	3.	Traits (list)
	4.	Acquaintances (list)
	5.	Purposes / Roles (if present)
	‚Ä¢	Clickable Links
	‚Ä¢	All listed words are rendered as tappable/clickable.
	‚Ä¢	On hover (desktop) or focus (accessibility), show subtle highlighting.

4. Traversal & Rules
	‚Ä¢	Click Behavior
	‚Ä¢	Clicking any metadata link (parent, child, trait, acquaintance, role) opens that word's definition page.
	‚Ä¢	Each forward click increments the step count by 1.
	‚Ä¢	No penalty for backtracking‚Äîclicking a breadcrumb simply restores the previous state.
	‚Ä¢	No Hard Blocks
	‚Ä¢	The player may navigate up (to parents), down (to children), or laterally (via traits/acquaintances) in any order.
	‚Ä¢	Cycle Handling
	‚Ä¢	The UI does not prevent clicking previously visited words. Instead, breadcrumb navigation lets players back out of loops if needed.

5. Winning the Game
	‚Ä¢	Arrival
	‚Ä¢	When the player clicks the destination word, the game immediately transitions to a "Victory" screen.
	‚Ä¢	Victory Screen

  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          üéâ Hooray! üéâ           ‚îÇ
‚îÇ You reached [Stop Sign] in 6 steps ‚îÇ
‚îÇ Path: Cat ‚Üí Night ‚Üí Street ‚Üí ‚Ä¶    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   [Share Your Score] [Play Again]  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚Ä¢	Show final step count, breadcrumb path, and two buttons:
	1.	Share Your Score (opens OS-level share sheet / copy link)
	2.	Play Again (restarts at the intro or immediately generates a new puzzle)














